{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import PercentFormatter #plot as percentage\n",
    "import seaborn #plot density and histogram at the same time\n",
    "# Set directory where files are downloaded to. Chdir has to be changed in order to run on another computer\n",
    "os.chdir('C:\\\\Users\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE') #change this to the folder where the data set is stored, all the results will be saved in the same folder\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1 Prepare Data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08 = pd.read_csv(os.getcwd()+'\\\\fs08.csv').set_index('CustID')\n",
    "#identifier\n",
    "TIME = ['QINTRVMO', 'QINTRVYR', 'rbtmo_1', 'rbtmo_2', 'diff_1', 'diff_2']\n",
    "ID = ['NEWID']\n",
    "\n",
    "#dependent variables\n",
    "CONS = ['FD','SND','ND','DUR','TOT']\n",
    "FUTCONS = ['fut_' + c for c in CONS]\n",
    "LRUNCONS = ['lrun_' + c for c in CONS]\n",
    "CHGCONS = ['chg_' + c for c in CONS]\n",
    "for i in range(len(LRUNCONS)):\n",
    "    fs08[LRUNCONS[i]] = fs08[[CONS[i],FUTCONS[i]]].sum(axis=1)\n",
    "\n",
    "#explanatory variables\n",
    "DEMO = ['age', 'adults', 'PERSLT18', 'MARITAL1', 'CUTENURE'] #exclude , 'FINCBTAX'\n",
    "    #age; number of adults; people below 18; marital status; housing tenure; income in the last 12 months\n",
    "DEMO2 = ['FSALARYM', 'FINCBTXM'] \n",
    "    #FSALARYM: income from salary and wages, CKBKACTX: balance/market value in balance accounts/brookerage accounts;    \n",
    "    #FINCBTXM: Total amount of family income before taxes (Imputed or collected data); (relevant demographics available for the second stimulus only)\n",
    "ASSETS = ['valid_finassets','finassets']\n",
    "    # finassets: sum of 1) SAVACCTX (Total balance/market value (including interest earned) CU had in savings accounts in banks, savings and loans,\n",
    "                         #credit unions, etc., as of the last day of previous month;)\n",
    "                # and    2)CKBKACTX (Total balance or market value (including interest earned) CU had in checking accounts, brokerage accounts, \n",
    "                            #and other similar accounts as of the last day of the previous month\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'qescrowx_sum', 'timeleft'] #exclude , 'orgmrtx_sum',\n",
    "    #morgpayment: morgage payment per month; qblncm1x_sum: sum of principal balances outstanding at the beginning of month M1; orgmrtx_sum: sum of mortgage amounts;\n",
    "    #qescrowx_sum: sum of last regular escrow payments; timeleft: maximum time left on mortgage payment\n",
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #\n",
    "#sample split\n",
    "RBT = ['rbtamt', 'rbtamt_chk', 'rbtamt_e']\n",
    "LAGRBT = ['last_' + var for var in RBT] #lagged variables\n",
    "FUTRBT = ['fut_' + var for var in RBT] #future variables\n",
    "\n",
    "for m in MORTGAGE:\n",
    "    fs08.loc[fs08[m].isna(),m]=0 #replace missing values of mortgages with zeros\n",
    "        \n",
    "\n",
    "\n",
    "fs08 = fs08[TIME + ID + CONS + DEMO + DEMO2 + ASSETS + MORTGAGE + RBT + ['rbtamt_1','rbtamt_2'] + LAGRBT + FUTRBT + FUTCONS + LRUNCONS + CHGCONS + EDUC] #+ CHGCONS + LAGCONS \n",
    "#fs08 = fs08.loc[fs08['timeleft']>0,:]\n",
    "fs08 = pd.get_dummies(fs08, columns=['CUTENURE','MARITAL1']) #change categorical variables to dummy variables\n",
    "\n",
    "DEMO = [s for s in DEMO if s!='CUTENURE' if s!='MARITAL1'] + ['CUTENURE' + f'_{j}' for j in list(range(1,6)) if j!=3] +['MARITAL1' + f'_{j}' for j in list(range(1,5))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the average rebate amount per individual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08['rbtamt_idmean'] = 0\n",
    "fs08['rbtamt_idmean'] = fs08.groupby('CustID')['rbtamt'].transform('mean') #household mean of rebates\n",
    "fs08['rbt_count'] = 0\n",
    "fs08['rbt_count'] = fs08.groupby('CustID')['rbtamt'].transform('count') #number of rebates a household received\n",
    "\n",
    "#\n",
    "#sometimes individuals give information of rebate receipt preceding (following) three months of the first (last) interview.\n",
    "#Wherever this is the case, the average rebate should be the weighted mean of rebates received before (after) the relevant time and the actual rebate \n",
    "#preceding rebate:\n",
    "\n",
    "#weighted mean including predeceding three-month period's rebate:\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['last_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) \n",
    "\n",
    "#change for all entries for a given individual\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist()\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first') \n",
    "                            \n",
    "\n",
    "#weighted mean including following three-month period's rebate:\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['fut_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) \n",
    "#change for all entries for a given individual\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0)  & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() \n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last')  \n",
    "\n",
    "#sometimes there is no entry for rebates received in the relevant time period but in the past (future): \n",
    "#future:\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & \n",
    "                   (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() #custids for hh with only future rebate                                                                              \n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 fs08['fut_rbtamt'], fs08['rbtamt_idmean']) #fut rebate amount is the mean for those hhs\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last') #change for all entries of hh\n",
    "\n",
    "#past:\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist() #custids for hh with only past rebate \n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())), \n",
    "                                fs08['last_rbtamt'], fs08['rbtamt_idmean']) #past rebate is mean for those hhs\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first') #change for all entries of hh\n",
    "\n",
    "\n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[(fs08['rbtamt']>0) | (fs08['fut_rbtamt']>0) | (fs08['last_rbtamt']>0) ,'rbt_flag'] = 1 #generate rebate flag\n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('sum') #change so that it is one is hh received a rebate at least once\n",
    "fs08.loc[fs08['rbt_flag']>0, 'rbt_flag'] = 1\n",
    "fs08 = fs08.loc[fs08['rbt_flag']==1]\n",
    "\n",
    "\n",
    "index = fs08.index[fs08['rbtamt_idmean'].isna()].tolist() #there are a couple of entries who have missing information on the rebate bc teh entry of actual rebate receipt might be missing\n",
    "index = list(set(index))\n",
    "fs08.loc[index, 'rbtamt_idmean'] = fs08.loc[index,'fut_rbtamt']\n",
    "fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'fut_rbtamt' ]\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'last_rbtamt' ]\n",
    "\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations, impute values for financial liquidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08 = fs08.reset_index()\n",
    "\n",
    "#Iterative imputation for financial liquidity\n",
    "#explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "fs08 = fs08.dropna(subset=CONS+DEMO+DEMO2+MORTGAGE) #Keep only observations that have all info on explanatory variables\n",
    "\n",
    "#generate subdata set\n",
    "fs08_finit = fs08.copy()\n",
    "fs08_finit = fs08_finit[ ID + CONS + DEMO + DEMO2 + ASSETS + MORTGAGE +  LRUNCONS + EDUC]\n",
    "\n",
    "labels = list(fs08_finit.columns) #capture column names\n",
    "imp_mean = IterativeImputer(random_state=0) #use python package iterative imputer\n",
    "imp_mean.fit(fs08_finit[2:]) \n",
    "fs08_finit = pd.DataFrame(imp_mean.transform(fs08_finit),columns=labels) #change values to imputed values\n",
    "fs08_finit = fs08_finit.loc[:,['finassets','NEWID']]\n",
    "fs08_finit = fs08_finit.rename(columns={'finassets':'finassets_it'})\n",
    "\n",
    "#merge back\n",
    "fs08 = pd.merge(fs08.sort_values(by = ['NEWID']).reset_index(), fs08_finit.sort_values(by = ['NEWID']).reset_index(), how = 'left', on = 'NEWID', validate = '1:1') #merge with previous data\n",
    "fs08 = fs08.drop(columns=['index_x','index_y']) \n",
    "ASSETS = ['valid_finassets','finassets', 'finassets_it' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate treatment and control groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate treatment group:\n",
    "fs08['treat1'] = 0 \n",
    "fs08.loc[fs08['rbtamt'].notna(),'treat1'] = 1 #all entries with actual info on rebate are in the treatment group\n",
    "\n",
    "#three different control groups:\n",
    "#control group 1: those who didn't receive the rebate in the given month\n",
    "fs08['cont1'] = 0\n",
    "fs08.loc[fs08['rbtamt'].isna(), 'cont1'] = 1\n",
    "\n",
    "#control group 2: drop one time period after receipt of rebate, as part of rebate might've been consumed one time period after\n",
    "fs08['cont2'] = 0 \n",
    "fs08.loc[(fs08['rbtamt'].isna())&(fs08['last_rbtamt'].isna()),'cont2'] = 1\n",
    "#treatment 2: all individuals who received a rebate last time period. This group should be compared to control group 2 only\n",
    "fs08['treat2'] = 0\n",
    "fs08.loc[(fs08['cont2']==0) & (fs08['treat1']==0),'treat2'] = 1\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt'].isna()), 'treat2'] = 1\n",
    "\n",
    "#treatment 3: long run consumption response: all individuals who received a rebate in this period and where the rebate interview is not the last\n",
    "fs08['treat3'] = 0\n",
    "fs08.loc[(fs08['rbtamt']>0) & (fs08[FUTCONS[0]].notna()), 'treat3'] = 1\n",
    "\n",
    "#control 4: long-run consumption: those who haven't received a rebate two periods from current period and have information on consumption for next period as well\n",
    "fs08['cont4'] = 0\n",
    "fs08.loc[(fs08['cont2']==1) & (fs08[FUTCONS[0]].notna()),'cont4'] = 1\n",
    "\n",
    "#control 3: those who haven't received the rebate yet\n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[fs08['rbtamt']>0,'rbt_flag'] = 1\n",
    "fs08['cont3'] = 0 \n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('cumsum') #starts counting from the point on which the first rebate was received\n",
    "fs08.loc[(fs08['rbtamt'].isna()) & (fs08['rbt_flag']==0),'cont3'] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations based on z-score lower than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08_cap = fs08[(np.abs(stats.zscore(fs08.loc[:,CONS+LRUNCONS])) < 3).all(axis=1)] #drop outliers\n",
    "fs08_cap = fs08_cap.loc[fs08_cap['FD']>0] #there are still two observations where food consumption is zero; drop bc of common sense\n",
    "fs08_cap = fs08_cap.loc[fs08['ND']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08_cap.to_csv( os.getcwd() + '\\\\fs08_cap.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: Machine learning approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Define sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Run random forest algorithm seperately for treatment and control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #'educ_bachelor','educ_master','educ_doctorate'\n",
    "DEMO = ['age', 'adults', 'PERSLT18','CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5', 'MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'] # exclude 'FINCBTAX',\n",
    "DEMO2 = [ 'FINCBTXM'] # exclude 'FSALARYM',\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum','qescrowx_sum', 'timeleft'] # 'orgmrtx_sum', \n",
    "CONS = ['FD', 'SND', 'ND', 'TOT']\n",
    "CONT = ['cont1', 'cont2', 'cont3']\n",
    "TREAT = 'treat1'\n",
    "treatgroup = TREAT\n",
    "trees = 1000\n",
    "\n",
    "#Random Forest for short term consumption: treatment group 1 with imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC + ['finassets_it']  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    rf[TREAT+'_id'] = fs08_cap.loc[(fs08_cap[treatgroup]==1),['CustID','NEWID']]\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "        rf[con+'_id'] = fs08_cap.loc[fs08_cap[con]==1, ['CustID','NEWID']]\n",
    "    for i in list(set([rf.split('_')[0] for rf in list(rf)])):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_finit.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    rf[TREAT+'_id'] = fs08_cap.loc[(fs08_cap[treatgroup]==1),['CustID','NEWID']]\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "        rf[con+'_id'] = fs08_cap.loc[fs08_cap[con]==1, ['CustID','NEWID']]        \n",
    "    for i in list(set([rf.split('_')[0] for rf in list(rf)])):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_nofin.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group with just the observations where financial assets are included\n",
    "TREAT = 'treat1'\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + ['finassets'] + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1) & (fs08_cap['valid_finassets']==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    rf[TREAT+'_id'] = fs08_cap.loc[(fs08_cap[treatgroup]==1) & (fs08_cap['valid_finassets']==1),['CustID','NEWID']]\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[(fs08_cap[con]==1) & (fs08_cap['valid_finassets']==1) , [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "        rf[con+'_id'] = fs08_cap.loc[(fs08_cap[con]==1) &(fs08_cap['valid_finassets']==1), ['CustID','NEWID']]        \n",
    "    for i in list(set([rf.split('_')[0] for rf in list(rf)])):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_fin.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment with treatment group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl' if i[-8:]=='diff.pkl']\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(rfdicts)\n",
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #'educ_bachelor','educ_master','educ_doctorate'\n",
    "DEMO = ['age', 'adults', 'PERSLT18','CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5', 'MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'] # exclude 'FINCBTAX',\n",
    "DEMO2 = [ 'FINCBTXM'] # exclude 'FSALARYM',\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum','qescrowx_sum', 'timeleft'] # 'orgmrtx_sum', \n",
    "CONS = ['FD', 'SND', 'ND', 'TOT']\n",
    "TREAT = 'treat2'\n",
    "treatgroup = TREAT\n",
    "trees = 1000\n",
    "\n",
    "#include finassets only\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + ['finassets'] + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "\n",
    "for c in CONS:\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1) & (fs08_cap['valid_finassets']==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rfdicts[f'{c}_fin'][TREAT] = treat\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_id'] = fs08_cap.loc[(fs08_cap[treatgroup]==1) & (fs08_cap['valid_finassets']==1),['CustID','NEWID']]\n",
    "    y = np.array(rfdicts[f'{c}_fin'][TREAT][depvar]) #array for dependent variable\n",
    "    X = np.array(rfdicts[f'{c}_fin'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "    X_labels = list(rfdicts[f'{c}_fin'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_X_labels'] = X_labels #save in dict\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_rbtamt'] = np.array(rfdicts[f'{c}_fin'][TREAT]['rbtamt_idmean'])\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_fin.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[f'{c}_fin'], output)\n",
    "    output.close()\n",
    "    \n",
    "#Random Forest for short term consumption: treatment group without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "for c in CONS:\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rfdicts[f'{c}_nofin'][TREAT] = treat\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_id'] = fs08_cap.loc[(fs08_cap[treatgroup]==1),['CustID','NEWID']]\n",
    "    y = np.array(rfdicts[f'{c}_nofin'][TREAT][depvar]) #array for dependent variable\n",
    "    X = np.array(rfdicts[f'{c}_nofin'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "    X_labels = list(rfdicts[f'{c}_nofin'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_X_labels'] = X_labels #save in dict\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_rbtamt'] = np.array(rfdicts[f'{c}_nofin'][TREAT]['rbtamt_idmean'])\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_nofin.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[f'{c}_nofin'], output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC + ['finassets_it']  #define explanatory variables + ['finassets_it'] \n",
    "for c in CONS:\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rfdicts[f'{c}_finit'][TREAT] = treat\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_id'] = fs08_cap.loc[(fs08_cap[treatgroup]==1),['CustID','NEWID']]\n",
    "    y = np.array(rfdicts[f'{c}_finit'][TREAT][depvar]) #array for dependent variable\n",
    "    X = np.array(rfdicts[f'{c}_finit'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "    X_labels = list(rfdicts[f'{c}_finit'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_X_labels'] = X_labels #save in dict\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_rbtamt'] = np.array(rfdicts[f'{c}_finit'][TREAT]['rbtamt_idmean'])\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_finit.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[f'{c}_finit'], output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Predict Outcomes for overall consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_predicitons_rbt(rf_treat, rf_cont, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if (type(X_cont) is not np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_treat_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('X_treat_rbamt needs to have an array like structure')\n",
    "            else:\n",
    "                X_temp = X_treat.copy()\n",
    "                rbtamt_temp = X_treat_rbtamt.copy()\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_cont_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('if X_cont is specified, X_cont_rbamt needs to have an array like structure')\n",
    "            if sorted(feature_ids_treat)!=sorted(feature_ids_cont):\n",
    "                raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont')\n",
    "            elif (len(feature_ids_treat)==0) | (len(feature_ids_cont)==0):\n",
    "                raise ValueError(f'if X_treat and X_cont are specified, feature_ids must not be empty')\n",
    "            else:\n",
    "                #stack treatment and control groups on top of each other\n",
    "                X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                #stack rebate for each household on top of each other\n",
    "                rbtamt_temp = pd.concat([pd.DataFrame(X_treat_rbtamt), pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                X_labels = list(X.columns)\n",
    "                X_temp = np.array(X)\n",
    "                rbtamt_temp = np.array(rbtamt_temp)\n",
    "        else: \n",
    "            raise ValueError('X_treat does not have an array like structure')\n",
    "        y = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp)) #for each household predict consumption response\n",
    "        mpc = y/rbtamt_temp[:,0] #calculate fraction of rebate that was consumed\n",
    "        return y,mpc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl' if i.split('_')[0]!='lrun' if i[-8:]=='diff.pkl']\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate consumption response for each household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FD_finit', 'FD_nofin', 'ND_finit', 'ND_nofin', 'SND_finit', 'SND_nofin', 'TOT_finit', 'TOT_nofin']\n",
      "FD_finit:\n",
      "cont2_treat3_y_pred (10847,)\n",
      "cont2_treat3_mpc_pred (10847,)\n",
      "cont2_treat3_id (10847, 2)\n",
      "FD_nofin:\n",
      "cont2_treat3_y_pred (10847,)\n",
      "cont2_treat3_mpc_pred (10847,)\n",
      "cont2_treat3_id (10847, 2)\n",
      "ND_finit:\n",
      "cont2_treat3_y_pred (10847,)\n",
      "cont2_treat3_mpc_pred (10847,)\n",
      "cont2_treat3_id (10847, 2)\n",
      "ND_nofin:\n",
      "cont2_treat3_y_pred (10847,)\n",
      "cont2_treat3_mpc_pred (10847,)\n",
      "cont2_treat3_id (10847, 2)\n",
      "SND_finit:\n",
      "cont2_treat3_y_pred (10847,)\n",
      "cont2_treat3_mpc_pred (10847,)\n",
      "cont2_treat3_id (10847, 2)\n",
      "SND_nofin:\n",
      "cont2_treat3_y_pred (10847,)\n",
      "cont2_treat3_mpc_pred (10847,)\n",
      "cont2_treat3_id (10847, 2)\n",
      "TOT_finit:\n",
      "cont2_treat3_y_pred (10847,)\n",
      "cont2_treat3_mpc_pred (10847,)\n",
      "cont2_treat3_id (10847, 2)\n",
      "TOT_nofin:\n",
      "cont2_treat3_y_pred (10847,)\n",
      "cont2_treat3_mpc_pred (10847,)\n",
      "cont2_treat3_id (10847, 2)\n"
     ]
    }
   ],
   "source": [
    "rfdicts_keys = list(rfdicts)\n",
    "rfdicts_keys = [k for k in rfdicts_keys if k.split('_')[1]!='fin']\n",
    "print(rfdicts_keys)\n",
    "\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    cont = ['cont2']\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    treat = ['treat3']\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\condistr\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for t in treat:\n",
    "        for c in cont:\n",
    "            if (t=='treat2') & (c=='cont1'):\n",
    "                pass\n",
    "            else:\n",
    "                y,mpc = uplift_predicitons_rbt(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                              rfdicts[k][c+'_X'],rfdicts[k][c+'_rbtamt'],rfdicts[k][t+'_X_labels'],rfdicts[k][c+'_X_labels'])\n",
    "                rfdicts[k][f'{c}_{t}_y_pred'] = y\n",
    "                print(f'{c}_{t}_y_pred', y.shape)\n",
    "                rfdicts[k][f'{c}_{t}_mpc_pred'] = mpc\n",
    "                print(f'{c}_{t}_mpc_pred', mpc.shape)\n",
    "                rfdicts[k][f'{c}_{t}_id'] = pd.concat([pd.DataFrame(rfdicts[k][t+'_id']), pd.DataFrame(rfdicts[k][c+'_id'])], join = 'inner', ignore_index=True)\n",
    "                print(f'{c}_{t}_id',rfdicts[k][f'{c}_{t}_id'].shape)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS regression to assess adequacy of consumption predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl' if i.split('_')[0]!='lrun' if i[-8:]=='diff.pkl']\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_keys = [k for k in list(rfdicts) if k.split('_')[1]=='nofin'] #do OLS regression for 'nofin' specification\n",
    "\n",
    "c='cont1' #control group\n",
    "t='treat1' #treatment group\n",
    "for i,k in enumerate(ols_keys): #different consumption responses\n",
    "    if i==0:\n",
    "        cr = pd.DataFrame(rfdicts[k][f'{c}_{t}_y_pred'],columns=[f'{c}_{t}_cr_{k}']) #capture previously estimated cr for specification\n",
    "        mpc = pd.DataFrame(rfdicts[k][f'{c}_{t}_mpc_pred'],columns=[f'{c}_{t}_mpc_{k}'])\n",
    "        identifier = rfdicts[k][f'{c}_{t}_id'] #identifier, same order as consumption responses\n",
    "        cr_pred = identifier.join(cr) #join column-wise\n",
    "        cr_pred = cr_pred.join(mpc)\n",
    "    else:\n",
    "        cr = pd.DataFrame(rfdicts[k][f'{c}_{t}_y_pred'],columns=[f'{c}_{t}_cr_{k}'])\n",
    "        mpc = pd.DataFrame(rfdicts[k][f'{c}_{t}_mpc_pred'],columns=[f'{c}_{t}_mpc_{k}'])\n",
    "        identifier = rfdicts[k][f'{c}_{t}_id']\n",
    "        cr = identifier.join(cr)\n",
    "        cr = cr.join(mpc)\n",
    "        cr_pred = pd.merge(cr_pred,cr,how='inner',on=['NEWID','CustID'])\n",
    "\n",
    "#display(cr_pred)\n",
    "cr_pred = pd.merge(cr_pred,fs08_cap,how='inner', on=['NEWID','CustID']) #merge with data frame\n",
    "cr_pred['const'] = 1 #add constant\n",
    "cr_pred = pd.get_dummies(cr_pred, columns=['QINTRVMO'], drop_first=True) #change month variables to dummy variables\n",
    "cr_pred['famsize'] = cr_pred['adults'] + cr_pred['PERSLT18'] #generate size of familiy\n",
    "cr_pred['last_famsize'] = cr_pred.sort_values(by=['CustID','NEWID']).groupby('CustID')['famsize'].shift(1) #lag of family size\n",
    "cr_pred['chg_famsize'] = cr_pred['famsize']-cr_pred['last_famsize'] #change in family size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    10438.000000\n",
      "mean         0.129593\n",
      "std          1.277173\n",
      "min        -24.947781\n",
      "25%         -0.064542\n",
      "50%          0.096509\n",
      "75%          0.274849\n",
      "max         86.403799\n",
      "Name: cont1_treat1_mpc_FD_nofin, dtype: float64\n",
      "count    3593.000000\n",
      "mean       -0.225020\n",
      "std         0.546226\n",
      "min       -24.947781\n",
      "25%        -0.270372\n",
      "50%        -0.133573\n",
      "75%        -0.059165\n",
      "max        -0.000024\n",
      "Name: cont1_treat1_mpc_FD_nofin, dtype: float64\n",
      "count    10438.000000\n",
      "mean         0.461770\n",
      "std          4.121496\n",
      "min        -45.904246\n",
      "25%          0.022669\n",
      "50%          0.313534\n",
      "75%          0.662818\n",
      "max        276.281006\n",
      "Name: cont1_treat1_mpc_SND_nofin, dtype: float64\n",
      "count    2450.000000\n",
      "mean       -0.380816\n",
      "std         1.080807\n",
      "min       -45.904246\n",
      "25%        -0.434783\n",
      "50%        -0.213544\n",
      "75%        -0.095409\n",
      "max        -0.000051\n",
      "Name: cont1_treat1_mpc_SND_nofin, dtype: float64\n",
      "count    10438.000000\n",
      "mean         0.486516\n",
      "std          4.722224\n",
      "min        -56.493835\n",
      "25%         -0.060694\n",
      "50%          0.316067\n",
      "75%          0.745750\n",
      "max        303.298954\n",
      "Name: cont1_treat1_mpc_ND_nofin, dtype: float64\n",
      "count    2994.000000\n",
      "mean       -0.481665\n",
      "std         1.220531\n",
      "min       -56.493835\n",
      "25%        -0.570803\n",
      "50%        -0.291523\n",
      "75%        -0.130225\n",
      "max        -0.000003\n",
      "Name: cont1_treat1_mpc_ND_nofin, dtype: float64\n",
      "count    10438.000000\n",
      "mean         0.636994\n",
      "std          4.883358\n",
      "min       -102.253803\n",
      "25%         -0.217371\n",
      "50%          0.435672\n",
      "75%          1.157090\n",
      "max        296.561846\n",
      "Name: cont1_treat1_mpc_TOT_nofin, dtype: float64\n",
      "count    3418.000000\n",
      "mean       -0.902188\n",
      "std         2.228825\n",
      "min      -102.253803\n",
      "25%        -1.068602\n",
      "50%        -0.534429\n",
      "75%        -0.232835\n",
      "max        -0.000591\n",
      "Name: cont1_treat1_mpc_TOT_nofin, dtype: float64\n",
      "\\begin{table}\n",
      "\\caption{}\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lcccc}\n",
      "\\hline\n",
      "                 &     FD     &    SND     &     ND     &    TOT      \\\\\n",
      "\\midrule\n",
      "const            & -154.28*** & -384.19*** & -394.25*** & -769.32***  \\\\\n",
      "                 & (31.50)    & (59.57)    & (72.00)    & (154.72)    \\\\\n",
      "rbtamt           & 0.05**     & 0.12**     & 0.15***    & 0.33***     \\\\\n",
      "                 & (0.03)     & (0.05)     & (0.06)     & (0.11)      \\\\\n",
      "age              & 0.66*      & 0.74       & 1.80**     & 2.60        \\\\\n",
      "                 & (0.35)     & (0.66)     & (0.82)     & (1.89)      \\\\\n",
      "chg\\_famsize     & 16.22      & 99.66      & 94.44      & 100.53      \\\\\n",
      "                 & (32.56)    & (62.28)    & (77.41)    & (162.64)    \\\\\n",
      "QINTRVMO\\_2      & 88.04**    & 180.37***  & 171.48**   & 331.15*     \\\\\n",
      "                 & (36.04)    & (67.01)    & (82.01)    & (174.22)    \\\\\n",
      "QINTRVMO\\_3      & 99.34***   & 410.30***  & 429.95***  & 659.94***   \\\\\n",
      "                 & (34.79)    & (61.68)    & (79.11)    & (170.81)    \\\\\n",
      "QINTRVMO\\_4      & 109.49**   & 535.60***  & 388.77***  & 137.47      \\\\\n",
      "                 & (46.18)    & (74.91)    & (96.96)    & (207.53)    \\\\\n",
      "QINTRVMO\\_5      & 197.55***  & 651.25***  & 533.50***  & 689.44***   \\\\\n",
      "                 & (41.81)    & (69.45)    & (87.09)    & (184.82)    \\\\\n",
      "QINTRVMO\\_6      & 133.83***  & 441.80***  & 352.23***  & 255.96      \\\\\n",
      "                 & (41.00)    & (72.21)    & (90.13)    & (187.87)    \\\\\n",
      "QINTRVMO\\_7      & 152.10***  & 436.50***  & 372.15***  & 528.18***   \\\\\n",
      "                 & (42.49)    & (74.65)    & (94.80)    & (191.27)    \\\\\n",
      "QINTRVMO\\_8      & 101.63**   & 344.01***  & 293.76***  & 366.91*     \\\\\n",
      "                 & (42.97)    & (75.85)    & (95.62)    & (195.89)    \\\\\n",
      "QINTRVMO\\_9      & 116.15***  & 345.38***  & 322.71***  & 567.73***   \\\\\n",
      "                 & (39.59)    & (74.54)    & (93.44)    & (186.66)    \\\\\n",
      "QINTRVMO\\_10     & 40.19      & 203.07***  & 175.86*    & 423.80**    \\\\\n",
      "                 & (43.25)    & (74.08)    & (95.79)    & (207.06)    \\\\\n",
      "QINTRVMO\\_11     & 39.07      & 119.62*    & 75.67      & 176.26      \\\\\n",
      "                 & (38.09)    & (68.33)    & (85.40)    & (180.89)    \\\\\n",
      "QINTRVMO\\_12     & 17.56      & -60.45     & -212.91*** & -578.92***  \\\\\n",
      "                 & (35.36)    & (64.73)    & (81.47)    & (178.11)    \\\\\n",
      "R-squared        & 0.01       & 0.02       & 0.02       & 0.01        \\\\\n",
      "No. observations & 10438      & 10438      & 10438      & 10438       \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\\end{table}\n",
      "\\begin{table}\n",
      "\\caption{}\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lcccc}\n",
      "\\hline\n",
      "                 &     FD    &    SND    &     ND    &    TOT      \\\\\n",
      "\\midrule\n",
      "const            & 83.75     & -59.86    & 27.82     & 126.94      \\\\\n",
      "                 & (66.83)   & (163.34)  & (167.18)  & (323.26)    \\\\\n",
      "rbtamt           & -0.39***  & -0.76***  & -0.77***  & -1.16***    \\\\\n",
      "                 & (0.04)    & (0.07)    & (0.09)    & (0.18)      \\\\\n",
      "age              & -0.75     & -0.55     & -0.10     & -4.83       \\\\\n",
      "                 & (0.76)    & (1.83)    & (2.05)    & (4.01)      \\\\\n",
      "chg\\_famsize     & -95.71*   & -98.67    & 94.13     & 136.66      \\\\\n",
      "                 & (56.83)   & (169.32)  & (155.28)  & (336.35)    \\\\\n",
      "QINTRVMO\\_2      & 51.88     & 90.61     & 172.25    & 343.48      \\\\\n",
      "                 & (75.26)   & (198.83)  & (198.14)  & (377.16)    \\\\\n",
      "QINTRVMO\\_3      & 54.95     & 427.77**  & 416.88**  & 874.45**    \\\\\n",
      "                 & (74.34)   & (174.26)  & (188.66)  & (357.63)    \\\\\n",
      "QINTRVMO\\_4      & 89.17     & 443.55**  & 404.85*   & -214.55     \\\\\n",
      "                 & (98.51)   & (203.57)  & (220.43)  & (414.16)    \\\\\n",
      "QINTRVMO\\_5      & 202.53**  & 842.53*** & 471.24**  & 724.38*     \\\\\n",
      "                 & (95.07)   & (186.69)  & (205.68)  & (407.65)    \\\\\n",
      "QINTRVMO\\_6      & -32.99    & 155.94    & -24.19    & -297.94     \\\\\n",
      "                 & (81.75)   & (180.80)  & (198.39)  & (357.24)    \\\\\n",
      "QINTRVMO\\_7      & 104.82    & 440.96**  & 190.50    & 325.31      \\\\\n",
      "                 & (78.69)   & (176.08)  & (189.91)  & (399.19)    \\\\\n",
      "QINTRVMO\\_8      & -70.15    & 96.21     & -79.37    & 68.23       \\\\\n",
      "                 & (79.85)   & (182.55)  & (195.39)  & (347.16)    \\\\\n",
      "QINTRVMO\\_9      & 145.16*   & 513.41*** & 489.91**  & 676.23*     \\\\\n",
      "                 & (74.36)   & (174.11)  & (196.91)  & (375.63)    \\\\\n",
      "QINTRVMO\\_10     & 110.58    & 594.42*** & 427.72**  & 974.29**    \\\\\n",
      "                 & (83.32)   & (188.72)  & (205.65)  & (400.02)    \\\\\n",
      "QINTRVMO\\_11     & 254.56*** & 815.77*** & 679.43*** & 1065.46***  \\\\\n",
      "                 & (74.97)   & (182.11)  & (195.81)  & (392.90)    \\\\\n",
      "QINTRVMO\\_12     & 38.50     & 182.15    & 97.19     & -510.77     \\\\\n",
      "                 & (74.34)   & (170.94)  & (185.45)  & (399.52)    \\\\\n",
      "R-squared        & 0.06      & 0.08      & 0.06      & 0.03        \\\\\n",
      "No. observations & 3593      & 2450      & 2994      & 3418        \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "#required packages\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "\n",
    "time = ['QINTRVMO' + f'_{j}' for j in list(range(2,13))] #get name of time dummies\n",
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #'educ_bachelor','educ_master','educ_doctorate'\n",
    "DEMO = ['age', 'adults', 'PERSLT18'] # exclude 'FINCBTAX',\n",
    "housing = ['CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5'] #'CUTENURE_1', \n",
    "marital = ['MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'] #, 'MARITAL1_1', \n",
    "DEMO2 = [ 'FINCBTXM'] # exclude 'FSALARYM',\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum','qescrowx_sum', 'timeleft'] # 'orgmrtx_sum', \n",
    "CONS = ['FD', 'SND', 'ND', 'TOT']\n",
    "LRUNCONS = ['lrun_' + c for c in CONS]\n",
    "CHGCONS = ['chg_'+ c for c in CONS]\n",
    "#explanatory variables for OLS\n",
    "expvars1 = ['const','rbtamt'] + DEMO + DEMO2 + MORTGAGE + EDUC + housing + marital + time #define explanatory variables + ['finassets_it'] \n",
    "expvars2 = ['const','rbtamt','age','chg_famsize'] + time #in line with Johnson et al. (2013)\n",
    "cr_pred.loc[cr_pred['rbtamt'].isna(),'rbtamt'] = 0 #change rebate amount from missing to zero\n",
    "\n",
    "reg_dict = dict()\n",
    "for c in ['FD','SND','ND','TOT']:\n",
    "    reg_full = cr_pred.loc[(cr_pred[f'chg_{c}'].notna())&(cr_pred['chg_famsize'].notna()),:] #drop missing values\n",
    "    reg_neg = reg_full.loc[cr_pred[f'cont1_treat1_cr_{c}_nofin']<0,:] #subsample of striclty negative consumption responses\n",
    "    # Estimate OLS regression for each set of variables, cluster on household level\n",
    "    reg1 = sm.OLS(reg_full[f'chg_{c}'], reg_full[expvars2], missing='drop').fit(cov_type='cluster', cov_kwds={'groups': reg_full['CustID']})\n",
    "    reg2 = sm.OLS(reg_neg[f'chg_{c}'], reg_neg[expvars2], missing='drop').fit(cov_type='cluster', cov_kwds={'groups': reg_neg['CustID']})\n",
    "    reg_dict[f'reg_full_{c}'] = reg1\n",
    "    reg_dict[f'reg_neg_{c}'] = reg2\n",
    "\n",
    "reg_dict_keys = list(reg_dict)\n",
    "#list of different regression estimators for whole sample and negative sample\n",
    "reg_list_full = [reg_dict[reg_dict_keys[j]] for j in list(range(len(reg_dict_keys))) if reg_dict_keys[j].split('_')[1]=='full']\n",
    "reg_list_neg = [reg_dict[reg_dict_keys[j]] for j in list(range(len(reg_dict_keys))) if reg_dict_keys[j].split('_')[1]=='neg']\n",
    "\n",
    "#print(model_names)\n",
    "info_dict={'R-squared' : lambda x: f\"{x.rsquared:.2f}\",\n",
    "           'No. observations' : lambda x: f\"{int(x.nobs):d}\"}\n",
    "#generate latex tables for table 7\n",
    "for l in [reg_list_full,reg_list_neg]:\n",
    "    results_table = summary_col(results=l,\n",
    "                                float_format='%0.2f',\n",
    "                                stars = True,\n",
    "                                model_names=['FD','SND','ND','TOT'],\n",
    "                                info_dict=info_dict)\n",
    "                                #regressor_order=[expvars2])\n",
    "    print(results_table.as_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get combination of treat1 and treat2-based consumption predicitons for section 6.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD\n",
      "FD\n",
      "CustID                2634\n",
      "NEWID_cont2_treat1    2634\n",
      "NEWID_cont2_treat2    2634\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "fintype = ['nofin','finit']\n",
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "cons = list(set([k.split('_')[0] for k in rfdicts_keys]))\n",
    "spec = list(set([k.split('_')[0] for k in rfdicts_keys]))\n",
    "c = 'cont1'\n",
    "t = 'treat1'\n",
    "cumcons = dict()\n",
    "for c in ['cont2']:\n",
    "    for t in ['treat1','treat2']:\n",
    "        if (c=='cont1')&(t=='treat2'):\n",
    "            pass\n",
    "        else:\n",
    "            rfdicts_subkeys = [k for k in rfdicts_keys if k.split('_')[1]!='fin' ]\n",
    "\n",
    "            for i,k in enumerate(rfdicts_subkeys):\n",
    "                cr = rfdicts[k][f'{c}_{t}_y_pred']\n",
    "                mpc = rfdicts[k][f'{c}_{t}_mpc_pred']\n",
    "                identifier = rfdicts[k][f'{c}_{t}_id'] \n",
    "                if i==0:\n",
    "                    print(k.split('_')[0])\n",
    "                    id_pred = identifier\n",
    "                    cr_pred = pd.DataFrame(cr, columns=[t+'_cr_'+k.split('_')[0]+'_'+k.split('_')[1]])\n",
    "                    cr_pred = identifier.join(cr_pred)\n",
    "                    mpc_pred = pd.DataFrame(mpc, columns=[t+'_mpc_'+k.split('_')[0]+'_'+k.split('_')[1]])\n",
    "                    mpc_pred = identifier.join(mpc_pred)\n",
    "                    #longterm = cr_pred.round(1).join(mpc_pred.round(3))\n",
    "                else:\n",
    "                    cr_pred = cr_pred.join(pd.DataFrame(cr, columns=[t+'_cr_'+k.split('_')[0]+'_'+k.split('_')[1]]))\n",
    "                    #longterm = longterm.join(pd.DataFrame(cr, columns=['cr_'+k.split('_')[0]+'_'+k.split('_')[1]]).describe().round(1))\n",
    "                    #display(cr_pred.head())\n",
    "                    mpc_pred = mpc_pred.join(pd.DataFrame(mpc, columns=[t+'_mpc_'+k.split('_')[0]+'_'+k.split('_')[1]]))\n",
    "                    #longterm = longterm.join(pd.DataFrame(mpc, columns=['mpc_'+k.split('_')[0]+'_'+k.split('_')[1]]).describe().round(3))\n",
    "                   #display(mpc_pred)\n",
    "            cr_pred['countunique'] = 1\n",
    "            cr_pred['countunique'] = cr_pred.groupby('NEWID')['countunique'].transform('cumsum')\n",
    "            cumcons[f'{t}_cr'] = cr_pred\n",
    "            cumcons[f'{t}_mpc'] = mpc_pred\n",
    "\n",
    "#generade idmerge, doesn't matter which consumption categoriy and specification to use; they are the same across categories\n",
    "mergeid = pd.merge(rfdicts['FD_finit']['treat1_id'],rfdicts['FD_finit']['treat2_id'], how='inner',on='CustID') \n",
    "mergeid = mergeid.rename(columns={'NEWID_x': 'NEWID_cont2_treat1', 'NEWID_y': 'NEWID_cont2_treat2'})\n",
    "mergeid['subs_period'] = mergeid['NEWID_cont2_treat2'] - mergeid['NEWID_cont2_treat1'] #generate var that captures subsequent interview periods\n",
    "mergeid.loc[mergeid['subs_period']==0]\n",
    "mergeid = mergeid.loc[mergeid['subs_period']==1] #keep hhs with subsequent periods\n",
    "mergeid = mergeid.drop(columns=['subs_period']) \n",
    "print(mergeid.count())\n",
    "mergeid = mergeid.append(rfdicts['FD_finit']['cont2_id'], sort=True)\n",
    "mergeid.loc[mergeid['NEWID_cont2_treat1'].isna(),'NEWID_cont2_treat1'] = mergeid.loc[mergeid['NEWID_cont2_treat1'].isna(),'NEWID']\n",
    "mergeid.loc[mergeid['NEWID_cont2_treat2'].isna(),'NEWID_cont2_treat2'] = mergeid.loc[mergeid['NEWID_cont2_treat2'].isna(),'NEWID']\n",
    "\n",
    "#print(mergeid['NEWID_cont2_treat1'].nunique())\n",
    "#print(mergeid['NEWID_cont2_treat2'].nunique())\n",
    "#print(mergeid['CustID'].count())\n",
    "mergeid = mergeid.drop(columns=['NEWID',])\n",
    "\n",
    "#merge with predicted values\n",
    "cr_cumcons = pd.merge(mergeid, cumcons['treat1_cr'], left_on=['NEWID_cont2_treat1','CustID'],right_on=['NEWID','CustID'])\n",
    "cr_cumcons= pd.merge(cr_cumcons,cumcons['treat2_cr'], left_on=['NEWID_cont2_treat2'], right_on=['NEWID'], how='left' )\n",
    "mpc_cumcons = pd.merge(mergeid, cumcons['treat1_mpc'], left_on=['NEWID_cont2_treat1','CustID'],right_on=['NEWID','CustID'])\n",
    "mpc_cumcons= pd.merge(mpc_cumcons,cumcons['treat2_mpc'], left_on=['NEWID_cont2_treat2'], right_on=['NEWID'], how='left' )\n",
    "\n",
    "for c in ['FD','SND','ND','TOT']:\n",
    "    for spec in ['finit','nofin']:\n",
    "        cr_cumcons[f'cumcons_cr_{c}_{spec}'] = cr_cumcons[[f'treat1_cr_{c}_{spec}',f'treat2_cr_{c}_{spec}']].sum(axis = 1)\n",
    "        #plt.scatter(cr_cumcons[[f'treat1_cr_{c}_{spec}']],cr_cumcons[[f'treat2_cr_{c}_{spec}']])\n",
    "        mpc_cumcons[f'cumcons_mpc_{c}_{spec}'] = mpc_cumcons[[f'treat1_mpc_{c}_{spec}',f'treat2_mpc_{c}_{spec}']].sum(axis = 1)\n",
    "\n",
    "cr_cumcons.to_csv(f'{os.getcwd()}\\\\condistr\\\\cr_cumcons.csv') #save dataframe\n",
    "mpc_cumcons.to_csv(f'{os.getcwd()}\\\\condistr\\\\mpc_cumcons.csv') #save dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Partial dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for uplift 2model partial dependency for numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_num_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[], types=['mean'], percentile='none', grid_lower=5, grid_upper=95 ): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor in a rf uplift 2 model approach if the values are numerical.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    Dataframe of:\n",
    "    1. grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    2. the corresposing types such as mean, median, etc.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is int):\n",
    "                if f_id > (X_temp.shape[1]-1):\n",
    "                    raise ValueError(f'positional number of {f_id} exceeds array shape')\n",
    "                else:\n",
    "                    X_temp = X_treat.copy()\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str):\n",
    "                if f_id not in feature_ids:\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids is not passed to the function')\n",
    "                else:\n",
    "                    f_id = feature_ids.index(f_id)\n",
    "                    f_id_label = f_id\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('f_id needs to be either an integer or a string')\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(f_id) is int):\n",
    "                raise ValueError(f'if X_cont is specified, then f_id needs to be a string variable')\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str) & ((f_id not in feature_ids_treat) |  (f_id not in feature_ids_cont)):\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids_treat or feature_ids_cont is not passed to the function')\n",
    "            else:\n",
    "                if (sorted(feature_ids_treat)!=sorted(feature_ids_cont)) & (type(X_cont_rbtamt) is not np.ndarray) :\n",
    "                    raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                else:\n",
    "                    X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_treat), pd.DataFrame(X_cont, columns = feature_ids_cont)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = np.array(mean_rbt)\n",
    "                    X_labels = list(X.columns)\n",
    "                    X_temp = np.array(X)\n",
    "                    f_id_label = f_id\n",
    "                    f_id = X_labels.index(f_id)             \n",
    "        else: \n",
    "            raise ValueError('Either X_cont or X_treat does not have an array like structure')\n",
    "\n",
    "                       #['age', 'adults', 'PERSLT18'\n",
    "        X_unique = np.array(list(set(X_temp[:, f_id])))\n",
    "        if len(X_unique)*3 > 100:\n",
    "            grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower), np.percentile(X_temp[:, f_id], grid_upper), 100)\n",
    "        else:\n",
    "            grid = np.linspace(np.percentile(X_temp[:, f_id],grid_lower),np.percentile(X_temp[:, f_id],grid_upper), len(X_unique)*2)\n",
    "       \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types.copy()\n",
    "        #column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "\n",
    "        if set(types) <= set(function_labels): #given types must be of possible types median, mean, percentile, std\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i] #fill functions list with corresponding numpy functions\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif type(percentile) is not list: # 0<=percentile<=100:\n",
    "                    raise ValueError('percentile must be list')\n",
    "                else:\n",
    "                    column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "                    if len(percentile)>1:\n",
    "                        for p in percentile[1:]:\n",
    "                            types = types + ['percentile'] #pass\n",
    "                            nptypes = nptypes + [np.percentile]\n",
    "                            column_labels = column_labels + ['percentile_' +str(p)] #augment column labels with percentile columns if specified\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "        y_pred = np.zeros((len(grid),len(types)))\n",
    "        mpc_pred = np.zeros((len(grid), len(types)))\n",
    "        p_pos = 0\n",
    "        for i, val in enumerate(grid): # i returns the counter, val returns the value at position of counter on grid \n",
    "            X_temp[:, f_id] = val #replace value with grid\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            p_pos = 0\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[i,j] = nptypes[j](y_temp,percentile[p_pos])\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp,percentile[p_pos])\n",
    "                    p_pos = p_pos + 1\n",
    "                else:\n",
    "                    y_pred[i,j] = nptypes[j](y_temp)\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp)\n",
    "        for j in range(len(column_labels)):\n",
    "            if column_labels[j] == 'percentile':\n",
    "                column_labels[j] = 'percentile_' + str(percentile)\n",
    "            else:\n",
    "                pass \n",
    "        column_labels_cr = ['cr_'+ lab for lab in column_labels]\n",
    "        column_labels_mpc = ['mpc_' + lab for lab in column_labels]\n",
    "        column_labels = ['grid']+column_labels_cr+column_labels_mpc\n",
    "        column_labels = [str(f_id_label)+ '_' + lab for lab in column_labels]\n",
    "        df = pd.DataFrame(np.c_[grid, y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return df #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for uplift 2model partial dependency for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_cat_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, feature_ids_treat, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[],  feature_ids_cont=[], types=['mean'], percentile='none'): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor in a rf uplift 2 model approach if the variables are categorical.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: list of variable names (strings) of categorical variable for which the partial dependence is calculated (necessary).\n",
    "\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to position of the variables in X_treat (necessary). \n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. \n",
    "    Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: list of values that corresponds to the percentiles if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 100].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average and other specified type of the prediction (for the whole sample) between the random forest models of treatment \n",
    "       and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    Dataframe of:\n",
    "    1. grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    2. the corresposing types such as mean, median, etc.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray: #if X_cont is not specified:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is list):\n",
    "                if (len(f_id)<2):\n",
    "                    raise ValueError(f'{f_id} must be a list of hot-encoding features a hence neither empty nor have a length of 1')\n",
    "                else:\n",
    "                    cat = dict()\n",
    "                    positions = []\n",
    "                    for f in f_id:\n",
    "                        if type(f) is not str:\n",
    "                            raise ValueError('features in f_id list must be variable names of string type')\n",
    "                        else:\n",
    "                            if f not in feature_ids_treat:\n",
    "                                raise ValueError(f'categorical variable {f_id} is not a varibale of data frame')                                     \n",
    "                            else:\n",
    "                                cat[f+'_id'] = feature_ids_treat.index(f) #capture position of categorical variable in whole list of explanatory variables\n",
    "                                cat[f+'_id_label'] = f #capture corresponding variable name\n",
    "                                positions = positions + [cat[f+'_id']] #list of positions of correspoding variable names\n",
    "                    f_tuple = list(zip(f_id,positions)) #generate tuple of positions varnames\n",
    "                    f_tuple = sorted(f_tuple, key = lambda x:x[1], reverse = False) #sort by position\n",
    "                    f_id = [i[0] for i in f_tuple]\n",
    "                    positions = [i[1] for i in f_tuple]\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('Either X_treat or f_id is not correctly specified')\n",
    "        elif (type(X_treat) is np.ndarray) & (type(X_cont) is np.ndarray) & (type(f_id) is list):\n",
    "            if (len(f_id)<2):\n",
    "                raise ValueError(f'{f_id} must be a list of hot-encoding features a hence neither empty nor have a length of 1')                            \n",
    "            else:\n",
    "                cat = dict()\n",
    "                positions = []\n",
    "                for f in f_id:\n",
    "                    if type(f) is not str:\n",
    "                        raise ValueError('features in f_id list must be variable names of string type')\n",
    "                    else:\n",
    "                        if (f not in feature_ids_treat) | (f not in feature_ids_cont):\n",
    "                            raise ValueError(f'categorical variable {f_id} is not a varibale of cont or treat data frame')                                     \n",
    "                        elif (sorted(feature_ids_treat)!=sorted(feature_ids_cont)) & (type(X_cont_rbtamt) is not np.ndarray):\n",
    "                            raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                        else:\n",
    "                            cat[f+'_id'] = feature_ids_treat.index(f) #capture position of categorical variable in whole list of explanatory variables\n",
    "                            cat[f+'_id_label'] = f\n",
    "                            #cat[f+'_tuple'] = listzip\n",
    "                            positions = positions + [cat[f+'_id']]\n",
    "                f_tuple = list(zip(f_id,positions))\n",
    "                f_tuple = sorted(f_tuple, key = lambda x:x[1], reverse = False)\n",
    "                f_id = [i[0] for i in f_tuple]\n",
    "                positions = [i[1] for i in f_tuple]\n",
    "                X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_treat), pd.DataFrame(X_cont, columns = feature_ids_cont)],\n",
    "                              join = 'inner', ignore_index=True) #concat treatment and control dataframe\n",
    "                mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True) #concat mean rebate df\n",
    "                mean_rbt = np.array(mean_rbt)\n",
    "                X_labels = list(X.columns)\n",
    "                X_temp = np.array(X)\n",
    "        else:\n",
    "            raise ValueError('X_treat and X_cont (if specified) need to be array types, f_id has to be a list of hot-encoded categorical variables')\n",
    "        \n",
    "        for f in f_id:\n",
    "            #print(np.max(X_temp[:,cat[f+'_id']]))\n",
    "            #print(np.min(X_temp[:,cat[f+'_id']]))\n",
    "            if (np.max(X_temp[:,cat[f+'_id']])!=1) | (np.min(X_temp[:,cat[f+'_id']])!=0):\n",
    "                raise ValueError('hot encoded variable is not of binary classification')\n",
    "            else:\n",
    "                pass\n",
    "        #grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower),\n",
    "        #np.percentile(X_temp[:, f_id], grid_upper),\n",
    "        \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types.copy()\n",
    "        #column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "\n",
    "        if set(types) <= set(function_labels): #given types must be of possible types median, mean, percentile, std\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i] #fill functions list with corresponding numpy functions\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif type(percentile) is not list: # 0<=percentile<=100:\n",
    "                    raise ValueError('percentile must be list')\n",
    "                else:\n",
    "                    column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "                    if len(percentile)>1:\n",
    "                        for p in percentile[1:]:\n",
    "                            types = types + ['percentile'] #pass\n",
    "                            nptypes = nptypes + [np.percentile]\n",
    "                            column_labels = column_labels + ['percentile_' +str(p)] #augment column labels with percentile columns if specified\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "        #y_pred = np.zeros((len(grid),len(types)))\n",
    "        #mpc_pred = np.zeros((len(grid), len(types)))        \n",
    "        y_pred = np.zeros((1, len(types)*len(f_id))) #grid for prediciton\n",
    "        mpc_pred = np.zeros((1, len(types)*len(f_id))) #grid for rebate means\n",
    "        grid_row = np.identity(len(f_id)) #identity matrix of length of categorical variable\n",
    "        k=0\n",
    "        \n",
    "        column_labels_cr=[]\n",
    "        column_labels_mpc=[]\n",
    "        for f in range(len(f_id)): \n",
    "            p_pos = 0\n",
    "            A = np.array([list(grid_row[f]) for _ in range(len(X_temp))]) #replace true values of cat variables with row of identity matrix for all observations\n",
    "            X_temp[:, positions] = A #replace actual values with A \n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp)) #predict\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[0,k] = nptypes[j](y_temp,percentile[p_pos]) #fill row with numpy functions of y_temp\n",
    "                    mpc_pred[0,k] = nptypes[j](mean_rbt_temp,percentile[p_pos])\n",
    "                    column_labels_cr = column_labels_cr + ['cr_'+ f_id[f] +'_'+ types[j] + str(percentile[p_pos])]\n",
    "                    column_labels_mpc= column_labels_mpc + ['mpc_'+ f_id[f] +'_'+ types[j] + str(percentile[p_pos])]\n",
    "                    p_pos = p_pos+1\n",
    "                else:\n",
    "                    y_pred[0,k] = nptypes[j](y_temp)\n",
    "                    mpc_pred[0,k] = nptypes[j](mean_rbt_temp)\n",
    "                    column_labels_cr = column_labels_cr + ['cr_'+ f_id[f] +'_'+ types[j]]\n",
    "                    column_labels_mpc= column_labels_mpc + ['mpc_'+ f_id[f] +'_'+ types[j]]\n",
    "                k = k+1\n",
    "        column_labels = column_labels_cr + column_labels_mpc\n",
    "        df = pd.DataFrame(np.c_[y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return df #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run partial dependence function for given sample and explanatory variables. this may take a while. Hence, save as later as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['treat1', 'cont1', 'cont2', 'cont3', 'treat1_X', 'treat1_X_labels', 'treat1_rbtamt', 'treat1_rf', 'cont1_X', 'cont1_X_labels', 'cont1_rbtamt', 'cont1_rf', 'cont2_X', 'cont2_X_labels', 'cont2_rbtamt', 'cont2_rf', 'cont3_X', 'cont3_X_labels', 'cont3_rbtamt', 'cont3_rf', 'cont2_treat1_y_pred', 'cont2_treat1_mpc_pred', 'cont1_treat1_y_pred', 'cont1_treat1_mpc_pred', 'cont3_treat1_y_pred', 'cont3_treat1_mpc_pred']\n",
      "['FD_fin', 'FD_finit', 'FD_nofin', 'ND_fin', 'ND_finit', 'ND_nofin', 'SND_fin', 'SND_finit', 'SND_nofin', 'TOT_fin', 'TOT_finit', 'TOT_nofin']\n"
     ]
    }
   ],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl' if i[-8:]=='diff.pkl']\n",
    "\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "\n",
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "INCOME = ['FINCBTXM'] #'FINCBTAX','FSALARYM',\n",
    "CONTROL = ['adults', 'PERSLT18']\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'qescrowx_sum', 'timeleft'] #'orgmrtx_sum'\n",
    "CAT = [['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5'],['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'],['educ_nodegree','educ_highschool','educ_higher']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SND_fin', 'SND_finit', 'SND_nofin', 'TOT_fin', 'TOT_finit', 'TOT_nofin']\n",
      "SND_fin:\n",
      "SND_fin treat1 cont1 FINCBTXM\n",
      "SND_fin treat1 cont1 finassets\n",
      "SND_fin treat1 cont1 morgpayment\n",
      "SND_fin treat1 cont1 qblncm1x_sum\n",
      "SND_fin treat1 cont1 qescrowx_sum\n",
      "SND_fin treat1 cont1 timeleft\n",
      "SND_fin treat1 cont1 age\n",
      "SND_fin treat1 cont1 adults\n",
      "SND_fin treat1 cont1 PERSLT18\n",
      "SND_fin treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_fin treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_finit:\n",
      "SND_finit treat1 cont1 FINCBTXM\n",
      "SND_finit treat1 cont1 finassets_it\n",
      "SND_finit treat1 cont1 morgpayment\n",
      "SND_finit treat1 cont1 qblncm1x_sum\n",
      "SND_finit treat1 cont1 qescrowx_sum\n",
      "SND_finit treat1 cont1 timeleft\n",
      "SND_finit treat1 cont1 age\n",
      "SND_finit treat1 cont1 adults\n",
      "SND_finit treat1 cont1 PERSLT18\n",
      "SND_finit treat1 cont1 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_finit treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_finit treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_nofin:\n",
      "SND_nofin treat1 cont1 FINCBTXM\n",
      "SND_nofin treat1 cont1 morgpayment\n",
      "SND_nofin treat1 cont1 qblncm1x_sum\n",
      "SND_nofin treat1 cont1 qescrowx_sum\n",
      "SND_nofin treat1 cont1 timeleft\n",
      "SND_nofin treat1 cont1 age\n",
      "SND_nofin treat1 cont1 adults\n",
      "SND_nofin treat1 cont1 PERSLT18\n",
      "SND_nofin treat1 cont1 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_nofin treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_nofin treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_fin:\n",
      "TOT_fin treat1 cont1 FINCBTXM\n",
      "TOT_fin treat1 cont1 finassets\n",
      "TOT_fin treat1 cont1 morgpayment\n",
      "TOT_fin treat1 cont1 qblncm1x_sum\n",
      "TOT_fin treat1 cont1 qescrowx_sum\n",
      "TOT_fin treat1 cont1 timeleft\n",
      "TOT_fin treat1 cont1 age\n",
      "TOT_fin treat1 cont1 adults\n",
      "TOT_fin treat1 cont1 PERSLT18\n",
      "TOT_fin treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_fin treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_finit:\n",
      "TOT_finit treat1 cont1 FINCBTXM\n",
      "TOT_finit treat1 cont1 finassets_it\n",
      "TOT_finit treat1 cont1 morgpayment\n",
      "TOT_finit treat1 cont1 qblncm1x_sum\n",
      "TOT_finit treat1 cont1 qescrowx_sum\n",
      "TOT_finit treat1 cont1 timeleft\n",
      "TOT_finit treat1 cont1 age\n",
      "TOT_finit treat1 cont1 adults\n",
      "TOT_finit treat1 cont1 PERSLT18\n",
      "TOT_finit treat1 cont1 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_finit treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_finit treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_nofin:\n",
      "TOT_nofin treat1 cont1 FINCBTXM\n",
      "TOT_nofin treat1 cont1 morgpayment\n",
      "TOT_nofin treat1 cont1 qblncm1x_sum\n",
      "TOT_nofin treat1 cont1 qescrowx_sum\n",
      "TOT_nofin treat1 cont1 timeleft\n",
      "TOT_nofin treat1 cont1 age\n",
      "TOT_nofin treat1 cont1 adults\n",
      "TOT_nofin treat1 cont1 PERSLT18\n",
      "TOT_nofin treat1 cont1 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_nofin treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_nofin treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#def vimp_plot_uplift()\n",
    "rfdicts_keys = list(rfdicts)\n",
    "rfdicts_keys = [key for key in rfdicts_keys if (key.split('_')[0]=='SND') | (key.split('_')[0]=='TOT')]\n",
    "#rfdicts_keys = rfdicts_keys[:1]\n",
    "print(rfdicts_keys)\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\pdp\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "    for t in treat:\n",
    "        for c in ['cont1']:\n",
    "            expvars = rfdicts[k][c+'_X_labels']\n",
    "            INCOME = ['FINCBTXM'] #'FINCBTAX','FSALARYM',\n",
    "            if 'finassets' in expvars:\n",
    "                INCOME = INCOME + ['finassets']\n",
    "            if 'finassets_it' in expvars:\n",
    "                INCOME = INCOME + ['finassets_it']\n",
    "            for v in INCOME:\n",
    "                print(k,t,c,v)\n",
    "                if INCOME.index(v)==0: #if this is the first income variable just run function\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                               feature_ids_treat=rfdicts[k][t+'_X_labels'], feature_ids_cont=rfdicts[k][t+'_X_labels'], \n",
    "                                                               types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else: #if this is the second variable, join data frame\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                                        feature_ids_treat=rfdicts[k][t+'_X_labels'],feature_ids_cont=rfdicts[k][t+'_X_labels'], \n",
    "                                                                        types=['mean','percentile','std','median'], percentile=[25,75])) \n",
    "            rfdicts[k][f'{c}_{t}_pdp_INCOME'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_INCOME.csv') #save data frame as csv file\n",
    "            MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'qescrowx_sum', 'timeleft']\n",
    "            for v in MORTGAGE:\n",
    "                print(k,t,c,v)\n",
    "                if MORTGAGE.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'], \n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                                        feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75]))    \n",
    "            rfdicts[k][f'{c}_{t}_pdp_MORTGAGE'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_MORTGAGE.csv')\n",
    "            \n",
    "            for v in ['age']:\n",
    "                print(k,t,c,v)\n",
    "                df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "            rfdicts[k][f'{c}_{t}_pdp_age'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_age.csv')\n",
    "            \n",
    "            for v in CONTROL:\n",
    "                print(k,t,c,v)\n",
    "                if CONTROL.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                                        feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75]))    \n",
    "            rfdicts[k][f'{c}_{t}_pdp_CONTROL'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_CONTROL.csv')\n",
    "            \n",
    "            CAT = [['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5'],['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'],['educ_nodegree','educ_highschool','educ_higher']]\n",
    "            i = 1\n",
    "            if 'finassets' in expvars:\n",
    "                CAT = [['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'],['educ_nodegree','educ_highschool','educ_higher']]\n",
    "            for cat in CAT:\n",
    "                print(k,t,c,cat)\n",
    "                df = uplift_cat_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],cat,rfdicts[k][t+'_X_labels'],rfdicts[k][t+'_X'],\n",
    "                                                           rfdicts[k][t+'_rbtamt'],X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                           feature_ids_cont=rfdicts[k][c+'_X_labels'],types=['mean','percentile','std','median'],\n",
    "                                                           percentile=[25,75])\n",
    "                rfdicts[k][f'{c}_{t}_pdp_CAT_{i}'] = df\n",
    "                df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_CAT_{i}.csv')\n",
    "                i = i+1\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SND_fin', 'SND_finit', 'SND_nofin', 'TOT_fin', 'TOT_finit', 'TOT_nofin']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "rfdicts_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7** Extension: Run random forest algorithm seperately for treatment and control group, include the difference in timing between reate receipt and when the interview took place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4028.000000\n",
       "mean        1.978898\n",
       "std         0.812004\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         3.000000\n",
       "max         3.000000\n",
       "Name: diff_1, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs08_cap.loc[fs08_cap['treat1']==1,'diff_1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #'educ_bachelor','educ_master','educ_doctorate'\n",
    "DEMO = ['age', 'adults', 'PERSLT18','CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5', 'MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'] # exclude 'FINCBTAX',\n",
    "DEMO2 = [ 'FINCBTXM'] # exclude 'FSALARYM',\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum','qescrowx_sum', 'timeleft'] # 'orgmrtx_sum', \n",
    "CONS = ['FD', 'SND', 'ND', 'TOT']\n",
    "CONT = ['cont1', 'cont2']\n",
    "TREAT = 'treat1'\n",
    "treatgroup = TREAT\n",
    "trees = 1000\n",
    "\n",
    "#Random Forest for short term consumption: treatment group 1 without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC   #define explanatory variables + ['finassets_it'] \n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf_diff = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean','diff_1']] #include diff_1 for treatment\n",
    "    rf_diff[TREAT] = treat\n",
    "    rf_diff[TREAT+'_id'] = fs08_cap.loc[(fs08_cap[treatgroup]==1),['CustID','NEWID']]\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf_diff[con] = cont\n",
    "        rf_diff[con+'_id'] = fs08_cap.loc[fs08_cap[con]==1, ['CustID','NEWID']]\n",
    "    for i in list(set([rf_diff.split('_')[0] for rf_diff in list(rf_diff)])):\n",
    "        y = np.array(rf_diff[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf_diff[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf_diff[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf_diff[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf_diff[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf_diff[i+'_rbtamt'] = np.array(rf_diff[i]['rbtamt_idmean'])\n",
    "        rf_diff[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        #rf_diff[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf_diff[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_nofin_diff.pkl', 'wb')\n",
    "    pickle.dump(rf_diff, output)\n",
    "    output.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine the partial dependency function. Necessary, because the variable diff_1 is only avaiblable for treatment group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_num_diff_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[], types=['mean'], percentile='none', grid_lower=5, grid_upper=95 ): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor in a rf uplift 2 model approach if the values are numerical.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    Dataframe of:\n",
    "    1. grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    2. the corresposing types such as mean, median, etc.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is int):\n",
    "                if f_id > (X_temp.shape[1]-1):\n",
    "                    raise ValueError(f'positional number of {f_id} exceeds array shape')\n",
    "                else:\n",
    "                    X_temp = X_treat.copy()\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str):\n",
    "                if f_id not in feature_ids_treat:\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids is not passed to the function')\n",
    "                else:\n",
    "                    f_id = feature_ids_treat.index(f_id)\n",
    "                    f_id_label = f_id\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('f_id needs to be either an integer or a string')\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(f_id) is int):\n",
    "                raise ValueError(f'if X_cont is specified, then f_id needs to be a string variable')\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str) & (f_id not in feature_ids_treat):\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids_treat or feature_ids_cont is not passed to the function')\n",
    "            else:\n",
    "                if (type(X_cont_rbtamt) is not np.ndarray) :\n",
    "                    raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                else:\n",
    "                    cont_df =  pd.DataFrame(X_cont, columns = feature_ids_cont)\n",
    "                    cont_astreat_df = cont_df.copy()\n",
    "                    cont_astreat_df[f_id] = np.nan\n",
    "                    treat_df = pd.DataFrame(X_treat, columns = feature_ids_treat)\n",
    "                    treat_ascont_df = treat_df.copy()\n",
    "                    treat_ascont_df = treat_ascont_df.drop(columns=[f_id])\n",
    "                    X_treat_df = pd.concat([treat_df,cont_astreat_df], join = 'inner', ignore_index=True) #treatment df includes diff_1\n",
    "                    X_cont_df = pd.concat([treat_ascont_df,cont_df],join='inner',ignore_index=True) #control df excludes diff_1\n",
    "                    mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = np.array(mean_rbt)\n",
    "                    X_labels = list(X_treat_df.columns)\n",
    "                    X_temp = np.array(X_treat_df)\n",
    "                    X_temp2 = np.array(X_cont_df)\n",
    "                    f_id_label = f_id\n",
    "                    f_id = X_labels.index(f_id)             \n",
    "        else: \n",
    "            raise ValueError('Either X_cont or X_treat does not have an array like structure')\n",
    "\n",
    "        X_unique = np.array(list(set(X_temp[:, f_id])))\n",
    "        grid = np.linspace(1, 3, 3) #redefine grid\n",
    "       \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types.copy()\n",
    "        #column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "\n",
    "        if set(types) <= set(function_labels): #given types must be of possible types median, mean, percentile, std\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i] #fill functions list with corresponding numpy functions\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif type(percentile) is not list: # 0<=percentile<=100:\n",
    "                    raise ValueError('percentile must be list')\n",
    "                else:\n",
    "                    column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "                    if len(percentile)>1:\n",
    "                        for p in percentile[1:]:\n",
    "                            types = types + ['percentile'] #pass\n",
    "                            nptypes = nptypes + [np.percentile]\n",
    "                            column_labels = column_labels + ['percentile_' +str(p)] #augment column labels with percentile columns if specified\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "        y_pred = np.zeros((len(grid),len(types)))\n",
    "        mpc_pred = np.zeros((len(grid), len(types)))\n",
    "        p_pos = 0\n",
    "        X_temp2 = rf_cont.predict(X_temp2)\n",
    "\n",
    "        for i, val in enumerate(grid): # i returns the counter, val returns the value at position of counter on grid \n",
    "            X_temp[:, f_id] = val #replace value with grid\n",
    "            y_temp = (rf_treat.predict(X_temp) - X_temp2)\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            p_pos = 0\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[i,j] = nptypes[j](y_temp,percentile[p_pos])\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp,percentile[p_pos])\n",
    "                    p_pos = p_pos + 1\n",
    "                else:\n",
    "                    y_pred[i,j] = nptypes[j](y_temp)\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp)\n",
    "        for j in range(len(column_labels)):\n",
    "            if column_labels[j] == 'percentile':\n",
    "                column_labels[j] = 'percentile_' + str(percentile)\n",
    "            else:\n",
    "                pass \n",
    "        column_labels_cr = ['cr_'+ lab for lab in column_labels]\n",
    "        column_labels_mpc = ['mpc_' + lab for lab in column_labels]\n",
    "        column_labels = ['grid']+column_labels_cr+column_labels_mpc\n",
    "        column_labels = [str(f_id_label)+ '_' + lab for lab in column_labels]\n",
    "        df = pd.DataFrame(np.c_[grid, y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return df #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD_nofin_diff.pkl\n",
      "['FD_nofin_diff.pkl', 'ND_nofin_diff.pkl', 'SND_nofin_diff.pkl', 'TOT_nofin_diff.pkl']\n"
     ]
    }
   ],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "print(rfdict_list[3])\n",
    "rfdict_list_diff = [i for i in rfdict_list if i[-8:]=='diff.pkl'] #use only rfdicts where diff is included.\n",
    "print(rfdict_list_diff)\n",
    "rfdicts_diff = dict()\n",
    "\n",
    "for rf in rfdict_list_diff:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts_diff[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run partial dependence plot for nofin, cont1 and treat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD_nofin_diff treat1 cont2 diff_1\n",
      "ND_nofin_diff treat1 cont2 diff_1\n",
      "SND_nofin_diff treat1 cont2 diff_1\n",
      "TOT_nofin_diff treat1 cont2 diff_1\n"
     ]
    }
   ],
   "source": [
    "rfdicts_diff_keys = list(rfdicts_diff)\n",
    "rfdicts_diff_keys\n",
    "t='treat1'\n",
    "c='cont2'\n",
    "for k in rfdicts_diff_keys:\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\pdp\\\\' + cons\n",
    "    for v in ['diff_1']:\n",
    "        print(k,t,c,v)\n",
    "        df = uplift_num_diff_2m_partial_dependency_mpc(rfdicts_diff[k][t+'_rf'],rfdicts_diff[k][c+'_rf'],v,rfdicts_diff[k][t+'_X'],rfdicts_diff[k][t+'_rbtamt'],\n",
    "                                                       X_cont=rfdicts_diff[k][c+'_X'],X_cont_rbtamt=rfdicts_diff[k][c+'_rbtamt'],feature_ids_treat=rfdicts_diff[k][t+'_X_labels'],\n",
    "                                                       feature_ids_cont=rfdicts_diff[k][c+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "        df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_diff1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
