{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import PercentFormatter #plot as percentage\n",
    "import seaborn #plot density and histogram at the same time\n",
    "# Set directory where files are downloaded to. Chdir has to be changed in order to run on another computer\n",
    "os.chdir('C:\\\\Users\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE') #change this to the folder where the data set is stored, all the results will be saved in the same folder\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Data** (necessary for part1 and part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08 = pd.read_csv(os.getcwd()+'\\\\fs08.csv').set_index('CustID')\n",
    "\n",
    "#identifier\n",
    "TIME = ['QINTRVMO', 'QINTRVYR', 'rbtmo_1', 'rbtmo_2', 'diff_1', 'diff_2']\n",
    "ID = ['NEWID']\n",
    "\n",
    "#dependent variables\n",
    "CONS = ['FD','SND','ND','DUR','TOT']\n",
    "FUTCONS = ['fut_' + c for c in CONS]\n",
    "LRUNCONS = ['lrun_' + c for c in CONS]\n",
    "\n",
    "for i in range(len(LRUNCONS)):\n",
    "    fs08[LRUNCONS[i]] = fs08[[CONS[i],FUTCONS[i]]].sum(axis=1)\n",
    "\n",
    "#explanatory variables\n",
    "DEMO = ['age', 'adults', 'PERSLT18', 'MARITAL1', 'CUTENURE', 'FINCBTAX'] \n",
    "    #age; number of adults; people below 18; marital status; housing tenure; income in the last 12 months\n",
    "DEMO2 = ['FSALARYM', 'FINCBTXM'] \n",
    "    #FSALARYM: income from salary and wages, CKBKACTX: balance/market value in balance accounts/brookerage accounts;    \n",
    "    #FINCBTXM: Total amount of family income before taxes (Imputed or collected data); (relevant demographics available for the second stimulus only)\n",
    "ASSETS = ['valid_finassets','finassets']\n",
    "    # finassets: sum of 1) SAVACCTX (Total balance/market value (including interest earned) CU had in savings accounts in banks, savings and loans,\n",
    "                         #credit unions, etc., as of the last day of previous month;)\n",
    "                # and    2)CKBKACTX (Total balance or market value (including interest earned) CU had in checking accounts, brokerage accounts, \n",
    "                            #and other similar accounts as of the last day of the previous month\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'orgmrtx_sum', 'qescrowx_sum', 'timeleft']\n",
    "    #morgpayment: morgage payment per month; qblncm1x_sum: sum of principal balances outstanding at the beginning of month M1; orgmrtx_sum: sum of mortgage amounts;\n",
    "    #qescrowx_sum: sum of last regular escrow payments; timeleft: maximum time left on mortgage payment\n",
    "EDUC = ['educ_nodegree','educ_highschool','educ_bachelor','educ_master','educ_doctorate']\n",
    "#sample split\n",
    "RBT = ['rbtamt', 'rbtamt_chk', 'rbtamt_e']\n",
    "LAGRBT = ['last_' + var for var in RBT] #lagged variables\n",
    "FUTRBT = ['fut_' + var for var in RBT] #future variables\n",
    "\n",
    "fs08 = fs08[TIME + ID + CONS + DEMO + DEMO2 + ASSETS + MORTGAGE + RBT + ['rbtamt_1','rbtamt_2'] + LAGRBT + FUTRBT + FUTCONS + LRUNCONS + EDUC] #+ CHGCONS + LAGCONS \n",
    "fs08 = pd.get_dummies(fs08, columns=['CUTENURE']) #change categorical variables to dummy variables\n",
    "DEMO = [s for s in DEMO if s != 'CUTENURE'] + ['CUTENURE' + f'_{j}' for j in list(range(1,6)) if j!=3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8287701228524802\n",
      "0.8955169267751125\n",
      "0.7627057548315713\n",
      "0.09380920937886113\n",
      "771\n",
      "4339\n"
     ]
    }
   ],
   "source": [
    "print(fs08['FINCBTAX'].corr(fs08['FINCBTXM']))\n",
    "print(fs08['FSALARYM'].corr(fs08['FINCBTXM']))\n",
    "print(fs08['FSALARYM'].corr(fs08['FINCBTAX']))\n",
    "print(fs08['FINCBTAX'].corr(fs08['morgpayment']))\n",
    "print(fs08.loc[(fs08['rbtamt']>0)&(fs08['valid_finassets']>0),'NEWID'].count())\n",
    "print(fs08.loc[fs08['rbtamt']>0,'NEWID'].count())\n",
    "fs08.loc[fs08['rbtamt_2']>0, ['rbtamt_1','rbtamt_2','diff_2', 'diff_1']]\n",
    "fs08['diff'] = np.nan\n",
    "fs08['diff'] = np.where((fs08['diff_1']>0) & (fs08['diff_2']>0), fs08[['diff_1','diff_2']].mean(axis=1),fs08['diff_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the average rebate amount per individual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08['rbtamt_idmean'] = 0\n",
    "fs08['rbtamt_idmean'] = fs08.groupby('CustID')['rbtamt'].transform('mean')\n",
    "fs08['rbt_count'] = 0\n",
    "fs08['rbt_count'] = fs08.groupby('CustID')['rbtamt'].transform('count')\n",
    "\n",
    "#\n",
    "#sometimes individuals give information of rebate receipt preceding (following) three months of the first (last) interview.\n",
    "#Wherever this is the case, the average rebate should be the weighted mean of rebates received before (after) the relevant time and the actual rebate \n",
    "\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['last_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) #weighted mean,  & (fs08['rbtamt_idmean']>0)\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist() # & (fs08['rbtamt_idmean']>0)\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first') #change for all entries for a given individual\n",
    "\n",
    "\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['fut_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) #weighted mean  & (fs08['rbtamt_idmean']>0)\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0)  & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() #& (fs08['rbtamt_idmean']>0)\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last')  #change for all entries for a given individual\n",
    "#display(fs08.loc[index, ['rbtamt_idmean', 'rbtamt', 'fut_rbtamt', 'last_rbtamt']])\n",
    "\n",
    "#wherever there is no entry for rebates received in the relevant time period but when there were rebates receivde in the past (future) change mean to the value\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() #                                                                               \n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 fs08['fut_rbtamt'], fs08['rbtamt_idmean'])\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last')\n",
    "\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist()\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())), \n",
    "                                fs08['last_rbtamt'], fs08['rbtamt_idmean'])\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first')\n",
    "\n",
    "\n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[(fs08['rbtamt']>0) | (fs08['fut_rbtamt']>0) | (fs08['last_rbtamt']>0) ,'rbt_flag'] = 1\n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('sum')\n",
    "fs08.loc[fs08['rbt_flag']>0, 'rbt_flag'] = 1\n",
    "fs08 = fs08.loc[fs08['rbt_flag']==1]\n",
    "\n",
    "\n",
    "index = fs08.index[fs08['rbtamt_idmean'].isna()].tolist()\n",
    "index = list(set(index))\n",
    "fs08.loc[index, 'rbtamt_idmean'] = fs08.loc[index,'fut_rbtamt']\n",
    "fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'fut_rbtamt' ]\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'last_rbtamt' ]\n",
    "\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations, impute values for financial liquidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(RBT)):\n",
    "    fs08.loc[fs08[RBT[i]]==0, RBT[i]] = np.nan\n",
    "    fs08.loc[fs08[LAGRBT[i]]==0, LAGRBT[i]] = np.nan\n",
    "    fs08.loc[fs08[FUTRBT[i]]==0, FUTRBT[i]] = np.nan\n",
    "    #fs08.loc[(fs08[LAGRBT[i]]==0) | (fs08[LAGRBT[i]].isna()), LAGRBT[i]] =  fs08.loc[(fs08[LAGRBT[i]]==0) | (fs08[LAGRBT[i]].isna())].groupby('CustID')[RBT[i]].shift(-1) \n",
    "    fs08.loc[(fs08[FUTRBT[i]]==0) | (fs08[FUTRBT[i]].isna()), FUTRBT[i]] =  fs08.loc[(fs08[FUTRBT[i]]==0) | (fs08[FUTRBT[i]].isna())].groupby('CustID')[RBT[i]].shift(-1) \n",
    "\n",
    "\n",
    "\n",
    "fs08 = fs08.reset_index()\n",
    "\n",
    "#Iterative imputation for financial liquidity\n",
    "#explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "fs08 = fs08.dropna(subset=CONS+DEMO+DEMO2+MORTGAGE) #Keep only observations that have all info on explanatory variables, dropping missing values on mortgage lowers the sample to half \n",
    "\n",
    "\n",
    "fs08_finit = fs08.copy()\n",
    "fs08_finit = fs08_finit.loc[:,CONS+DEMO+DEMO2+MORTGAGE+['CustID','NEWID','finassets']]\n",
    "labels = list(fs08_finit.columns)\n",
    "imp_mean = IterativeImputer(random_state=0) #use python package iterative imputer\n",
    "imp_mean.fit(fs08_finit)\n",
    "fs08_finit = pd.DataFrame(imp_mean.transform(fs08_finit),columns=labels)\n",
    "fs08_finit = fs08_finit.loc[:,['finassets','NEWID']]\n",
    "fs08_finit = fs08_finit.rename(columns={'finassets':'finassets_it'})\n",
    "\n",
    "fs08 = pd.merge(fs08.sort_values(by = ['NEWID']).reset_index(), fs08_finit.sort_values(by = ['NEWID']).reset_index(), how = 'left', on = 'NEWID', validate = '1:1')\n",
    "fs08 = fs08.drop(columns=['index_x','index_y']) \n",
    "ASSETS = ['valid_finassets','finassets', 'finassets_it' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate treatment and control groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate treatment group:\n",
    "fs08['treat1'] = 0 \n",
    "fs08.loc[fs08['rbtamt'].notna(),'treat1'] = 1 #all entries with actual info on rebate are in the treatment group\n",
    "\n",
    "#three different control groups:\n",
    "#control group 1: those who didn't receive the rebate in the given month\n",
    "fs08['cont1'] = 0\n",
    "fs08.loc[fs08['rbtamt'].isna(), 'cont1'] = 1\n",
    "\n",
    "#control group 2: drop one time period after receipt of rebate, as part of rebate might've been consumed one time period after\n",
    "fs08['cont2'] = 0 \n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[fs08['rbtamt']>0,'rbt_flag'] = 1 #identifier for rebate\n",
    "fs08['rbt_flag_lag'] = fs08.groupby('CustID')['rbt_flag'].shift(1) #identifier for rebate a period before (lag)\n",
    "fs08.loc[fs08['rbt_flag_lag']==1,'rbt_flag']=1 #change rebate identifier so it capture now if a rebate was received this period or the period before\n",
    "fs08.loc[(fs08['rbtamt'].isna()) & (fs08['rbt_flag']==0), 'cont2'] = 1 #those who didn't receive a rebate now or a period before are in the control group\n",
    "\n",
    "#treatment 2: all individuals who received a rebate last time period. This group should be compared to control group 2 only\n",
    "fs08['treat2'] = 0\n",
    "fs08.loc[(fs08['cont2']==0) & (fs08['treat1']==0),'treat2'] = 1\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt'].isna()), 'treat2'] = 1\n",
    "\n",
    "#treatment 3: long run consumption response: all individuals who received a rebate in this period and where the rebate interview is not the last\n",
    "fs08['treat3'] = 0\n",
    "fs08.loc[(fs08['rbtamt']>0) & (fs08[FUTCONS[0]].notna()), 'treat3'] = 1\n",
    "\n",
    "#control 3: long-run consumption: those who haven't received a rebate two periods from current period and have information on consumption for next period as well\n",
    "fs08['cont4'] = 0\n",
    "fs08.loc[(fs08['cont2']==1) & (fs08[FUTCONS[0]].notna()),'cont4'] = 1\n",
    "\n",
    "#control 4: those who haven't received the rebate yet\n",
    "fs08['cont3'] = 0 \n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('cumsum') #starts counting from the point on which the first rebate was received\n",
    "fs08.loc[(fs08['rbtamt'].isna()) & (fs08['rbt_flag']==0),'cont3'] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08_cap = fs08[(np.abs(stats.zscore(fs08.loc[:,CONS+LRUNCONS])) < 3).all(axis=1)] #drop outliers\n",
    "fs08_cap = fs08_cap.loc[fs08_cap['FD']>0] #there are still two observations where food consumption is zero; drop bc of common sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Descriptive statistics** (part 1 and part 2 cn be run seperately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Descriptives for dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary stats for full sample on consumption:\n",
      "                 FD           SND            ND           DUR           TOT\n",
      "count   7901.000000   7901.000000   7901.000000   7901.000000   7901.000000\n",
      "mean    2214.125173   5107.249167   6357.100416   6241.762099  12598.862515\n",
      "std     1309.135172   2681.752964   3277.032339   5821.221010   7599.242540\n",
      "min        0.000000    611.000100  -4420.575000    169.499900   1818.500100\n",
      "25%     1371.000000   3415.000200   4204.999800   3099.000200   7865.991000\n",
      "50%     1975.000200   4620.750200   5761.999900   4756.000200  10843.200100\n",
      "75%     2750.000100   6157.000000   7722.000200   7262.999900  14921.999800\n",
      "max    37678.000300  50375.052100  54477.052200  90264.999900  98775.000000\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &            FD &           SND &            ND &           DUR &           TOT \\\\\n",
      "\\midrule\n",
      "count &   7901.000000 &   7901.000000 &   7901.000000 &   7901.000000 &   7901.000000 \\\\\n",
      "mean  &   2214.125173 &   5107.249167 &   6357.100416 &   6241.762099 &  12598.862515 \\\\\n",
      "std   &   1309.135172 &   2681.752964 &   3277.032339 &   5821.221010 &   7599.242540 \\\\\n",
      "min   &      0.000000 &    611.000100 &  -4420.575000 &    169.499900 &   1818.500100 \\\\\n",
      "25\\%   &   1371.000000 &   3415.000200 &   4204.999800 &   3099.000200 &   7865.991000 \\\\\n",
      "50\\%   &   1975.000200 &   4620.750200 &   5761.999900 &   4756.000200 &  10843.200100 \\\\\n",
      "75\\%   &   2750.000100 &   6157.000000 &   7722.000200 &   7262.999900 &  14921.999800 \\\\\n",
      "max   &  37678.000300 &  50375.052100 &  54477.052200 &  90264.999900 &  98775.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "summary stats for capped sample on consumption:\n",
      "                FD           SND            ND           DUR           TOT\n",
      "count  7387.000000   7387.000000   7387.000000   7387.000000   7387.000000\n",
      "mean   2076.565723   4790.183461   5969.592649   5369.431765  11339.024413\n",
      "std     995.507110   1962.094025   2502.130182   3425.051949   5002.073047\n",
      "min      51.999900    611.000100    876.250100    169.499900   1818.500100\n",
      "25%    1352.000100   3354.999900   4132.499900   3021.000150   7688.000150\n",
      "50%    1940.000100   4499.999900   5598.999900   4549.000100  10432.000100\n",
      "75%    2638.499850   5921.624850   7390.000000   6792.500050  14121.000150\n",
      "max    6133.999800  12905.000000  15994.133600  23508.999900  35115.305600\n",
      "summary stats for treat1 on consumption:\n",
      "                FD           SND            ND           DUR           TOT\n",
      "count  1941.000000   1941.000000   1941.000000   1941.000000   1941.000000\n",
      "mean   2152.072131   5058.814060   6228.362786   5491.371516  11719.734302\n",
      "std    1042.759107   2074.626364   2644.063350   3496.791974   5150.980866\n",
      "min      78.000000    611.000100    876.250100    364.250000   2415.000200\n",
      "25%    1399.999900   3569.000100   4259.500100   3068.000000   7845.000100\n",
      "50%    1985.000100   4815.000000   5877.000000   4724.000000  10973.000200\n",
      "75%    2740.000200   6267.000100   7784.499700   6965.000000  14561.500000\n",
      "max    5986.000000  12854.999800  15849.000200  23508.999900  34784.631000\n",
      "summary stats for treat2 on consumption:\n",
      "                FD           SND            ND           DUR           TOT\n",
      "count  1543.000000   1543.000000   1543.000000   1543.000000   1543.000000\n",
      "mean   2068.246919   4778.278549   5937.603797   5402.084633  11339.688430\n",
      "std     977.377297   2039.755937   2558.325553   3524.572116   5198.172438\n",
      "min     164.000100    675.999900   1154.000100    289.249800   2269.999900\n",
      "25%    1344.499950   3301.499750   4008.999950   3034.375000   7680.999900\n",
      "50%    1950.000000   4456.500200   5576.999700   4477.999900  10308.999900\n",
      "75%    2684.000100   5922.000050   7426.875100   6853.500050  14074.499850\n",
      "max    5950.000200  12905.000000  15763.000200  23356.000000  34705.750300\n",
      "summary stats for treat3 on consumption:\n",
      "                FD           SND            ND           DUR           TOT\n",
      "count  1402.000000   1402.000000   1402.000000   1402.000000   1402.000000\n",
      "mean   2140.641943   4965.111044   6122.462998   5521.644163  11644.107160\n",
      "std    1032.678913   2019.809074   2564.180975   3483.577224   5110.939627\n",
      "min     156.000000    611.000100    969.999900    364.250000   2415.000200\n",
      "25%    1372.499850   3487.937375   4183.250050   3023.124900   7775.799800\n",
      "50%    1958.499950   4682.500000   5767.500000   4782.000150  10945.999950\n",
      "75%    2730.000000   6151.499975   7698.000200   7131.687575  14603.749925\n",
      "max    5986.000000  12626.000100  15695.200200  23508.999900  34211.000000\n",
      "summary stats for cont1 on consumption:\n",
      "                FD           SND            ND           DUR           TOT\n",
      "count  5446.000000   5446.000000   5446.000000   5446.000000   5446.000000\n",
      "mean   2049.654607   4694.441266   5877.364805   5325.971416  11203.336221\n",
      "std     976.803289   1911.486019   2443.192098   3398.385126   4941.313979\n",
      "min      51.999900    675.999900    940.000100    169.499900   1818.500100\n",
      "25%    1331.250150   3296.000150   4092.000050   3013.187500   7635.250075\n",
      "50%    1919.000150   4411.000100   5527.499800   4481.500050  10244.625250\n",
      "75%    2600.000100   5820.793800   7259.875000   6728.874925  13905.449550\n",
      "max    6133.999800  12905.000000  15994.133600  23356.000000  35115.305600\n",
      "summary stats for cont2 on consumption:\n",
      "                FD           SND            ND           DUR           TOT\n",
      "count  4112.000000   4112.000000   4112.000000   4112.000000   4112.000000\n",
      "mean   2037.880835   4656.034098   5851.641311   5298.342659  11149.983970\n",
      "std     972.227584   1861.118434   2400.194419   3364.686945   4857.661757\n",
      "min      51.999900    804.000100    940.000100    169.499900   1818.500100\n",
      "25%    1325.999975   3293.921575   4114.891200   2992.062375   7625.124600\n",
      "50%    1901.000100   4382.999750   5511.999950   4481.500100  10206.500250\n",
      "75%    2570.250225   5758.500050   7186.500000   6669.437600  13840.150025\n",
      "max    6133.999800  12799.000300  15994.133600  23280.000200  35115.305600\n",
      "summary stats for cont3 on consumption:\n",
      "                FD           SND            ND           DUR           TOT\n",
      "count  3100.000000   3100.000000   3100.000000   3100.000000   3100.000000\n",
      "mean   2061.246127   4714.807701   5893.056351   5245.049850  11138.106202\n",
      "std     980.464294   1868.317160   2398.532732   3303.254718   4806.547369\n",
      "min      51.999900    811.999900    940.000100    174.500100   1818.500100\n",
      "25%    1349.999900   3348.375100   4149.500150   2974.000000   7657.500100\n",
      "50%    1930.000000   4447.000100   5560.999900   4472.875050  10245.699950\n",
      "75%    2599.999875   5811.000200   7202.249975   6567.546025  13799.750000\n",
      "max    6133.999800  12799.000300  15841.005900  23280.000200  35115.305600\n",
      "summary stats for cont4 on consumption:\n",
      "                FD           SND            ND           DUR           TOT\n",
      "count  3315.000000   3315.000000   3315.000000   3315.000000   3315.000000\n",
      "mean   2059.880239   4710.366308   5899.196202   5237.446787  11136.642989\n",
      "std     985.138584   1875.359532   2413.618555   3305.340328   4830.630031\n",
      "min      51.999900    811.999900    940.000100    169.499900   1818.500100\n",
      "25%    1344.999900   3344.999850   4144.250150   2967.749850   7644.874700\n",
      "50%    1929.999900   4446.000000   5561.400200   4436.000100  10241.899900\n",
      "75%    2599.999800   5811.499900   7237.000100   6599.124950  13774.005500\n",
      "max    6133.999800  12799.000300  15994.133600  23280.000200  35115.305600\n"
     ]
    }
   ],
   "source": [
    "print('summary stats for full sample on consumption:')\n",
    "print(fs08[CONS].describe())\n",
    "print(fs08[CONS].describe().to_latex())\n",
    "print('summary stats for capped sample on consumption:')\n",
    "print(fs08_cap[CONS].describe())\n",
    "\n",
    "treat = ['treat1','treat2','treat3']\n",
    "cont = ['cont1','cont2','cont3','cont4']\n",
    "\n",
    "for group in treat+cont:\n",
    "    print('summary stats for', group, 'on consumption:')\n",
    "    print(fs08_cap.loc[fs08_cap[group]==1,CONS].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "descriptives for rebate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average amount of rebate received per household\n",
      "count    7387.000000\n",
      "mean     1105.015545\n",
      "std       526.007328\n",
      "min         6.000000\n",
      "25%       600.000000\n",
      "50%      1200.000000\n",
      "75%      1500.000000\n",
      "max      3660.000000\n",
      "Name: rbtamt_idmean, dtype: float64\n",
      "Actual number of rebate received\n",
      "count    1941.000000\n",
      "mean     1095.451829\n",
      "std       526.709045\n",
      "min         1.000000\n",
      "25%       600.000000\n",
      "50%      1200.000000\n",
      "75%      1500.000000\n",
      "max      3660.000000\n",
      "Name: rbtamt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Average amount of rebate received per household')\n",
    "print(fs08_cap.loc[fs08['rbtamt_idmean'].notna(),'rbtamt_idmean'].describe())\n",
    "print('Actual number of rebate received')\n",
    "print(fs08_cap.loc[fs08['rbtamt']>0,'rbtamt'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare histograms of treatment group with different control groups\n",
    "        \n",
    "for i in range(len(CONS)):\n",
    "    for t in treat[0:1]:\n",
    "        for c in cont[0:1]:\n",
    "            plt.figure(figsize=(3.2,2.4))\n",
    "            plt.title(f'{CONS[i]}')\n",
    "            plt.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            plt.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            #plt.set_title(f'{CONS[i]}:')\n",
    "            #plt.tight_layout()\n",
    "            plt.legend(loc='upper right', frameon=False)\n",
    "            plt.savefig(os.getcwd() + f'\\\\descriptives\\\\{CONS[i]}_pattern.pdf')\n",
    "            plt.close()\n",
    "\n",
    "plot_count = 1\n",
    "for i in range(len(CONS)):\n",
    "    #plt.figure(figsize=(6.4,2.4))\n",
    "    for t in treat[0:1]:\n",
    "        fig=plt.figure(figsize=(6.4,2.4))\n",
    "        for c in cont[0:3]:     \n",
    "            #plot = plt.figure(figsize=(10,3))\n",
    "            plot = plt.subplot(1,3,cont.index(c)+1)\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            plot.set_title(f'{CONS[i]}:')\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc='upper right', frameon=False)\n",
    "            plot_count = plot_count+1\n",
    "            #plt.savefig(os.getcwd() + f'\\\\descriptives\\\\{CONS[i]}_pattern_groupcomp.pdf')\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Take a look at the explanatory variables used for random forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole sample (capped):\n",
      "               age       adults     PERSLT18     MARITAL1       FINCBTAX  \\\n",
      "count  7387.000000  7387.000000  7387.000000  7387.000000    7387.000000   \n",
      "mean     46.631312     2.071748     0.919182     1.782591   67313.695817   \n",
      "std      12.262306     0.775153     1.177616     1.361278   51152.007450   \n",
      "min      21.000000     1.000000     0.000000     1.000000 -104854.000000   \n",
      "25%      37.000000     2.000000     0.000000     1.000000   32307.000000   \n",
      "50%      46.000000     2.000000     0.000000     1.000000   61966.000000   \n",
      "75%      55.000000     2.000000     2.000000     3.000000   95200.000000   \n",
      "max      84.500000     7.000000     8.000000     5.000000  434017.000000   \n",
      "\n",
      "       CUTENURE_1   CUTENURE_2   CUTENURE_4   CUTENURE_5       FSALARYM  \\\n",
      "count  7387.00000  7387.000000  7387.000000  7387.000000    7387.000000   \n",
      "mean      0.97834     0.010288     0.010288     0.001083   71112.944578   \n",
      "std       0.14558     0.100915     0.100915     0.032893   51190.681413   \n",
      "min       0.00000     0.000000     0.000000     0.000000       0.000000   \n",
      "25%       1.00000     0.000000     0.000000     0.000000   37800.000000   \n",
      "50%       1.00000     0.000000     0.000000     0.000000   65000.000000   \n",
      "75%       1.00000     0.000000     0.000000     0.000000   96906.200000   \n",
      "max       1.00000     1.000000     1.000000     1.000000  435782.000000   \n",
      "\n",
      "            FINCBTXM  valid_finassets     finassets  finassets_it  \\\n",
      "count    7387.000000      7387.000000  1.072000e+03  7.387000e+03   \n",
      "mean    81158.785759         0.145120  3.842598e+04  3.219493e+04   \n",
      "std     51632.783198         0.352245  1.743121e+05  7.912637e+04   \n",
      "min    -36146.300000         0.000000  0.000000e+00 -1.567352e+05   \n",
      "25%     46417.400000         0.000000  7.000000e+02  4.306232e+03   \n",
      "50%     71361.000000         0.000000  4.059500e+03  2.106392e+04   \n",
      "75%    104134.600000         0.000000  1.907500e+04  4.275887e+04   \n",
      "max    439195.000000         1.000000  3.999868e+06  3.999868e+06   \n",
      "\n",
      "       morgpayment  qblncm1x_sum   orgmrtx_sum  qescrowx_sum     timeleft  \n",
      "count  7387.000000  7.387000e+03  7.387000e+03   7387.000000  7387.000000  \n",
      "mean     39.467280  1.243554e+05  1.507023e+05    299.211182    19.245454  \n",
      "std      61.408699  1.111639e+05  1.193793e+05    336.397894     8.951968  \n",
      "min   -1126.011000  0.000000e+00  7.600000e+02      0.000000   -19.500000  \n",
      "25%      19.537621  5.241050e+04  7.500000e+04     47.000000    12.000000  \n",
      "50%      28.984022  9.661100e+04  1.210000e+05    219.000000    21.833333  \n",
      "75%      44.951954  1.613305e+05  1.890000e+05    416.000000    26.916667  \n",
      "max    1521.626700  1.143621e+06  1.395339e+06   4221.000000    39.333333  \n",
      "----------------------------------------------\n",
      "treat (capped):\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'treat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'treat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-0fc33e1892e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'----------------------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'(capped):'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfs08_cap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfs08_cap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDEMO\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mDEMO2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mASSETS\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mMORTGAGE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfs08_cap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfs08_cap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDEMO\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mDEMO2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mASSETS\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mMORTGAGE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_latex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'treat'"
     ]
    }
   ],
   "source": [
    "print('Whole sample (capped):')\n",
    "print(fs08_cap.loc[:,DEMO+DEMO2+ASSETS+MORTGAGE].describe())\n",
    "for group in ['treat','cont1','cont2','cont3']:\n",
    "    print('----------------------------------------------')\n",
    "    print(group, '(capped):')\n",
    "    print(fs08_cap.loc[fs08_cap[group]==1,DEMO+DEMO2+ASSETS+MORTGAGE].describe())\n",
    "    print(fs08_cap.loc[fs08_cap[group]==1,DEMO+DEMO2+ASSETS+MORTGAGE].describe().to_latex())\n",
    "\n",
    "for group in ['treat','cont1','cont2','cont3']:\n",
    "    print('----------------------------------------------')\n",
    "    print(group, '(capped) solely on liquid assets:')\n",
    "    print(fs08_cap.loc[fs08_cap[group]==1, ASSETS].describe() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: Machine learning approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Define sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDUC = ['educ_nodegree','educ_highschool','educ_bachelor','educ_master','educ_doctorate']\n",
    "DEMO = EDUC + ['age', 'adults', 'PERSLT18', 'MARITAL1', 'FINCBTAX'] \n",
    "DEMO2 = ['FSALARYM', 'FINCBTXM' ] \n",
    "CONS = ['FD', 'SND', 'ND', 'DUR']\n",
    "CONT = ['cont1', 'cont2', 'cont4']\n",
    "TREAT = 'treat1'\n",
    "treatgroup = TREAT\n",
    "trees = 2000\n",
    "\n",
    "#Random Forest for short term consumption: treatment group 1 with imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + ['finassets_it']  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_finit.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE  #define explanatory variables + ['finassets_it'] \n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_nofin.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group with just the observations where financial assets are included\n",
    "TREAT = 'treat1'\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + ['finassets']  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1) & (fs08_cap['valid_finassets']==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[(fs08_cap[con]==1) & (fs08_cap['valid_finassets']==1) , [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_fin.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Run random forest algorithm seperately for treatment and control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "#pkl_file = open('myfile.pkl', 'rb')\n",
    "#rf2 = pickle.load(pkl_file)\n",
    "#pkl_file.close()\n",
    "#\n",
    "#print(rf)\n",
    "#print(rf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Predict Outcomes for overall consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_predicitons_rbt(rf_treat, rf_cont, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if (type(X_cont) is not np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_treat_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('X_treat_rbamt needs to have an array like structure')\n",
    "            else:\n",
    "                X_temp = X_treat.copy()\n",
    "                rbtamt_temp = X_treat_rbtamt.copy()\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_cont_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('if X_cont is specified, X_cont_rbamt needs to have an array like structure')\n",
    "            if sorted(feature_ids_treat)!=sorted(feature_ids_cont):\n",
    "                raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont')\n",
    "            elif (len(feature_ids_treat)==0) | (len(feature_ids_cont)==0):\n",
    "                raise ValueError(f'if X_treat and X_cont are specified, feature_ids must not be empty')\n",
    "            else:\n",
    "                X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                rbtamt_temp = pd.concat([pd.DataFrame(X_treat_rbtamt), pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                X_labels = list(X.columns)\n",
    "                X_temp = np.array(X)\n",
    "                rbtamt_temp = np.array(rbtamt_temp)\n",
    "        else: \n",
    "            raise ValueError('X_treat does not have an array like structure')\n",
    "        y = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "        mpc = y/rbtamt_temp[:,0]\n",
    "        return y,mpc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUR_fin:\n",
      "cont2_treat1_y_pred (741,)\n",
      "cont2_treat1_mpc_pred (741,)\n",
      "cont1_treat1_y_pred (1073,)\n",
      "cont1_treat1_mpc_pred (1073,)\n",
      "cont4_treat1_y_pred (393,)\n",
      "cont4_treat1_mpc_pred (393,)\n",
      "DUR_finit:\n",
      "cont2_treat1_y_pred (6062,)\n",
      "cont2_treat1_mpc_pred (6062,)\n",
      "cont1_treat1_y_pred (7396,)\n",
      "cont1_treat1_mpc_pred (7396,)\n",
      "cont4_treat1_y_pred (5264,)\n",
      "cont4_treat1_mpc_pred (5264,)\n",
      "DUR_nofin:\n",
      "cont2_treat1_y_pred (6062,)\n",
      "cont2_treat1_mpc_pred (6062,)\n",
      "cont1_treat1_y_pred (7396,)\n",
      "cont1_treat1_mpc_pred (7396,)\n",
      "cont4_treat1_y_pred (5264,)\n",
      "cont4_treat1_mpc_pred (5264,)\n",
      "FD_fin:\n",
      "cont2_treat1_y_pred (741,)\n",
      "cont2_treat1_mpc_pred (741,)\n",
      "cont1_treat1_y_pred (1073,)\n",
      "cont1_treat1_mpc_pred (1073,)\n",
      "cont4_treat1_y_pred (393,)\n",
      "cont4_treat1_mpc_pred (393,)\n",
      "FD_finit:\n",
      "cont2_treat1_y_pred (6062,)\n",
      "cont2_treat1_mpc_pred (6062,)\n",
      "cont1_treat1_y_pred (7396,)\n",
      "cont1_treat1_mpc_pred (7396,)\n",
      "cont4_treat1_y_pred (5264,)\n",
      "cont4_treat1_mpc_pred (5264,)\n",
      "FD_nofin:\n",
      "cont2_treat1_y_pred (6062,)\n",
      "cont2_treat1_mpc_pred (6062,)\n",
      "cont1_treat1_y_pred (7396,)\n",
      "cont1_treat1_mpc_pred (7396,)\n",
      "cont4_treat1_y_pred (5264,)\n",
      "cont4_treat1_mpc_pred (5264,)\n",
      "ND_fin:\n",
      "cont2_treat1_y_pred (741,)\n",
      "cont2_treat1_mpc_pred (741,)\n",
      "cont1_treat1_y_pred (1073,)\n",
      "cont1_treat1_mpc_pred (1073,)\n",
      "cont4_treat1_y_pred (393,)\n",
      "cont4_treat1_mpc_pred (393,)\n",
      "ND_finit:\n",
      "cont2_treat1_y_pred (6062,)\n",
      "cont2_treat1_mpc_pred (6062,)\n",
      "cont1_treat1_y_pred (7396,)\n",
      "cont1_treat1_mpc_pred (7396,)\n",
      "cont4_treat1_y_pred (5264,)\n",
      "cont4_treat1_mpc_pred (5264,)\n",
      "ND_nofin:\n",
      "cont2_treat1_y_pred (6062,)\n",
      "cont2_treat1_mpc_pred (6062,)\n",
      "cont1_treat1_y_pred (7396,)\n",
      "cont1_treat1_mpc_pred (7396,)\n",
      "cont4_treat1_y_pred (5264,)\n",
      "cont4_treat1_mpc_pred (5264,)\n",
      "SND_fin:\n",
      "cont2_treat1_y_pred (741,)\n",
      "cont2_treat1_mpc_pred (741,)\n",
      "cont1_treat1_y_pred (1073,)\n",
      "cont1_treat1_mpc_pred (1073,)\n",
      "cont4_treat1_y_pred (393,)\n",
      "cont4_treat1_mpc_pred (393,)\n",
      "SND_finit:\n",
      "cont2_treat1_y_pred (6062,)\n",
      "cont2_treat1_mpc_pred (6062,)\n",
      "cont1_treat1_y_pred (7396,)\n",
      "cont1_treat1_mpc_pred (7396,)\n",
      "cont4_treat1_y_pred (5264,)\n",
      "cont4_treat1_mpc_pred (5264,)\n",
      "SND_nofin:\n",
      "cont2_treat1_y_pred (6062,)\n",
      "cont2_treat1_mpc_pred (6062,)\n",
      "cont1_treat1_y_pred (7396,)\n",
      "cont1_treat1_mpc_pred (7396,)\n",
      "cont4_treat1_y_pred (5264,)\n",
      "cont4_treat1_mpc_pred (5264,)\n"
     ]
    }
   ],
   "source": [
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\condistr\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for t in treat:\n",
    "        for c in cont:\n",
    "            y,mpc = uplift_predicitons_rbt(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                          rfdicts[k][c+'_X'],rfdicts[k][c+'_rbtamt'],rfdicts[k][t+'_X_labels'],rfdicts[k][c+'_X_labels'])\n",
    "            rfdicts[k][f'{c}_{t}_y_pred'] = y\n",
    "            print(f'{c}_{t}_y_pred', y.shape)\n",
    "            rfdicts[k][f'{c}_{t}_mpc_pred'] = mpc\n",
    "            print(f'{c}_{t}_mpc_pred', mpc.shape)\n",
    "            plt.hist(y, bins=40,  edgecolor='black')\n",
    "            lower = round((min(y)/100),1)*100\n",
    "            upper = round((max(y)/100),1)*100+1\n",
    "            plt.xticks(np.arange(lower, upper, 1000))\n",
    "            plt.title(f'Pred cons resp distr., {vartype},{c},{t}')\n",
    "            plt.xlabel(f'consumption in {cons}, number of observations {y.shape[0]}')\n",
    "            plt.ylabel(f'number of individuals in bin')\n",
    "            plt.savefig(newpath + f'\\\\{vartype}_{c}_{t}_y_pred.pdf')\n",
    "            plt.close()         \n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** Plot distribution of consumption response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5** Variable importance plot for treatment and control group separately and as a weighted sum for the whole sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DUR_fin', 'DUR_finit', 'DUR_nofin', 'FD_fin', 'FD_finit', 'FD_nofin', 'ND_fin', 'ND_finit', 'ND_nofin', 'SND_fin', 'SND_finit', 'SND_nofin']\n",
      "['treat1', 'cont1', 'cont2', 'cont4', 'treat1_X', 'treat1_X_labels', 'treat1_rbtamt', 'treat1_rf', 'cont1_X', 'cont1_X_labels', 'cont1_rbtamt', 'cont1_rf', 'cont2_X', 'cont2_X_labels', 'cont2_rbtamt', 'cont2_rf', 'cont4_X', 'cont4_X_labels', 'cont4_rbtamt', 'cont4_rf', 'cont2_treat1_y_pred', 'cont2_treat1_mpc_pred', 'cont1_treat1_y_pred', 'cont1_treat1_mpc_pred', 'cont4_treat1_y_pred', 'cont4_treat1_mpc_pred']\n"
     ]
    }
   ],
   "source": [
    "rfdicts_keys = list(rfdicts)\n",
    "print(rfdicts_keys)\n",
    "print(list(rfdicts[rfdicts_keys[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['treat1', 'cont2', 'cont1', 'cont4']\n",
      "DUR_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont4']\n",
      "DUR_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont4']\n",
      "DUR_nofin\n",
      "['treat1', 'cont2', 'cont1', 'cont4']\n",
      "FD_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont4']\n",
      "FD_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont4']\n",
      "FD_nofin\n",
      "['treat1', 'cont2', 'cont1', 'cont4']\n",
      "ND_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont4']\n",
      "ND_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont4']\n",
      "ND_nofin\n",
      "['treat1', 'cont2', 'cont1', 'cont4']\n",
      "SND_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont4']\n",
      "SND_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont4']\n",
      "SND_nofin\n"
     ]
    }
   ],
   "source": [
    "#def vimp_plot_uplift()\n",
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "for k in rfdicts_keys:\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\varimp\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    print(treat+cont)\n",
    "    uplift_imp = dict()\n",
    "    for i in treat+cont:\n",
    "        importances = (rfdicts[k][i+'_rf'].feature_importances_)\n",
    "        X_importances = [(label, importance) for label, importance in zip(rfdicts[k][i+'_X_labels'],importances)]\n",
    "        #X_importances = [(round(importance,2), label) for importance, label in zip(importances, rf[i+'_X_labels'])]\n",
    "        X_importances = sorted(X_importances, key = lambda x:x[1], reverse = False)\n",
    "        uplift_imp[i+'_varimp_values'] = [x[1] for x in X_importances]\n",
    "        uplift_imp[i+'_varimp_labels'] = [x[0] for x in X_importances]\n",
    "        uplift_imp[i+'_varimp_tuples'] = X_importances\n",
    "        \n",
    "    \n",
    "    for i in treat + cont:\n",
    "        X_importances = sorted(X_importances, key = lambda x:x[0].upper(), reverse = False) #sort in alphabetical order\n",
    "        uplift_imp[i+'_values'] = [x[1] for x in X_importances] #importances \n",
    "        uplift_imp[i+'_labels'] = [x[0] for x in X_importances] \n",
    "        shape = rfdicts[k][i+'_X'].shape\n",
    "        uplift_imp[i+'_sample'] = shape[0] \n",
    "    \n",
    "    for t in treat:\n",
    "        plotgroups = [t]\n",
    "        for c in cont:\n",
    "            plotgroups = plotgroups + [c] + [c+'_'+t]\n",
    "            uplift_imp[f'{c}_{t}_sample'] = uplift_imp[f'{t}_sample'] + uplift_imp[f'{c}_sample'] \n",
    "            uplift_imp[f'{c}_{t}_varimp_values'] = [uplift_imp[f'{t}_sample']/(uplift_imp[f'{c}_{t}_sample'])*uplift_imp[f'{t}_values'][i] + \n",
    "            uplift_imp[f'{c}_sample']/(uplift_imp[f'{c}_{t}_sample'])*uplift_imp[f'{c}_values'][i] for i in range(len(uplift_imp[f'{t}_values']))]\n",
    "\n",
    "            up_importances = [(label, importance) for label, importance in zip(uplift_imp[f'{t}_labels'],uplift_imp[f'{c}_{t}_varimp_values'])]\n",
    "            up_importances = sorted(up_importances, key = lambda x:x[1], reverse = False)\n",
    "            uplift_imp[f'{c}_{t}_varimp_tuples'] = up_importances            \n",
    "            uplift_imp[f'{c}_{t}_varimp_values'] = [up[1] for up in up_importances]\n",
    "            uplift_imp[f'{c}_{t}_varimp_labels'] = [up[0] for up in up_importances]\n",
    "        print(k)   \n",
    "    for g in plotgroups:\n",
    "        freq_series = pd.Series(uplift_imp[g+'_varimp_values'])\n",
    "        y_labels = uplift_imp[g+'_varimp_labels']\n",
    "\n",
    "        # Plot the figure.\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        ax = freq_series.plot(kind='barh')\n",
    "        ssize=uplift_imp[g+'_sample']\n",
    "        ax.set_title(f'Variable Importance Plot for {k},{g.upper()} Sample')\n",
    "        ax.set_xlabel(f'Frequency, sample size = {str(ssize)}')\n",
    "        ax.set_ylabel(f'Variable')\n",
    "        ax.set_yticklabels(y_labels)\n",
    "        #ax.set_xlim(-40, 300) # expand xlim to make labels easier to read\n",
    "\n",
    "        rects = ax.patches\n",
    "\n",
    "        # For each bar: Place a label\n",
    "        for rect in rects:\n",
    "            # Get X and Y placement of label from rect.\n",
    "            x_value = rect.get_width()\n",
    "            y_value = rect.get_y() + rect.get_height() / 2\n",
    "\n",
    "            # Number of points between bar and label. Change to your liking.\n",
    "            space = 3\n",
    "            # Vertical alignment for positive values\n",
    "            ha = 'left'\n",
    "\n",
    "            # Use X value as label and format number with one decimal place\n",
    "            label = \"{:.3f}\".format(x_value)\n",
    "\n",
    "            # Create annotation\n",
    "            plt.annotate(\n",
    "                label,                      # Use `label` as label\n",
    "                (x_value, y_value),         # Place label at end of the bar\n",
    "                xytext=(space, 0),          # Horizontally shift label by `space`\n",
    "                textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "                va='center',                # Vertically center label\n",
    "                ha=ha)                      # Horizontally align label differently for\n",
    "                                            # positive and negative values.\n",
    "        plt.savefig(f'{newpath}\\\\{vartype}_{g}.pdf')\n",
    "        plt.close()\n",
    "        #plt.savefig(\"image.png\")\n",
    "        #plt.savefig(newpath + '\\\\'+ pathend +f'_{i}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Partial dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6.1** Function for simple partial dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_dependency(rf, X, f_id, feature_ids = []): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the dependency (or partial dependency) of a response variable on a predictor (or multiple predictors)\n",
    "    1. Sample a grid of values of a predictor.\n",
    "    2. For each value, replace every row of that predictor with this value, calculate the average prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    X_temp = X.copy()\n",
    "    \n",
    "    if type(f_id) is int:\n",
    "        if f_id > (X_temp.shape[1]-1):\n",
    "            raise ValueError(f'positional number of {f_id} exceeds array shape')\n",
    "        else:\n",
    "            column = f_id+1\n",
    "    elif type(f_id) is str:\n",
    "        if f_id not in feature_ids:\n",
    "            raise ValueError(f\"explanatory variable {f_id} is not in data frame or feature_ids is not passed to the function\")\n",
    "        else:\n",
    "            f_id = feature_ids.index(f_id)\n",
    "    else:\n",
    "        raise ValueError('f_id needs to be either an integer or a string')\n",
    "        #return\n",
    "    \n",
    "    grid = np.linspace(np.percentile(X_temp[:, f_id], 0.1),\n",
    "                       np.percentile(X_temp[:, f_id], 99.5),\n",
    "                       100)\n",
    "    y_pred = np.zeros(len(grid))\n",
    "    \n",
    "    for i, val in enumerate(grid): # i returns the counter, val returns the value at position of counter on grid\n",
    "        X_temp[:, f_id] = val\n",
    "        #data = xgb.DMatrix( X_temp[:, feature_ids].reshape( (len(X_temp), len(feature_ids)) ) )\n",
    "        y_pred[i] = np.average(rf.predict(X_temp)) #any function other than mean is also possible\n",
    "    return grid, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6.2** Function for uplift 2model partial dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_2m_partial_dependency(rf_treat, rf_cont, f_id, X_treat, X_cont=[], feature_ids_treat=[], feature_ids_cont=[], types=['mean'], percentile='none', grid_lower=5, grid_upper=95 ): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor (or multiple predictors) in a random forest uplift 2 model approach.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    y_pred: corresponding predicted values of dependent variable (type: ndarray). \n",
    "            If input types is a list the columns in the array correspond to the chosen types in the same order\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is int):\n",
    "                if f_id > (X_temp.shape[1]-1):\n",
    "                    raise ValueError(f'positional number of {f_id} exceeds array shape')\n",
    "                else:\n",
    "                    X_temp = X_treat.copy()\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str):\n",
    "                if f_id not in feature_ids:\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids is not passed to the function')\n",
    "                else:\n",
    "                    f_id = feature_ids.index(f_id)\n",
    "                    f_id_label = f_id\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('f_id needs to be either an integer or a string')\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(f_id) is int):\n",
    "                raise ValueError(f'if X_cont is specified, then f_id needs to be a string variable')\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str) & ((f_id not in feature_ids_treat) |  (f_id not in feature_ids_cont)):\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids_treat or feature_ids_cont is not passed to the function')\n",
    "            else:\n",
    "                if sorted(feature_ids_treat)!=sorted(feature_ids_cont):\n",
    "                    raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont')\n",
    "                else:\n",
    "                    X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                    X_labels = list(X.columns)\n",
    "                    X_temp = np.array(X)\n",
    "                    f_id_label = f_id\n",
    "                    f_id = X_labels.index(f_id)             \n",
    "        else: \n",
    "            raise ValueError('Either X_cont or X_treat does not have an array like structure')\n",
    "\n",
    "        grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower),\n",
    "                           np.percentile(X_temp[:, f_id], grid_upper),\n",
    "                           100)\n",
    "        \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types\n",
    "        \n",
    "        if set(types) <= set(function_labels):\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i]\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif not 0<=percentile<=100:\n",
    "                    raise ValueError('percentile out of range')\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "\n",
    "    \n",
    "        y_pred = np.zeros((len(grid), len(types)))\n",
    "\n",
    "        for i, val in enumerate(grid): # i returns the counter, val returns the value at position of counter on grid\n",
    "            X_temp[:, f_id] = val\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[i,j] = nptypes[j](y_temp,percentile)\n",
    "                else:\n",
    "                    y_pred[i,j] = nptypes[j](y_temp)\n",
    "\n",
    "        for j in range(len(column_labels)):\n",
    "            if column_labels[j] == 'percentile':\n",
    "                column_labels[j] = 'percentile_' + str(percentile)\n",
    "            else:\n",
    "                pass       \n",
    "        column_labels = ['grid']+column_labels\n",
    "        column_labels = [str(f_id_label)+ '_' + lab for lab in column_labels]\n",
    "\n",
    "        pd = pd.DataFrame(np.c_[grid, y_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return pd #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[], types=['mean'], percentile='none', grid_lower=5, grid_upper=95 ): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor (or multiple predictors) in a random forest uplift 2 model approach.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    y_pred: corresponding predicted values of dependent variable (type: ndarray). \n",
    "            If input types is a list the columns in the array correspond to the chosen types in the same order\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is int):\n",
    "                if f_id > (X_temp.shape[1]-1):\n",
    "                    raise ValueError(f'positional number of {f_id} exceeds array shape')\n",
    "                else:\n",
    "                    X_temp = X_treat.copy()\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str):\n",
    "                if f_id not in feature_ids:\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids is not passed to the function')\n",
    "                else:\n",
    "                    f_id = feature_ids.index(f_id)\n",
    "                    f_id_label = f_id\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('f_id needs to be either an integer or a string')\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(f_id) is int):\n",
    "                raise ValueError(f'if X_cont is specified, then f_id needs to be a string variable')\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str) & ((f_id not in feature_ids_treat) |  (f_id not in feature_ids_cont)):\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids_treat or feature_ids_cont is not passed to the function')\n",
    "            else:\n",
    "                if (sorted(feature_ids_treat)!=sorted(feature_ids_cont)) & (type(X_cont_rbtamt) is not np.ndarray) :\n",
    "                    raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                else:\n",
    "                    X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = np.array(mean_rbt)\n",
    "                    X_labels = list(X.columns)\n",
    "                    X_temp = np.array(X)\n",
    "                    f_id_label = f_id\n",
    "                    f_id = X_labels.index(f_id)             \n",
    "        else: \n",
    "            raise ValueError('Either X_cont or X_treat does not have an array like structure')\n",
    "\n",
    "        grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower),\n",
    "                           np.percentile(X_temp[:, f_id], grid_upper),\n",
    "                           100)\n",
    "        \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types\n",
    "        \n",
    "        if set(types) <= set(function_labels):\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i]\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif not 0<=percentile<=100:\n",
    "                    raise ValueError('percentile out of range')\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "\n",
    "    \n",
    "        y_pred = np.zeros((len(grid), len(types)))\n",
    "        mpc_pred = np.zeros((len(grid), len(types)))\n",
    "        for i, val in enumerate(grid): # i returns the counter, val returns the value at position of counter on grid\n",
    "            X_temp[:, f_id] = val\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[i,j] = nptypes[j](y_temp,percentile)\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp,percentile)\n",
    "                else:\n",
    "                    y_pred[i,j] = nptypes[j](y_temp)\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp)\n",
    "        for j in range(len(column_labels)):\n",
    "            if column_labels[j] == 'percentile':\n",
    "                column_labels[j] = 'percentile_' + str(percentile)\n",
    "            else:\n",
    "                pass \n",
    "        column_labels_cr = ['cr_'+ lab for lab in column_labels]\n",
    "        column_labels_mpc = ['mpc_' + lab for lab in column_labels]\n",
    "        column_labels = ['grid']+column_labels_cr+column_labels_mpc\n",
    "        column_labels = [str(f_id_label)+ '_' + lab for lab in column_labels]\n",
    "\n",
    "        pd = pd.DataFrame(np.c_[grid, y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return pd #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run partial dependence function for given sample and explanatory variables. this may take a while. Hence, save as later as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "\n",
    "rfdicts_keys = list(rfdicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(rfdicts[rfdicts_keys[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def vimp_plot_uplift()\n",
    "rfdicts_keys = list(rfdicts)\n",
    "#rfdicts_keys = rfdicts_keys[:1]\n",
    "print(rfdicts_keys)\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\pdp\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "    for t in treat:\n",
    "        for c in cont:\n",
    "            expvars = rfdicts[k][c+'_X_labels']           \n",
    "            pdp = uplift_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],expvars[0],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "            X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'], \n",
    "            feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=30)\n",
    "            for v in expvars[1:]:\n",
    "                pdp = pdp.join(uplift_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'], \n",
    "                feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=30))\n",
    "                \n",
    "            rfdicts[k][f'{c}_{t}_pdp'] = pdp\n",
    "            pdp.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}.csv')\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vars = expvars\n",
    "#\n",
    "#pdp = uplift_2m_partial_dependency(rf['treat_rf'], rf['cont_rf'], vars[0], rf['treat_X'], rf['cont_X'], \n",
    "#                                            feature_ids_treat=rf['treat_X_labels'], feature_ids_cont = rf['cont_X_labels'], \n",
    "#                                             types = ['mean','percentile','std','median'], percentile=30)\n",
    "#\n",
    "#for var in vars[1:]:\n",
    "#    pdp = pdp.join(uplift_2m_partial_dependency(rf['treat_rf'], rf['cont_rf'], var, rf['treat_X'], rf['cont_X'], \n",
    "#                                                feature_ids_treat=rf['treat_X_labels'], feature_ids_cont = rf['cont_X_labels'],                                                 \n",
    "#                                                types = ['mean','percentile','std','median'], percentile=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vars = ['age','FINCBTAX']\n",
    "#\n",
    "#pdp = uplift_2m_partial_dependency_mpc(rf['treat_rf'], rf['cont_rf'], vars[0], rf['treat_X'], rf['treat_rbtamt'], X_cont = rf['cont_X'], X_cont_rbtamt=rf['cont_rbtamt'], \n",
    "#                                            feature_ids_treat=rf['treat_X_labels'], feature_ids_cont = rf['cont_X_labels'], \n",
    "#                                             types = ['mean','percentile','std','median'], percentile=30)\n",
    "#\n",
    "#for var in vars[1:]:\n",
    "#    pdp = pdp.join(uplift_2m_partial_dependency(rf['treat_rf'], rf['cont_rf'], var, rf['treat_X'], rf['cont_X'], \n",
    "#                                                feature_ids_treat=rf['treat_X_labels'], feature_ids_cont = rf['cont_X_labels'],                                                 \n",
    "#                                                types = ['mean','percentile','std','median'], percentile=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data frame as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_grid</th>\n",
       "      <th>age_cr_mean</th>\n",
       "      <th>age_cr_percentile_30</th>\n",
       "      <th>age_cr_std</th>\n",
       "      <th>age_cr_median</th>\n",
       "      <th>age_mpc_mean</th>\n",
       "      <th>age_mpc_percentile_30</th>\n",
       "      <th>age_mpc_std</th>\n",
       "      <th>age_mpc_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>370.843239</td>\n",
       "      <td>132.853910</td>\n",
       "      <td>506.938559</td>\n",
       "      <td>365.924840</td>\n",
       "      <td>0.958758</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>269.890186</td>\n",
       "      <td>1.628889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.404040</td>\n",
       "      <td>380.987243</td>\n",
       "      <td>141.193544</td>\n",
       "      <td>508.634730</td>\n",
       "      <td>375.626818</td>\n",
       "      <td>1.743750</td>\n",
       "      <td>0.771117</td>\n",
       "      <td>91.994473</td>\n",
       "      <td>1.625173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.808081</td>\n",
       "      <td>382.083828</td>\n",
       "      <td>143.253810</td>\n",
       "      <td>509.247192</td>\n",
       "      <td>377.409834</td>\n",
       "      <td>0.995660</td>\n",
       "      <td>0.771237</td>\n",
       "      <td>95.569600</td>\n",
       "      <td>1.623875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.212121</td>\n",
       "      <td>383.284882</td>\n",
       "      <td>144.689416</td>\n",
       "      <td>509.304835</td>\n",
       "      <td>378.245810</td>\n",
       "      <td>-93.454401</td>\n",
       "      <td>0.776429</td>\n",
       "      <td>8749.393515</td>\n",
       "      <td>1.626481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.616162</td>\n",
       "      <td>386.513091</td>\n",
       "      <td>148.974087</td>\n",
       "      <td>511.650902</td>\n",
       "      <td>379.880213</td>\n",
       "      <td>-9.506080</td>\n",
       "      <td>0.764236</td>\n",
       "      <td>1043.286742</td>\n",
       "      <td>1.607405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30.020202</td>\n",
       "      <td>384.373291</td>\n",
       "      <td>146.935043</td>\n",
       "      <td>511.830827</td>\n",
       "      <td>379.834350</td>\n",
       "      <td>2.286673</td>\n",
       "      <td>0.760356</td>\n",
       "      <td>74.746137</td>\n",
       "      <td>1.604022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30.424242</td>\n",
       "      <td>386.338028</td>\n",
       "      <td>149.271362</td>\n",
       "      <td>512.081182</td>\n",
       "      <td>381.012704</td>\n",
       "      <td>2.702126</td>\n",
       "      <td>0.759806</td>\n",
       "      <td>301.364391</td>\n",
       "      <td>1.599085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30.828283</td>\n",
       "      <td>389.423591</td>\n",
       "      <td>151.822197</td>\n",
       "      <td>515.317748</td>\n",
       "      <td>383.138657</td>\n",
       "      <td>-106.072405</td>\n",
       "      <td>0.755318</td>\n",
       "      <td>9288.889446</td>\n",
       "      <td>1.588675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31.232323</td>\n",
       "      <td>392.145991</td>\n",
       "      <td>153.803107</td>\n",
       "      <td>516.066230</td>\n",
       "      <td>387.548576</td>\n",
       "      <td>0.193688</td>\n",
       "      <td>0.758671</td>\n",
       "      <td>164.479699</td>\n",
       "      <td>1.588461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31.636364</td>\n",
       "      <td>398.500335</td>\n",
       "      <td>155.294120</td>\n",
       "      <td>522.816542</td>\n",
       "      <td>390.806746</td>\n",
       "      <td>2.394913</td>\n",
       "      <td>0.753275</td>\n",
       "      <td>66.856040</td>\n",
       "      <td>1.570227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32.040404</td>\n",
       "      <td>397.619932</td>\n",
       "      <td>152.845431</td>\n",
       "      <td>523.400088</td>\n",
       "      <td>391.825781</td>\n",
       "      <td>1.544964</td>\n",
       "      <td>0.741349</td>\n",
       "      <td>157.924522</td>\n",
       "      <td>1.559890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32.444444</td>\n",
       "      <td>395.966401</td>\n",
       "      <td>151.338705</td>\n",
       "      <td>523.603578</td>\n",
       "      <td>389.608423</td>\n",
       "      <td>3.575192</td>\n",
       "      <td>0.740804</td>\n",
       "      <td>178.946930</td>\n",
       "      <td>1.563272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32.848485</td>\n",
       "      <td>395.251531</td>\n",
       "      <td>147.378484</td>\n",
       "      <td>524.890021</td>\n",
       "      <td>386.814417</td>\n",
       "      <td>6.553764</td>\n",
       "      <td>0.733087</td>\n",
       "      <td>467.264972</td>\n",
       "      <td>1.555269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33.252525</td>\n",
       "      <td>394.692859</td>\n",
       "      <td>145.944484</td>\n",
       "      <td>526.012359</td>\n",
       "      <td>386.322392</td>\n",
       "      <td>-9.127129</td>\n",
       "      <td>0.730543</td>\n",
       "      <td>771.555013</td>\n",
       "      <td>1.553527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>33.656566</td>\n",
       "      <td>394.771779</td>\n",
       "      <td>147.277546</td>\n",
       "      <td>526.523284</td>\n",
       "      <td>385.521189</td>\n",
       "      <td>-1.175959</td>\n",
       "      <td>0.730930</td>\n",
       "      <td>364.325320</td>\n",
       "      <td>1.555026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>34.060606</td>\n",
       "      <td>391.388363</td>\n",
       "      <td>142.354616</td>\n",
       "      <td>526.169424</td>\n",
       "      <td>382.181560</td>\n",
       "      <td>0.214875</td>\n",
       "      <td>0.722078</td>\n",
       "      <td>113.060818</td>\n",
       "      <td>1.549723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>34.464646</td>\n",
       "      <td>388.660079</td>\n",
       "      <td>140.291226</td>\n",
       "      <td>526.395167</td>\n",
       "      <td>381.436169</td>\n",
       "      <td>-1.517814</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>487.595912</td>\n",
       "      <td>1.551908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>34.868687</td>\n",
       "      <td>387.994306</td>\n",
       "      <td>138.180324</td>\n",
       "      <td>527.155288</td>\n",
       "      <td>380.920113</td>\n",
       "      <td>0.663406</td>\n",
       "      <td>0.711543</td>\n",
       "      <td>66.983365</td>\n",
       "      <td>1.545646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>35.272727</td>\n",
       "      <td>382.528014</td>\n",
       "      <td>131.121276</td>\n",
       "      <td>528.778613</td>\n",
       "      <td>375.827540</td>\n",
       "      <td>-7.026872</td>\n",
       "      <td>0.701778</td>\n",
       "      <td>612.905942</td>\n",
       "      <td>1.543342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>35.676768</td>\n",
       "      <td>380.079103</td>\n",
       "      <td>126.676270</td>\n",
       "      <td>529.324866</td>\n",
       "      <td>373.453375</td>\n",
       "      <td>-1.014282</td>\n",
       "      <td>0.695111</td>\n",
       "      <td>130.497185</td>\n",
       "      <td>1.540265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>36.080808</td>\n",
       "      <td>377.254983</td>\n",
       "      <td>124.461958</td>\n",
       "      <td>529.336400</td>\n",
       "      <td>370.395840</td>\n",
       "      <td>-2.547803</td>\n",
       "      <td>0.686733</td>\n",
       "      <td>245.368158</td>\n",
       "      <td>1.537032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>36.484848</td>\n",
       "      <td>364.446898</td>\n",
       "      <td>112.628164</td>\n",
       "      <td>528.816883</td>\n",
       "      <td>358.671400</td>\n",
       "      <td>2.421474</td>\n",
       "      <td>0.673641</td>\n",
       "      <td>369.907057</td>\n",
       "      <td>1.543784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>36.888889</td>\n",
       "      <td>364.183590</td>\n",
       "      <td>111.372869</td>\n",
       "      <td>530.096930</td>\n",
       "      <td>359.300803</td>\n",
       "      <td>7.399130</td>\n",
       "      <td>0.670583</td>\n",
       "      <td>253.511363</td>\n",
       "      <td>1.544912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>37.292929</td>\n",
       "      <td>362.855421</td>\n",
       "      <td>108.521272</td>\n",
       "      <td>531.118170</td>\n",
       "      <td>357.274985</td>\n",
       "      <td>28.821298</td>\n",
       "      <td>0.664715</td>\n",
       "      <td>2381.017430</td>\n",
       "      <td>1.540570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>37.696970</td>\n",
       "      <td>360.664645</td>\n",
       "      <td>107.666422</td>\n",
       "      <td>530.551893</td>\n",
       "      <td>355.454845</td>\n",
       "      <td>12.044123</td>\n",
       "      <td>0.661755</td>\n",
       "      <td>1073.462463</td>\n",
       "      <td>1.543093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>38.101010</td>\n",
       "      <td>361.080984</td>\n",
       "      <td>108.898005</td>\n",
       "      <td>530.283664</td>\n",
       "      <td>356.581862</td>\n",
       "      <td>1.287046</td>\n",
       "      <td>0.661964</td>\n",
       "      <td>93.771369</td>\n",
       "      <td>1.540632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38.505051</td>\n",
       "      <td>363.479839</td>\n",
       "      <td>110.740168</td>\n",
       "      <td>531.225245</td>\n",
       "      <td>357.775833</td>\n",
       "      <td>6.257838</td>\n",
       "      <td>0.661196</td>\n",
       "      <td>287.724582</td>\n",
       "      <td>1.531524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>38.909091</td>\n",
       "      <td>368.057137</td>\n",
       "      <td>115.651710</td>\n",
       "      <td>532.687640</td>\n",
       "      <td>362.696229</td>\n",
       "      <td>0.893151</td>\n",
       "      <td>0.659390</td>\n",
       "      <td>137.442297</td>\n",
       "      <td>1.522954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>39.313131</td>\n",
       "      <td>368.906510</td>\n",
       "      <td>116.417693</td>\n",
       "      <td>533.062133</td>\n",
       "      <td>363.679682</td>\n",
       "      <td>5.177576</td>\n",
       "      <td>0.658980</td>\n",
       "      <td>450.378009</td>\n",
       "      <td>1.523138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>39.717172</td>\n",
       "      <td>367.704575</td>\n",
       "      <td>113.628971</td>\n",
       "      <td>533.793928</td>\n",
       "      <td>362.239059</td>\n",
       "      <td>30.494894</td>\n",
       "      <td>0.659107</td>\n",
       "      <td>2464.461257</td>\n",
       "      <td>1.525532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>56.282828</td>\n",
       "      <td>374.992295</td>\n",
       "      <td>142.359414</td>\n",
       "      <td>520.654393</td>\n",
       "      <td>371.957118</td>\n",
       "      <td>191.697454</td>\n",
       "      <td>0.780223</td>\n",
       "      <td>16546.457265</td>\n",
       "      <td>1.635368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>56.686869</td>\n",
       "      <td>378.449606</td>\n",
       "      <td>148.932263</td>\n",
       "      <td>518.580886</td>\n",
       "      <td>374.643152</td>\n",
       "      <td>-18.297035</td>\n",
       "      <td>0.795690</td>\n",
       "      <td>1785.379022</td>\n",
       "      <td>1.645308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>57.090909</td>\n",
       "      <td>380.994813</td>\n",
       "      <td>151.021029</td>\n",
       "      <td>517.856985</td>\n",
       "      <td>378.981860</td>\n",
       "      <td>14.893062</td>\n",
       "      <td>0.805382</td>\n",
       "      <td>859.232628</td>\n",
       "      <td>1.653755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>57.494949</td>\n",
       "      <td>376.077438</td>\n",
       "      <td>144.265823</td>\n",
       "      <td>517.959845</td>\n",
       "      <td>374.400999</td>\n",
       "      <td>13.189363</td>\n",
       "      <td>0.803199</td>\n",
       "      <td>1011.224175</td>\n",
       "      <td>1.658700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>57.898990</td>\n",
       "      <td>363.719091</td>\n",
       "      <td>135.892979</td>\n",
       "      <td>516.109379</td>\n",
       "      <td>363.317697</td>\n",
       "      <td>2.812707</td>\n",
       "      <td>0.785967</td>\n",
       "      <td>99.012475</td>\n",
       "      <td>1.653340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>58.303030</td>\n",
       "      <td>358.498838</td>\n",
       "      <td>129.940847</td>\n",
       "      <td>516.021521</td>\n",
       "      <td>361.821000</td>\n",
       "      <td>4.007176</td>\n",
       "      <td>0.766138</td>\n",
       "      <td>222.502283</td>\n",
       "      <td>1.641098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>58.707071</td>\n",
       "      <td>358.694403</td>\n",
       "      <td>133.514600</td>\n",
       "      <td>515.561054</td>\n",
       "      <td>362.795396</td>\n",
       "      <td>-1.324747</td>\n",
       "      <td>0.767643</td>\n",
       "      <td>163.664933</td>\n",
       "      <td>1.642547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>59.111111</td>\n",
       "      <td>351.333181</td>\n",
       "      <td>126.325748</td>\n",
       "      <td>513.398504</td>\n",
       "      <td>359.680578</td>\n",
       "      <td>280.575716</td>\n",
       "      <td>0.761092</td>\n",
       "      <td>24429.776400</td>\n",
       "      <td>1.646673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>59.515152</td>\n",
       "      <td>355.829983</td>\n",
       "      <td>132.179613</td>\n",
       "      <td>513.071975</td>\n",
       "      <td>363.230945</td>\n",
       "      <td>9.733424</td>\n",
       "      <td>0.770910</td>\n",
       "      <td>761.541311</td>\n",
       "      <td>1.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>59.919192</td>\n",
       "      <td>346.929156</td>\n",
       "      <td>123.025722</td>\n",
       "      <td>513.571913</td>\n",
       "      <td>356.639297</td>\n",
       "      <td>1.833439</td>\n",
       "      <td>0.749572</td>\n",
       "      <td>77.192531</td>\n",
       "      <td>1.641052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>60.323232</td>\n",
       "      <td>347.575887</td>\n",
       "      <td>121.739735</td>\n",
       "      <td>512.484905</td>\n",
       "      <td>355.595821</td>\n",
       "      <td>0.881245</td>\n",
       "      <td>0.748608</td>\n",
       "      <td>57.374690</td>\n",
       "      <td>1.640532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>60.727273</td>\n",
       "      <td>344.155821</td>\n",
       "      <td>118.464128</td>\n",
       "      <td>512.359692</td>\n",
       "      <td>350.880792</td>\n",
       "      <td>0.041314</td>\n",
       "      <td>0.739995</td>\n",
       "      <td>84.815778</td>\n",
       "      <td>1.637448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>61.131313</td>\n",
       "      <td>333.429207</td>\n",
       "      <td>110.186664</td>\n",
       "      <td>513.404042</td>\n",
       "      <td>340.980088</td>\n",
       "      <td>1.234134</td>\n",
       "      <td>0.724022</td>\n",
       "      <td>129.780184</td>\n",
       "      <td>1.636767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>61.535354</td>\n",
       "      <td>325.605748</td>\n",
       "      <td>102.724200</td>\n",
       "      <td>509.554965</td>\n",
       "      <td>332.738936</td>\n",
       "      <td>4.893000</td>\n",
       "      <td>0.716283</td>\n",
       "      <td>729.572057</td>\n",
       "      <td>1.643339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>61.939394</td>\n",
       "      <td>319.134293</td>\n",
       "      <td>97.318066</td>\n",
       "      <td>509.603537</td>\n",
       "      <td>325.893763</td>\n",
       "      <td>4.513358</td>\n",
       "      <td>0.706727</td>\n",
       "      <td>291.618988</td>\n",
       "      <td>1.644712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>62.343434</td>\n",
       "      <td>312.900925</td>\n",
       "      <td>92.262503</td>\n",
       "      <td>508.889092</td>\n",
       "      <td>320.385073</td>\n",
       "      <td>2.188431</td>\n",
       "      <td>0.698066</td>\n",
       "      <td>106.779095</td>\n",
       "      <td>1.650858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>62.747475</td>\n",
       "      <td>310.847374</td>\n",
       "      <td>90.345212</td>\n",
       "      <td>508.067666</td>\n",
       "      <td>317.252419</td>\n",
       "      <td>18.920028</td>\n",
       "      <td>0.695289</td>\n",
       "      <td>1618.348184</td>\n",
       "      <td>1.655978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>63.151515</td>\n",
       "      <td>314.705315</td>\n",
       "      <td>94.448238</td>\n",
       "      <td>506.379438</td>\n",
       "      <td>322.606984</td>\n",
       "      <td>7.136936</td>\n",
       "      <td>0.717376</td>\n",
       "      <td>262.813738</td>\n",
       "      <td>1.668704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>63.555556</td>\n",
       "      <td>310.681635</td>\n",
       "      <td>91.150672</td>\n",
       "      <td>505.879887</td>\n",
       "      <td>321.078854</td>\n",
       "      <td>3.356241</td>\n",
       "      <td>0.704921</td>\n",
       "      <td>300.078208</td>\n",
       "      <td>1.666226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>63.959596</td>\n",
       "      <td>301.968339</td>\n",
       "      <td>80.889603</td>\n",
       "      <td>506.047890</td>\n",
       "      <td>312.123554</td>\n",
       "      <td>-0.133859</td>\n",
       "      <td>0.678081</td>\n",
       "      <td>139.577481</td>\n",
       "      <td>1.656981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>64.363636</td>\n",
       "      <td>301.367046</td>\n",
       "      <td>80.620831</td>\n",
       "      <td>505.933727</td>\n",
       "      <td>311.714472</td>\n",
       "      <td>6.224494</td>\n",
       "      <td>0.679974</td>\n",
       "      <td>344.302009</td>\n",
       "      <td>1.661042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>64.767677</td>\n",
       "      <td>297.596520</td>\n",
       "      <td>78.173976</td>\n",
       "      <td>506.634021</td>\n",
       "      <td>307.496430</td>\n",
       "      <td>2.292101</td>\n",
       "      <td>0.672011</td>\n",
       "      <td>140.904807</td>\n",
       "      <td>1.660106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>65.171717</td>\n",
       "      <td>297.806507</td>\n",
       "      <td>78.682072</td>\n",
       "      <td>506.717713</td>\n",
       "      <td>307.557143</td>\n",
       "      <td>-1.081125</td>\n",
       "      <td>0.671885</td>\n",
       "      <td>262.962889</td>\n",
       "      <td>1.658886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>65.575758</td>\n",
       "      <td>295.241250</td>\n",
       "      <td>76.505955</td>\n",
       "      <td>506.700852</td>\n",
       "      <td>302.721790</td>\n",
       "      <td>4.391816</td>\n",
       "      <td>0.652783</td>\n",
       "      <td>216.250036</td>\n",
       "      <td>1.647989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>65.979798</td>\n",
       "      <td>291.285319</td>\n",
       "      <td>73.089960</td>\n",
       "      <td>506.933809</td>\n",
       "      <td>298.577505</td>\n",
       "      <td>3.943861</td>\n",
       "      <td>0.635328</td>\n",
       "      <td>215.223080</td>\n",
       "      <td>1.640948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>66.383838</td>\n",
       "      <td>288.307002</td>\n",
       "      <td>70.516088</td>\n",
       "      <td>506.718372</td>\n",
       "      <td>295.118839</td>\n",
       "      <td>0.831224</td>\n",
       "      <td>0.630194</td>\n",
       "      <td>50.165295</td>\n",
       "      <td>1.644417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>66.787879</td>\n",
       "      <td>287.749749</td>\n",
       "      <td>71.265814</td>\n",
       "      <td>505.086465</td>\n",
       "      <td>293.168701</td>\n",
       "      <td>4.105404</td>\n",
       "      <td>0.642503</td>\n",
       "      <td>209.909644</td>\n",
       "      <td>1.660653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>67.191919</td>\n",
       "      <td>287.598966</td>\n",
       "      <td>70.712084</td>\n",
       "      <td>505.145229</td>\n",
       "      <td>293.178884</td>\n",
       "      <td>-5.100644</td>\n",
       "      <td>0.642069</td>\n",
       "      <td>602.317451</td>\n",
       "      <td>1.661314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>67.595960</td>\n",
       "      <td>284.107138</td>\n",
       "      <td>63.092589</td>\n",
       "      <td>505.686333</td>\n",
       "      <td>290.924256</td>\n",
       "      <td>-2.360790</td>\n",
       "      <td>0.622516</td>\n",
       "      <td>484.180990</td>\n",
       "      <td>1.655262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>68.000000</td>\n",
       "      <td>285.253027</td>\n",
       "      <td>63.613822</td>\n",
       "      <td>505.745380</td>\n",
       "      <td>292.558761</td>\n",
       "      <td>0.077427</td>\n",
       "      <td>0.624622</td>\n",
       "      <td>159.389980</td>\n",
       "      <td>1.652096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age_grid  age_cr_mean  age_cr_percentile_30  age_cr_std  age_cr_median  \\\n",
       "0   28.000000   370.843239            132.853910  506.938559     365.924840   \n",
       "1   28.404040   380.987243            141.193544  508.634730     375.626818   \n",
       "2   28.808081   382.083828            143.253810  509.247192     377.409834   \n",
       "3   29.212121   383.284882            144.689416  509.304835     378.245810   \n",
       "4   29.616162   386.513091            148.974087  511.650902     379.880213   \n",
       "5   30.020202   384.373291            146.935043  511.830827     379.834350   \n",
       "6   30.424242   386.338028            149.271362  512.081182     381.012704   \n",
       "7   30.828283   389.423591            151.822197  515.317748     383.138657   \n",
       "8   31.232323   392.145991            153.803107  516.066230     387.548576   \n",
       "9   31.636364   398.500335            155.294120  522.816542     390.806746   \n",
       "10  32.040404   397.619932            152.845431  523.400088     391.825781   \n",
       "11  32.444444   395.966401            151.338705  523.603578     389.608423   \n",
       "12  32.848485   395.251531            147.378484  524.890021     386.814417   \n",
       "13  33.252525   394.692859            145.944484  526.012359     386.322392   \n",
       "14  33.656566   394.771779            147.277546  526.523284     385.521189   \n",
       "15  34.060606   391.388363            142.354616  526.169424     382.181560   \n",
       "16  34.464646   388.660079            140.291226  526.395167     381.436169   \n",
       "17  34.868687   387.994306            138.180324  527.155288     380.920113   \n",
       "18  35.272727   382.528014            131.121276  528.778613     375.827540   \n",
       "19  35.676768   380.079103            126.676270  529.324866     373.453375   \n",
       "20  36.080808   377.254983            124.461958  529.336400     370.395840   \n",
       "21  36.484848   364.446898            112.628164  528.816883     358.671400   \n",
       "22  36.888889   364.183590            111.372869  530.096930     359.300803   \n",
       "23  37.292929   362.855421            108.521272  531.118170     357.274985   \n",
       "24  37.696970   360.664645            107.666422  530.551893     355.454845   \n",
       "25  38.101010   361.080984            108.898005  530.283664     356.581862   \n",
       "26  38.505051   363.479839            110.740168  531.225245     357.775833   \n",
       "27  38.909091   368.057137            115.651710  532.687640     362.696229   \n",
       "28  39.313131   368.906510            116.417693  533.062133     363.679682   \n",
       "29  39.717172   367.704575            113.628971  533.793928     362.239059   \n",
       "..        ...          ...                   ...         ...            ...   \n",
       "70  56.282828   374.992295            142.359414  520.654393     371.957118   \n",
       "71  56.686869   378.449606            148.932263  518.580886     374.643152   \n",
       "72  57.090909   380.994813            151.021029  517.856985     378.981860   \n",
       "73  57.494949   376.077438            144.265823  517.959845     374.400999   \n",
       "74  57.898990   363.719091            135.892979  516.109379     363.317697   \n",
       "75  58.303030   358.498838            129.940847  516.021521     361.821000   \n",
       "76  58.707071   358.694403            133.514600  515.561054     362.795396   \n",
       "77  59.111111   351.333181            126.325748  513.398504     359.680578   \n",
       "78  59.515152   355.829983            132.179613  513.071975     363.230945   \n",
       "79  59.919192   346.929156            123.025722  513.571913     356.639297   \n",
       "80  60.323232   347.575887            121.739735  512.484905     355.595821   \n",
       "81  60.727273   344.155821            118.464128  512.359692     350.880792   \n",
       "82  61.131313   333.429207            110.186664  513.404042     340.980088   \n",
       "83  61.535354   325.605748            102.724200  509.554965     332.738936   \n",
       "84  61.939394   319.134293             97.318066  509.603537     325.893763   \n",
       "85  62.343434   312.900925             92.262503  508.889092     320.385073   \n",
       "86  62.747475   310.847374             90.345212  508.067666     317.252419   \n",
       "87  63.151515   314.705315             94.448238  506.379438     322.606984   \n",
       "88  63.555556   310.681635             91.150672  505.879887     321.078854   \n",
       "89  63.959596   301.968339             80.889603  506.047890     312.123554   \n",
       "90  64.363636   301.367046             80.620831  505.933727     311.714472   \n",
       "91  64.767677   297.596520             78.173976  506.634021     307.496430   \n",
       "92  65.171717   297.806507             78.682072  506.717713     307.557143   \n",
       "93  65.575758   295.241250             76.505955  506.700852     302.721790   \n",
       "94  65.979798   291.285319             73.089960  506.933809     298.577505   \n",
       "95  66.383838   288.307002             70.516088  506.718372     295.118839   \n",
       "96  66.787879   287.749749             71.265814  505.086465     293.168701   \n",
       "97  67.191919   287.598966             70.712084  505.145229     293.178884   \n",
       "98  67.595960   284.107138             63.092589  505.686333     290.924256   \n",
       "99  68.000000   285.253027             63.613822  505.745380     292.558761   \n",
       "\n",
       "    age_mpc_mean  age_mpc_percentile_30   age_mpc_std  age_mpc_median  \n",
       "0       0.958758               0.758333    269.890186        1.628889  \n",
       "1       1.743750               0.771117     91.994473        1.625173  \n",
       "2       0.995660               0.771237     95.569600        1.623875  \n",
       "3     -93.454401               0.776429   8749.393515        1.626481  \n",
       "4      -9.506080               0.764236   1043.286742        1.607405  \n",
       "5       2.286673               0.760356     74.746137        1.604022  \n",
       "6       2.702126               0.759806    301.364391        1.599085  \n",
       "7    -106.072405               0.755318   9288.889446        1.588675  \n",
       "8       0.193688               0.758671    164.479699        1.588461  \n",
       "9       2.394913               0.753275     66.856040        1.570227  \n",
       "10      1.544964               0.741349    157.924522        1.559890  \n",
       "11      3.575192               0.740804    178.946930        1.563272  \n",
       "12      6.553764               0.733087    467.264972        1.555269  \n",
       "13     -9.127129               0.730543    771.555013        1.553527  \n",
       "14     -1.175959               0.730930    364.325320        1.555026  \n",
       "15      0.214875               0.722078    113.060818        1.549723  \n",
       "16     -1.517814               0.717293    487.595912        1.551908  \n",
       "17      0.663406               0.711543     66.983365        1.545646  \n",
       "18     -7.026872               0.701778    612.905942        1.543342  \n",
       "19     -1.014282               0.695111    130.497185        1.540265  \n",
       "20     -2.547803               0.686733    245.368158        1.537032  \n",
       "21      2.421474               0.673641    369.907057        1.543784  \n",
       "22      7.399130               0.670583    253.511363        1.544912  \n",
       "23     28.821298               0.664715   2381.017430        1.540570  \n",
       "24     12.044123               0.661755   1073.462463        1.543093  \n",
       "25      1.287046               0.661964     93.771369        1.540632  \n",
       "26      6.257838               0.661196    287.724582        1.531524  \n",
       "27      0.893151               0.659390    137.442297        1.522954  \n",
       "28      5.177576               0.658980    450.378009        1.523138  \n",
       "29     30.494894               0.659107   2464.461257        1.525532  \n",
       "..           ...                    ...           ...             ...  \n",
       "70    191.697454               0.780223  16546.457265        1.635368  \n",
       "71    -18.297035               0.795690   1785.379022        1.645308  \n",
       "72     14.893062               0.805382    859.232628        1.653755  \n",
       "73     13.189363               0.803199   1011.224175        1.658700  \n",
       "74      2.812707               0.785967     99.012475        1.653340  \n",
       "75      4.007176               0.766138    222.502283        1.641098  \n",
       "76     -1.324747               0.767643    163.664933        1.642547  \n",
       "77    280.575716               0.761092  24429.776400        1.646673  \n",
       "78      9.733424               0.770910    761.541311        1.646700  \n",
       "79      1.833439               0.749572     77.192531        1.641052  \n",
       "80      0.881245               0.748608     57.374690        1.640532  \n",
       "81      0.041314               0.739995     84.815778        1.637448  \n",
       "82      1.234134               0.724022    129.780184        1.636767  \n",
       "83      4.893000               0.716283    729.572057        1.643339  \n",
       "84      4.513358               0.706727    291.618988        1.644712  \n",
       "85      2.188431               0.698066    106.779095        1.650858  \n",
       "86     18.920028               0.695289   1618.348184        1.655978  \n",
       "87      7.136936               0.717376    262.813738        1.668704  \n",
       "88      3.356241               0.704921    300.078208        1.666226  \n",
       "89     -0.133859               0.678081    139.577481        1.656981  \n",
       "90      6.224494               0.679974    344.302009        1.661042  \n",
       "91      2.292101               0.672011    140.904807        1.660106  \n",
       "92     -1.081125               0.671885    262.962889        1.658886  \n",
       "93      4.391816               0.652783    216.250036        1.647989  \n",
       "94      3.943861               0.635328    215.223080        1.640948  \n",
       "95      0.831224               0.630194     50.165295        1.644417  \n",
       "96      4.105404               0.642503    209.909644        1.660653  \n",
       "97     -5.100644               0.642069    602.317451        1.661314  \n",
       "98     -2.360790               0.622516    484.180990        1.655262  \n",
       "99      0.077427               0.624622    159.389980        1.652096  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pdp\n",
    "#%whos DataFrame\n",
    "#pdp.to_csv(os.getcwd() + '\\\\pdp\\\\pdp_' + pathend + '.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot partial dependeny as comparison between the different specifications for a given control group and type of consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import PercentFormatter #plot as percentage\n",
    "import seaborn #plot density and histogram at the same time\n",
    "# Set directory where files are downloaded to. Chdir has to be changed in order to run on another computer\n",
    "os.chdir('C:\\\\Users\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE')\n",
    "os.getcwd()\n",
    "\n",
    "pds_dir = os.listdir(os.getcwd()+'\\\\pdp')\n",
    "pds_dir = [s for s in pds_dir if s[-4:]=='.csv']\n",
    "pds = dict()\n",
    "pds_dir\n",
    "\n",
    "\n",
    "for i in pds_dir:\n",
    "    pds[i[:-4]] = pd.read_csv(os.getcwd()+'\\\\pdp\\\\'+ i)\n",
    "pds\n",
    "pds_keys = list(pds.keys())\n",
    "\n",
    "pathlist=[p[:13] for p in pds_keys]\n",
    "pathlist = list(set(pathlist))\n",
    "for path in pathlist:\n",
    "    newpath = os.getcwd() + '\\\\pdp\\\\' + path\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "#pd.read_csv(os.getcwd()+'\\\\pdp\\\\'+ )\n",
    "\n",
    "var_plot = ['age']\n",
    "type_plot = ['_mean']\n",
    "var_plot = ['age','FINCBTXM','FSALARYM','FINCBTAX','orgmrtx_sum','qblncm1x_sum','adults','PERSLT18','morgpayment']\n",
    "vars_label = ['age', 'tot amount of family income bef taxes', 'income from wages', 'income in last 12 months', \n",
    "              'sum of mortgage amounts', 'sum of principal balances outstanding at the beginning of month', 'number of adults', \n",
    "              'people below 18 in hh', 'mortgage payment per month']\n",
    "\n",
    "cons = ['FD_','SND']\n",
    "#check = [key for key in pds_keys if key[0:7] == 'pdp_' + cons[0]]\n",
    "\n",
    "for c in cons:\n",
    "    pds_cons = [key for key in pds_keys if key[0:7] == 'pdp_' + c]\n",
    "    for var in var_plot:\n",
    "        for i in pds_cons:\n",
    "            for j in type_plot:\n",
    "                plt.plot(pds[i][var+'_grid'], pds[i][var+j], label=i[4:])\n",
    "        plt.legend()\n",
    "        plt.xlabel(vars_label[var_plot.index(var)])\n",
    "        plt.ylabel(j[1:] + ' estimated consumption response')\n",
    "        plt.title('Partial Dependence Plot,'+ pds_cons[0][4:7])\n",
    "        plt.savefig(os.getcwd() + '\\\\pdp\\\\' + f'{i[:13]}' + f'\\\\pdp_{var}.pdf')\n",
    "        plt.show()\n",
    "   \n",
    "    \n",
    "fin_keys = [key for key in pds_keys if key[-5:]!='nofin']\n",
    "\n",
    "for c in cons:\n",
    "    fin_cons = [key for key in fin_keys if key[0:7] == 'pdp_' + c]\n",
    "    for j in type_plot:\n",
    "        for i in fin_cons:\n",
    "            if 'finassets_it' + j  in list(pds[i]):\n",
    "                plt.plot( pds[i]['finassets_it_grid'],pds[i]['finassets_it'+j],label=i[4:])\n",
    "            else:\n",
    "                plt.plot(pds[i]['finassets_grid'], pds[i]['finassets'+j],label=i[4:])\n",
    "        plt.xlabel('sum of checkings and savings account')\n",
    "        plt.ylabel(j[1:] + ' estimated consumption response')\n",
    "        plt.title('Partial Dependence Plot,'+ fin_cons[0][4:7])\n",
    "        plt.legend()\n",
    "        plt.savefig(os.getcwd() + '\\\\pdp\\\\' + f'{i[:13]}' + f'\\\\pdp_finassets.pdf')\n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7** Visualize tree (not done yet) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cons = ['FD_','SND']\n",
    "check = [key for key in pds_keys if key[0:7] == 'pdp_' + cons[0]]\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pull out one tree from forest\n",
    "tree = rf.estimators_[5]\n",
    "\n",
    "#export image to a dot file\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = x_list, rounded = True, precision = 1 )\n",
    "(graph, )= pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree_check.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
