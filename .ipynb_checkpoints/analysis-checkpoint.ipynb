{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import PercentFormatter #plot as percentage\n",
    "import seaborn #plot density and histogram at the same time\n",
    "# Set directory where files are downloaded to. Chdir has to be changed in order to run on another computer\n",
    "os.chdir('C:\\\\Users\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE') #change this to the folder where the data set is stored, all the results will be saved in the same folder\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Data** (necessary for part1 and part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08 = pd.read_csv(os.getcwd()+'\\\\fs08.csv').set_index('CustID')\n",
    "\n",
    "#identifier\n",
    "TIME = ['QINTRVMO', 'QINTRVYR', 'rbtmo_1', 'rbtmo_2', 'diff_1', 'diff_2']\n",
    "ID = ['NEWID']\n",
    "\n",
    "#dependent variables\n",
    "CONS = ['FD','SND','ND','DUR','TOT']\n",
    "FUTCONS = ['fut_' + c for c in CONS]\n",
    "LRUNCONS = ['lrun_' + c for c in CONS]\n",
    "\n",
    "for i in range(len(LRUNCONS)):\n",
    "    fs08[LRUNCONS[i]] = fs08[[CONS[i],FUTCONS[i]]].sum(axis=1)\n",
    "\n",
    "#explanatory variables\n",
    "DEMO = ['age', 'adults', 'PERSLT18', 'MARITAL1', 'CUTENURE', 'FINCBTAX'] \n",
    "    #age; number of adults; people below 18; marital status; housing tenure; income in the last 12 months\n",
    "DEMO2 = ['FSALARYM', 'FINCBTXM'] \n",
    "    #FSALARYM: income from salary and wages, CKBKACTX: balance/market value in balance accounts/brookerage accounts;    \n",
    "    #FINCBTXM: Total amount of family income before taxes (Imputed or collected data); (relevant demographics available for the second stimulus only)\n",
    "ASSETS = ['valid_finassets','finassets']\n",
    "    # finassets: sum of 1) SAVACCTX (Total balance/market value (including interest earned) CU had in savings accounts in banks, savings and loans,\n",
    "                         #credit unions, etc., as of the last day of previous month;)\n",
    "                # and    2)CKBKACTX (Total balance or market value (including interest earned) CU had in checking accounts, brokerage accounts, \n",
    "                            #and other similar accounts as of the last day of the previous month\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'orgmrtx_sum', 'qescrowx_sum', 'timeleft']\n",
    "    #morgpayment: morgage payment per month; qblncm1x_sum: sum of principal balances outstanding at the beginning of month M1; orgmrtx_sum: sum of mortgage amounts;\n",
    "    #qescrowx_sum: sum of last regular escrow payments; timeleft: maximum time left on mortgage payment\n",
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #\n",
    "#sample split\n",
    "RBT = ['rbtamt', 'rbtamt_chk', 'rbtamt_e']\n",
    "LAGRBT = ['last_' + var for var in RBT] #lagged variables\n",
    "FUTRBT = ['fut_' + var for var in RBT] #future variables\n",
    "\n",
    "fs08 = fs08[TIME + ID + CONS + DEMO + DEMO2 + ASSETS + MORTGAGE + RBT + ['rbtamt_1','rbtamt_2'] + LAGRBT + FUTRBT + FUTCONS + LRUNCONS + EDUC] #+ CHGCONS + LAGCONS \n",
    "fs08 = pd.get_dummies(fs08, columns=['CUTENURE','MARITAL1']) #change categorical variables to dummy variables\n",
    "\n",
    "DEMO = [s for s in DEMO if s!='CUTENURE' if s!='MARITAL1'] + ['CUTENURE' + f'_{j}' for j in list(range(1,6)) if j!=3] +['MARITAL1' + f'_{j}' for j in list(range(1,5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8287701228524802\n",
      "0.8955169267751125\n",
      "0.7627057548315713\n",
      "0.09380920937886113\n",
      "771\n",
      "4339\n"
     ]
    }
   ],
   "source": [
    "print(fs08['FINCBTAX'].corr(fs08['FINCBTXM']))\n",
    "print(fs08['FSALARYM'].corr(fs08['FINCBTXM']))\n",
    "print(fs08['FSALARYM'].corr(fs08['FINCBTAX']))\n",
    "print(fs08['FINCBTAX'].corr(fs08['morgpayment']))\n",
    "print(fs08.loc[(fs08['rbtamt']>0)&(fs08['valid_finassets']>0),'NEWID'].count())\n",
    "print(fs08.loc[fs08['rbtamt']>0,'NEWID'].count())\n",
    "fs08.loc[fs08['rbtamt_2']>0, ['rbtamt_1','rbtamt_2','diff_2', 'diff_1']]\n",
    "fs08['diff'] = np.nan\n",
    "fs08['diff'] = np.where((fs08['diff_1']>0) & (fs08['diff_2']>0), fs08[['diff_1','diff_2']].mean(axis=1),fs08['diff_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the average rebate amount per individual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08['rbtamt_idmean'] = 0\n",
    "fs08['rbtamt_idmean'] = fs08.groupby('CustID')['rbtamt'].transform('mean')\n",
    "fs08['rbt_count'] = 0\n",
    "fs08['rbt_count'] = fs08.groupby('CustID')['rbtamt'].transform('count')\n",
    "\n",
    "#\n",
    "#sometimes individuals give information of rebate receipt preceding (following) three months of the first (last) interview.\n",
    "#Wherever this is the case, the average rebate should be the weighted mean of rebates received before (after) the relevant time and the actual rebate \n",
    "\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['last_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) #weighted mean,  & (fs08['rbtamt_idmean']>0)\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist() # & (fs08['rbtamt_idmean']>0)\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first') #change for all entries for a given individual\n",
    "\n",
    "\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['fut_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) #weighted mean  & (fs08['rbtamt_idmean']>0)\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0)  & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() #& (fs08['rbtamt_idmean']>0)\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last')  #change for all entries for a given individual\n",
    "#display(fs08.loc[index, ['rbtamt_idmean', 'rbtamt', 'fut_rbtamt', 'last_rbtamt']])\n",
    "\n",
    "#wherever there is no entry for rebates received in the relevant time period but when there were rebates receivde in the past (future) change mean to the value\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() #                                                                               \n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 fs08['fut_rbtamt'], fs08['rbtamt_idmean'])\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last')\n",
    "\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist()\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())), \n",
    "                                fs08['last_rbtamt'], fs08['rbtamt_idmean'])\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first')\n",
    "\n",
    "\n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[(fs08['rbtamt']>0) | (fs08['fut_rbtamt']>0) | (fs08['last_rbtamt']>0) ,'rbt_flag'] = 1\n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('sum')\n",
    "fs08.loc[fs08['rbt_flag']>0, 'rbt_flag'] = 1\n",
    "fs08 = fs08.loc[fs08['rbt_flag']==1]\n",
    "\n",
    "\n",
    "index = fs08.index[fs08['rbtamt_idmean'].isna()].tolist()\n",
    "index = list(set(index))\n",
    "fs08.loc[index, 'rbtamt_idmean'] = fs08.loc[index,'fut_rbtamt']\n",
    "fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'fut_rbtamt' ]\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'last_rbtamt' ]\n",
    "\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations, impute values for financial liquidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(RBT)):\n",
    "    fs08.loc[fs08[RBT[i]]==0, RBT[i]] = np.nan\n",
    "    fs08.loc[fs08[LAGRBT[i]]==0, LAGRBT[i]] = np.nan\n",
    "    fs08.loc[fs08[FUTRBT[i]]==0, FUTRBT[i]] = np.nan\n",
    "    #fs08.loc[(fs08[LAGRBT[i]]==0) | (fs08[LAGRBT[i]].isna()), LAGRBT[i]] =  fs08.loc[(fs08[LAGRBT[i]]==0) | (fs08[LAGRBT[i]].isna())].groupby('CustID')[RBT[i]].shift(-1) \n",
    "    fs08.loc[(fs08[FUTRBT[i]]==0) | (fs08[FUTRBT[i]].isna()), FUTRBT[i]] =  fs08.loc[(fs08[FUTRBT[i]]==0) | (fs08[FUTRBT[i]].isna())].groupby('CustID')[RBT[i]].shift(-1) \n",
    "\n",
    "\n",
    "\n",
    "fs08 = fs08.reset_index()\n",
    "\n",
    "#Iterative imputation for financial liquidity\n",
    "#explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "fs08 = fs08.dropna(subset=CONS+DEMO+DEMO2+MORTGAGE) #Keep only observations that have all info on explanatory variables, dropping missing values on mortgage lowers the sample to half \n",
    "\n",
    "\n",
    "fs08_finit = fs08.copy()\n",
    "fs08_finit = fs08_finit[ ID + CONS + DEMO + DEMO2 + ASSETS + MORTGAGE +  LRUNCONS + EDUC]\n",
    "#fs08_finit = fs08_finit.loc[:,CONS+DEMO+DEMO2+MORTGAGE+['CustID','NEWID','finassets']]\n",
    "labels = list(fs08_finit.columns)\n",
    "imp_mean = IterativeImputer(random_state=0) #use python package iterative imputer\n",
    "imp_mean.fit(fs08_finit[2:])\n",
    "fs08_finit = pd.DataFrame(imp_mean.transform(fs08_finit),columns=labels)\n",
    "fs08_finit = fs08_finit.loc[:,['finassets','NEWID']]\n",
    "fs08_finit = fs08_finit.rename(columns={'finassets':'finassets_it'})\n",
    "\n",
    "fs08 = pd.merge(fs08.sort_values(by = ['NEWID']).reset_index(), fs08_finit.sort_values(by = ['NEWID']).reset_index(), how = 'left', on = 'NEWID', validate = '1:1')\n",
    "fs08 = fs08.drop(columns=['index_x','index_y']) \n",
    "ASSETS = ['valid_finassets','finassets', 'finassets_it' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate treatment and control groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate treatment group:\n",
    "fs08['treat1'] = 0 \n",
    "fs08.loc[fs08['rbtamt'].notna(),'treat1'] = 1 #all entries with actual info on rebate are in the treatment group\n",
    "\n",
    "#three different control groups:\n",
    "#control group 1: those who didn't receive the rebate in the given month\n",
    "fs08['cont1'] = 0\n",
    "fs08.loc[fs08['rbtamt'].isna(), 'cont1'] = 1\n",
    "\n",
    "#control group 2: drop one time period after receipt of rebate, as part of rebate might've been consumed one time period after\n",
    "fs08['cont2'] = 0 \n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[fs08['rbtamt']>0,'rbt_flag'] = 1 #identifier for rebate\n",
    "fs08['rbt_flag_lag'] = fs08.groupby('CustID')['rbt_flag'].shift(1) #identifier for rebate a period before (lag)\n",
    "fs08.loc[fs08['rbt_flag_lag']==1,'rbt_flag']=1 #change rebate identifier so it capture now if a rebate was received this period or the period before\n",
    "fs08.loc[(fs08['rbtamt'].isna()) & (fs08['rbt_flag']==0), 'cont2'] = 1 #those who didn't receive a rebate now or a period before are in the control group\n",
    "\n",
    "#treatment 2: all individuals who received a rebate last time period. This group should be compared to control group 2 only\n",
    "fs08['treat2'] = 0\n",
    "fs08.loc[(fs08['cont2']==0) & (fs08['treat1']==0),'treat2'] = 1\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt'].isna()), 'treat2'] = 1\n",
    "\n",
    "#treatment 3: long run consumption response: all individuals who received a rebate in this period and where the rebate interview is not the last\n",
    "fs08['treat3'] = 0\n",
    "fs08.loc[(fs08['rbtamt']>0) & (fs08[FUTCONS[0]].notna()), 'treat3'] = 1\n",
    "\n",
    "#control 3: long-run consumption: those who haven't received a rebate two periods from current period and have information on consumption for next period as well\n",
    "fs08['cont4'] = 0\n",
    "fs08.loc[(fs08['cont2']==1) & (fs08[FUTCONS[0]].notna()),'cont4'] = 1\n",
    "\n",
    "#control 4: those who haven't received the rebate yet\n",
    "fs08['cont3'] = 0 \n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('cumsum') #starts counting from the point on which the first rebate was received\n",
    "fs08.loc[(fs08['rbtamt'].isna()) & (fs08['rbt_flag']==0),'cont3'] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08_cap = fs08[(np.abs(stats.zscore(fs08.loc[:,CONS+LRUNCONS])) < 3).all(axis=1)] #drop outliers\n",
    "fs08_cap = fs08_cap.loc[fs08_cap['FD']>0] #there are still two observations where food consumption is zero; drop bc of common sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Descriptive statistics** (part 1 and part 2 cn be run seperately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Take a look at the explanatory variables used for random forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole sample (capped):\n",
      "age\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1941.000000\n",
       "mean       46.505152\n",
       "std        12.354085\n",
       "min        21.000000\n",
       "25%        37.000000\n",
       "50%        46.000000\n",
       "75%        55.000000\n",
       "max        84.500000\n",
       "Name: age, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{FINCBTAX} & \\multicolumn{2}{c}{FSALARYM} & \\multicolumn{2}{c}{FINCBTXM} & \\multicolumn{2}{c}{finassets} & \\multicolumn{2}{c}{finassets\\_it} \\\\\n",
      "{} &   treat1 &    cont1 &   treat1 &   cont1 &   treat1 &   cont1 &    treat1 &     cont1 &       treat1 &     cont1 \\\\\n",
      "\\midrule\n",
      "count &    1,941 &    5,446 &    1,941 &   5,446 &    1,941 &   5,446 &       344 &       728 &        1,941 &     5,446 \\\\\n",
      "mean  &   67,099 &   67,390 &   72,060 &  70,776 &   81,479 &  81,045 &    40,343 &    37,520 &       37,775 &    38,583 \\\\\n",
      "std   &   52,023 &   50,843 &   53,364 &  50,394 &   54,095 &  50,731 &   188,758 &   167,181 &       88,764 &    76,242 \\\\\n",
      "min   &  -56,860 & -104,854 &        0 &       0 &  -36,146 & -36,146 &         0 &         0 &      -91,991 &  -154,508 \\\\\n",
      "25\\%   &   32,000 &   32,756 &   38,000 &  37,591 &   46,180 &  46,527 &       686 &       800 &        5,496 &     7,753 \\\\\n",
      "50\\%   &   61,100 &   62,000 &   64,799 &  65,000 &   71,211 &  71,461 &     3,954 &     4,500 &       26,497 &    27,995 \\\\\n",
      "75\\%   &   95,000 &   95,296 &   96,321 &  97,000 &  104,000 & 104,340 &    18,000 &    20,000 &       51,120 &    51,982 \\\\\n",
      "max   &  423,096 &  434,017 &  435,782 & 435,782 &  439,195 & 436,595 & 3,080,141 & 3,999,868 &    3,080,141 & 3,999,868 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Whole sample (capped):')\n",
    "print(DEMO[0])\n",
    "expvars = DEMO+DEMO2+ASSETS+MORTGAGE+EDUC\n",
    "for exp in expvars:\n",
    "    if exp == DEMO[0]:\n",
    "        des = fs08_cap.loc[fs08_cap['treat1']==1,exp].describe()\n",
    "        display(des)\n",
    "        des = pd.concat([des, fs08_cap.loc[fs08['cont1']==1,exp].describe()], axis=1, join='inner')\n",
    "    else:\n",
    "        pass\n",
    "        for g in ['treat1','cont1']:\n",
    "            des = pd.concat([des, fs08_cap.loc[fs08[g]==1,exp].describe()], axis=1, join='inner')\n",
    "#index1 = [DEMO*2+DEMO2*2+ASSETS*2+MORTGAGE*2+EDUC*2\n",
    "index1 = [ i for i in expvars for reps in range(2) ]\n",
    "index2 = ['treat1','cont1']*25\n",
    "tuples = list(zip(index1,index2))\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples) \n",
    "des_cols = list(des)\n",
    "des_cols=[i for i in des_cols if i[0] in ['FINCBTAX','FSALARYM', 'FINCBTXM','finassets','finassets_it']]\n",
    "print(des.to_latex(float_format=\"{:,.0f}\".format, columns=des_cols,multicolumn_format='c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average amount of rebate received per household\n",
      "\\begin{tabular}{llrrrrrrr}\n",
      "\\toprule\n",
      "       &             &  count &  mean &   max &  min &  25\\% &   75\\% &  std \\\\\n",
      "\\midrule\n",
      "cont1 & rbtamt\\_mean &  5,446 & 1,108 & 3,660 &    6 &  600 & 1,500 &  528 \\\\\n",
      "treat1 & rbtamt &  1,941 & 1,095 & 3,660 &    1 &  600 & 1,500 &  527 \\\\\n",
      "       & rbtamt\\_mean &  1,941 & 1,097 & 3,660 &    6 &  600 & 1,500 &  521 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Average amount of rebate received per household')\n",
    "des = fs08_cap.loc[fs08_cap['cont1']==1,'rbtamt_idmean'].describe()\n",
    "des = pd.concat([des, fs08_cap.loc[fs08_cap['treat1']==1,'rbtamt_idmean'].describe()], axis=1, join='inner',names=['cont1','treat1'])\n",
    "des = pd.concat([des, fs08_cap.loc[fs08['rbtamt']>0,'rbtamt'].describe()], axis=1, join='inner',names=['cont1','treat1','treat1'])\n",
    "tuples = [('rbtamt_mean', 'cont1'), ('rbtamt_mean', 'treat1'), ('rbtamt', 'treat1')]\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples)\n",
    "print(des.stack().unstack(level=0).stack(level=0).to_latex(float_format=\"{:,.0f}\".format,  columns=['count','mean','max','min','25%','75%','std']))\n",
    "#print(des1.loc[['count', 'mean', 'std', 'min','50%','max'],:].to_latex(float_format=\"{:,.1f}\".format))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Descriptives for dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary stats for full sample on consumption:\n",
      "                 FD           SND            ND           DUR           TOT\n",
      "count   7901.000000   7901.000000   7901.000000   7901.000000   7901.000000\n",
      "mean    2214.125173   5107.249167   6357.100416   6241.762099  12598.862515\n",
      "std     1309.135172   2681.752964   3277.032339   5821.221010   7599.242540\n",
      "min        0.000000    611.000100  -4420.575000    169.499900   1818.500100\n",
      "25%     1371.000000   3415.000200   4204.999800   3099.000200   7865.991000\n",
      "50%     1975.000200   4620.750200   5761.999900   4756.000200  10843.200100\n",
      "75%     2750.000100   6157.000000   7722.000200   7262.999900  14921.999800\n",
      "max    37678.000300  50375.052100  54477.052200  90264.999900  98775.000000\n",
      "summary stats for capped sample on consumption:\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &      FD &      SND &       ND &      DUR &      TOT \\\\\n",
      "\\midrule\n",
      "count & 7387.00 &  7387.00 &  7387.00 &  7387.00 &  7387.00 \\\\\n",
      "mean  & 2076.57 &  4790.18 &  5969.59 &  5369.43 & 11339.02 \\\\\n",
      "std   &  995.51 &  1962.09 &  2502.13 &  3425.05 &  5002.07 \\\\\n",
      "min   &   52.00 &   611.00 &   876.25 &   169.50 &  1818.50 \\\\\n",
      "25\\%   & 1352.00 &  3355.00 &  4132.50 &  3021.00 &  7688.00 \\\\\n",
      "50\\%   & 1940.00 &  4500.00 &  5599.00 &  4549.00 & 10432.00 \\\\\n",
      "75\\%   & 2638.50 &  5921.62 &  7390.00 &  6792.50 & 14121.00 \\\\\n",
      "max   & 6134.00 & 12905.00 & 15994.13 & 23509.00 & 35115.31 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{FD} & \\multicolumn{2}{c}{SND} & \\multicolumn{2}{c}{ND} & \\multicolumn{2}{c}{TOT} \\\\\n",
      "{} &  treat1 &   cont1 &   treat1 &    cont1 &   treat1 &    cont1 &   treat1 &    cont1 \\\\\n",
      "\\midrule\n",
      "count & 1,941.0 & 5,446.0 &  1,941.0 &  5,446.0 &  1,941.0 &  5,446.0 &  1,941.0 &  5,446.0 \\\\\n",
      "mean  & 2,152.1 & 2,049.7 &  5,058.8 &  4,694.4 &  6,228.4 &  5,877.4 & 11,719.7 & 11,203.3 \\\\\n",
      "std   & 1,042.8 &   976.8 &  2,074.6 &  1,911.5 &  2,644.1 &  2,443.2 &  5,151.0 &  4,941.3 \\\\\n",
      "min   &    78.0 &    52.0 &    611.0 &    676.0 &    876.3 &    940.0 &  2,415.0 &  1,818.5 \\\\\n",
      "25\\%   & 1,400.0 & 1,331.3 &  3,569.0 &  3,296.0 &  4,259.5 &  4,092.0 &  7,845.0 &  7,635.3 \\\\\n",
      "50\\%   & 1,985.0 & 1,919.0 &  4,815.0 &  4,411.0 &  5,877.0 &  5,527.5 & 10,973.0 & 10,244.6 \\\\\n",
      "75\\%   & 2,740.0 & 2,600.0 &  6,267.0 &  5,820.8 &  7,784.5 &  7,259.9 & 14,561.5 & 13,905.4 \\\\\n",
      "max   & 5,986.0 & 6,134.0 & 12,855.0 & 12,905.0 & 15,849.0 & 15,994.1 & 34,784.6 & 35,115.3 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>count</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">cont1</th>\n",
       "      <th>FD</th>\n",
       "      <td>1331.250150</td>\n",
       "      <td>1919.00015</td>\n",
       "      <td>2600.00010</td>\n",
       "      <td>5446.0</td>\n",
       "      <td>6133.9998</td>\n",
       "      <td>2049.654607</td>\n",
       "      <td>51.9999</td>\n",
       "      <td>976.803289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ND</th>\n",
       "      <td>4092.000050</td>\n",
       "      <td>5527.49980</td>\n",
       "      <td>7259.87500</td>\n",
       "      <td>5446.0</td>\n",
       "      <td>15994.1336</td>\n",
       "      <td>5877.364805</td>\n",
       "      <td>940.0001</td>\n",
       "      <td>2443.192098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SND</th>\n",
       "      <td>3296.000150</td>\n",
       "      <td>4411.00010</td>\n",
       "      <td>5820.79380</td>\n",
       "      <td>5446.0</td>\n",
       "      <td>12905.0000</td>\n",
       "      <td>4694.441266</td>\n",
       "      <td>675.9999</td>\n",
       "      <td>1911.486019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOT</th>\n",
       "      <td>7635.250075</td>\n",
       "      <td>10244.62525</td>\n",
       "      <td>13905.44955</td>\n",
       "      <td>5446.0</td>\n",
       "      <td>35115.3056</td>\n",
       "      <td>11203.336221</td>\n",
       "      <td>1818.5001</td>\n",
       "      <td>4941.313979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">treat1</th>\n",
       "      <th>FD</th>\n",
       "      <td>1399.999900</td>\n",
       "      <td>1985.00010</td>\n",
       "      <td>2740.00020</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>5986.0000</td>\n",
       "      <td>2152.072131</td>\n",
       "      <td>78.0000</td>\n",
       "      <td>1042.759107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ND</th>\n",
       "      <td>4259.500100</td>\n",
       "      <td>5877.00000</td>\n",
       "      <td>7784.49970</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>15849.0002</td>\n",
       "      <td>6228.362786</td>\n",
       "      <td>876.2501</td>\n",
       "      <td>2644.063350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SND</th>\n",
       "      <td>3569.000100</td>\n",
       "      <td>4815.00000</td>\n",
       "      <td>6267.00010</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>12854.9998</td>\n",
       "      <td>5058.814060</td>\n",
       "      <td>611.0001</td>\n",
       "      <td>2074.626364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOT</th>\n",
       "      <td>7845.000100</td>\n",
       "      <td>10973.00020</td>\n",
       "      <td>14561.50000</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>34784.6310</td>\n",
       "      <td>11719.734302</td>\n",
       "      <td>2415.0002</td>\n",
       "      <td>5150.980866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    25%          50%          75%   count         max  \\\n",
       "cont1  FD   1331.250150   1919.00015   2600.00010  5446.0   6133.9998   \n",
       "       ND   4092.000050   5527.49980   7259.87500  5446.0  15994.1336   \n",
       "       SND  3296.000150   4411.00010   5820.79380  5446.0  12905.0000   \n",
       "       TOT  7635.250075  10244.62525  13905.44955  5446.0  35115.3056   \n",
       "treat1 FD   1399.999900   1985.00010   2740.00020  1941.0   5986.0000   \n",
       "       ND   4259.500100   5877.00000   7784.49970  1941.0  15849.0002   \n",
       "       SND  3569.000100   4815.00000   6267.00010  1941.0  12854.9998   \n",
       "       TOT  7845.000100  10973.00020  14561.50000  1941.0  34784.6310   \n",
       "\n",
       "                    mean        min          std  \n",
       "cont1  FD    2049.654607    51.9999   976.803289  \n",
       "       ND    5877.364805   940.0001  2443.192098  \n",
       "       SND   4694.441266   675.9999  1911.486019  \n",
       "       TOT  11203.336221  1818.5001  4941.313979  \n",
       "treat1 FD    2152.072131    78.0000  1042.759107  \n",
       "       ND    6228.362786   876.2501  2644.063350  \n",
       "       SND   5058.814060   611.0001  2074.626364  \n",
       "       TOT  11719.734302  2415.0002  5150.980866  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('summary stats for full sample on consumption:')\n",
    "print(fs08[CONS].describe())\n",
    "#print(fs08[CONS].describe().to_latex())\n",
    "print('summary stats for capped sample on consumption:')\n",
    "print(fs08_cap[CONS].describe().to_latex(float_format=\"%.2f\" ))\n",
    "\n",
    "treat = ['treat1','treat2','treat3']\n",
    "cont = ['cont1','cont2','cont3','cont4']\n",
    "#baseline:\n",
    "CONS=['FD','SND','ND','TOT']\n",
    "treat = ['treat1']\n",
    "cont = ['cont1']\n",
    "\n",
    "for i in CONS:\n",
    "    if i=='FD':\n",
    "        des = fs08_cap.loc[fs08_cap['treat1']==1,i].describe()\n",
    "        des = pd.concat([des,fs08_cap.loc[fs08_cap['cont1']==1,i].describe()],join='inner',axis=1)\n",
    "    else:\n",
    "        for g in ['treat1','cont1']:\n",
    "            des = pd.concat([des, fs08_cap.loc[fs08_cap[g]==1,i].describe()],join='inner', axis=1)\n",
    "\n",
    "index1 = ['FD']*2 + ['SND']*2 + ['ND']*2 + ['TOT']*2\n",
    "index2 = ['treat1','cont1']*8\n",
    "tuples = list(zip(index1,index2))\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples)\n",
    "print(des.to_latex(float_format=\"{:,.1f}\".format ,multicolumn_format='c'))\n",
    "des.stack().unstack(level=0).stack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare histograms of treatment group with different control groups\n",
    "\n",
    "for i in range(len(CONS)):\n",
    "    for t in treat[0:1]:\n",
    "        for c in cont[0:1]:\n",
    "            plt.figure(figsize=(3.2,2.4))\n",
    "            plt.title(f'{CONS[i]}')\n",
    "            plt.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            #plt.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, density=True, bins=30, color='red')\n",
    "            plt.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            #plt.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= t, density=True, bins=30, color='green')\n",
    "            #plt.set_title(f'{CONS[i]}:')\n",
    "            #plt.tight_layout()\n",
    "            plt.xticks(fontsize = 6)\n",
    "            plt.yticks(fontsize=6)\n",
    "            plt.legend(loc='upper right', frameon=True, fontsize=6)\n",
    "            plt.savefig(os.getcwd() + f'\\\\descriptives\\\\{CONS[i]}_pattern.pdf')\n",
    "            plt.close()\n",
    "CONS=['SND','TOT']\n",
    "fig=plt.figure(figsize=(6.4,2.4))\n",
    "for i in range(len(CONS)):\n",
    "    #plt.figure(figsize=(6.4,2.4))\n",
    "    plot = plt.subplot(1,2,i+1)\n",
    "    for t in treat[0:1]:\n",
    "        for c in cont[0:1]:     \n",
    "            #plot = plt.figure(figsize=(10,3))\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            plot.set_title(f'{CONS[i]}:')\n",
    "            plt.tight_layout()\n",
    "            plt.xticks(fontsize = 8)\n",
    "            plt.yticks(fontsize=8)\n",
    "            plt.legend(loc='upper right', frameon=False, fontsize=8)\n",
    "            #plot_count = plot_count+1\n",
    "            plt.savefig(os.getcwd() + f'\\\\descriptives\\\\SND_TOT_pattern_group1_treat1.pdf')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "CONS=['FD','SND','ND','TOT']        \n",
    "plot_count = 1\n",
    "for i in range(len(CONS)):\n",
    "    #plt.figure(figsize=(6.4,2.4))\n",
    "    for t in treat[0:1]:\n",
    "        fig=plt.figure(figsize=(6.4,2.4))\n",
    "        for c in cont[0:3]:     \n",
    "            #plot = plt.figure(figsize=(10,3))\n",
    "            plot = plt.subplot(1,3,cont.index(c)+1)\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            plot.set_title(f'{CONS[i]}:')\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc='upper right', frameon=False)\n",
    "            plot_count = plot_count+1\n",
    "            #plt.savefig(os.getcwd() + f'\\\\descriptives\\\\{CONS[i]}_pattern_groupcomp.pdf')\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "descriptives for rebate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: Machine learning approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Define sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Run random forest algorithm seperately for treatment and control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #'educ_bachelor','educ_master','educ_doctorate'\n",
    "DEMO = ['age', 'adults', 'PERSLT18', 'FINCBTAX', 'CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5', 'MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'] \n",
    "DEMO2 = ['FSALARYM', 'FINCBTXM' ] \n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'orgmrtx_sum', 'qescrowx_sum', 'timeleft']\n",
    "CONS = ['FD', 'SND', 'ND', 'TOT']\n",
    "CONT = ['cont1', 'cont2', 'cont3']\n",
    "TREAT = 'treat1'\n",
    "treatgroup = TREAT\n",
    "trees = 1000\n",
    "\n",
    "#Random Forest for short term consumption: treatment group 1 with imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC + ['finassets_it']  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_finit.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_nofin.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group with just the observations where financial assets are included\n",
    "TREAT = 'treat1'\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + ['finassets'] + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1) & (fs08_cap['valid_finassets']==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[(fs08_cap[con]==1) & (fs08_cap['valid_finassets']==1) , [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_fin.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "#pkl_file = open('myfile.pkl', 'rb')\n",
    "#rf2 = pickle.load(pkl_file)\n",
    "#pkl_file.close()\n",
    "#\n",
    "#print(rf)\n",
    "#print(rf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Predict Outcomes for overall consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_predicitons_rbt(rf_treat, rf_cont, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if (type(X_cont) is not np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_treat_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('X_treat_rbamt needs to have an array like structure')\n",
    "            else:\n",
    "                X_temp = X_treat.copy()\n",
    "                rbtamt_temp = X_treat_rbtamt.copy()\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_cont_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('if X_cont is specified, X_cont_rbamt needs to have an array like structure')\n",
    "            if sorted(feature_ids_treat)!=sorted(feature_ids_cont):\n",
    "                raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont')\n",
    "            elif (len(feature_ids_treat)==0) | (len(feature_ids_cont)==0):\n",
    "                raise ValueError(f'if X_treat and X_cont are specified, feature_ids must not be empty')\n",
    "            else:\n",
    "                X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                rbtamt_temp = pd.concat([pd.DataFrame(X_treat_rbtamt), pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                X_labels = list(X.columns)\n",
    "                X_temp = np.array(X)\n",
    "                rbtamt_temp = np.array(rbtamt_temp)\n",
    "        else: \n",
    "            raise ValueError('X_treat does not have an array like structure')\n",
    "        y = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "        mpc = y/rbtamt_temp[:,0]\n",
    "        return y,mpc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD_fin:\n",
      "cont3_treat1_y_pred (393,)\n",
      "cont3_treat1_mpc_pred (393,)\n",
      "cont1_treat1_y_pred (1072,)\n",
      "cont1_treat1_mpc_pred (1072,)\n",
      "cont2_treat1_y_pred (740,)\n",
      "cont2_treat1_mpc_pred (740,)\n",
      "FD_finit:\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "FD_nofin:\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "ND_fin:\n",
      "cont3_treat1_y_pred (393,)\n",
      "cont3_treat1_mpc_pred (393,)\n",
      "cont1_treat1_y_pred (1072,)\n",
      "cont1_treat1_mpc_pred (1072,)\n",
      "cont2_treat1_y_pred (740,)\n",
      "cont2_treat1_mpc_pred (740,)\n",
      "ND_finit:\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "ND_nofin:\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "SND_fin:\n",
      "cont3_treat1_y_pred (393,)\n",
      "cont3_treat1_mpc_pred (393,)\n",
      "cont1_treat1_y_pred (1072,)\n",
      "cont1_treat1_mpc_pred (1072,)\n",
      "cont2_treat1_y_pred (740,)\n",
      "cont2_treat1_mpc_pred (740,)\n",
      "SND_finit:\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "SND_nofin:\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "TOT_fin:\n",
      "cont3_treat1_y_pred (393,)\n",
      "cont3_treat1_mpc_pred (393,)\n",
      "cont1_treat1_y_pred (1072,)\n",
      "cont1_treat1_mpc_pred (1072,)\n",
      "cont2_treat1_y_pred (740,)\n",
      "cont2_treat1_mpc_pred (740,)\n",
      "TOT_finit:\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "TOT_nofin:\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n"
     ]
    }
   ],
   "source": [
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\condistr\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for t in treat:\n",
    "        for c in cont:\n",
    "            y,mpc = uplift_predicitons_rbt(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                          rfdicts[k][c+'_X'],rfdicts[k][c+'_rbtamt'],rfdicts[k][t+'_X_labels'],rfdicts[k][c+'_X_labels'])\n",
    "            rfdicts[k][f'{c}_{t}_y_pred'] = y\n",
    "            print(f'{c}_{t}_y_pred', y.shape)\n",
    "            rfdicts[k][f'{c}_{t}_mpc_pred'] = mpc\n",
    "            print(f'{c}_{t}_mpc_pred', mpc.shape)\n",
    "            plt.hist(y, bins=40,  edgecolor='black')\n",
    "            lower = round((min(y)/100),1)*100\n",
    "            upper = round((max(y)/100),1)*100+1\n",
    "            plt.xticks(np.arange(lower, upper, 1000))\n",
    "            plt.title(f'Pred cons resp distr., {vartype},{c},{t}')\n",
    "            plt.xlabel(f'consumption in {cons}, number of observations {y.shape[0]}')\n",
    "            plt.ylabel(f'number of individuals in bin')\n",
    "            plt.savefig(newpath + f'\\\\{vartype}_{c}_{t}_y_pred.pdf')\n",
    "            plt.close()         \n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** Plot distribution of consumption response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5** Variable importance plot for treatment and control group separately and as a weighted sum for the whole sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfdicts_keys = list(rfdicts)\n",
    "#print(rfdicts_keys)\n",
    "#print(list(rfdicts[rfdicts_keys[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['treat1', 'cont3', 'cont1', 'cont2']\n",
      "FD_fin\n",
      "['treat1', 'cont3', 'cont1', 'cont2']\n",
      "FD_finit\n",
      "['treat1', 'cont3', 'cont1', 'cont2']\n",
      "FD_nofin\n",
      "['treat1', 'cont3', 'cont1', 'cont2']\n",
      "ND_fin\n",
      "['treat1', 'cont3', 'cont1', 'cont2']\n",
      "ND_finit\n",
      "['treat1', 'cont3', 'cont1', 'cont2']\n",
      "ND_nofin\n",
      "['treat1', 'cont3', 'cont1', 'cont2']\n",
      "SND_fin\n",
      "['treat1', 'cont3', 'cont1', 'cont2']\n",
      "SND_finit\n",
      "['treat1', 'cont3', 'cont1', 'cont2']\n",
      "SND_nofin\n",
      "['treat1', 'cont3', 'cont1', 'cont2']\n",
      "TOT_fin\n",
      "['treat1', 'cont3', 'cont1', 'cont2']\n",
      "TOT_finit\n",
      "['treat1', 'cont3', 'cont1', 'cont2']\n",
      "TOT_nofin\n"
     ]
    }
   ],
   "source": [
    "#def vimp_plot_uplift()\n",
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "for k in rfdicts_keys:\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\varimp\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    print(treat+cont)\n",
    "    uplift_imp = dict()\n",
    "    for i in treat+cont:\n",
    "        importances = (rfdicts[k][i+'_rf'].feature_importances_)\n",
    "        X_importances = [(label, importance) for label, importance in zip(rfdicts[k][i+'_X_labels'],importances)]\n",
    "        #X_importances = [(round(importance,2), label) for importance, label in zip(importances, rf[i+'_X_labels'])]\n",
    "        X_importances = sorted(X_importances, key = lambda x:x[1], reverse = False)\n",
    "        uplift_imp[i+'_varimp_values'] = [x[1] for x in X_importances]\n",
    "        uplift_imp[i+'_varimp_labels'] = [x[0] for x in X_importances]\n",
    "        uplift_imp[i+'_varimp_tuples'] = X_importances\n",
    "        \n",
    "    \n",
    "    for i in treat + cont:\n",
    "        X_importances = sorted(X_importances, key = lambda x:x[0].upper(), reverse = False) #sort in alphabetical order\n",
    "        uplift_imp[i+'_values'] = [x[1] for x in X_importances] #importances \n",
    "        uplift_imp[i+'_labels'] = [x[0] for x in X_importances] \n",
    "        shape = rfdicts[k][i+'_X'].shape\n",
    "        uplift_imp[i+'_sample'] = shape[0] \n",
    "    \n",
    "    for t in treat:\n",
    "        plotgroups = [t]\n",
    "        for c in cont:\n",
    "            plotgroups = plotgroups + [c] + [c+'_'+t]\n",
    "            uplift_imp[f'{c}_{t}_sample'] = uplift_imp[f'{t}_sample'] + uplift_imp[f'{c}_sample'] \n",
    "            uplift_imp[f'{c}_{t}_varimp_values'] = [uplift_imp[f'{t}_sample']/(uplift_imp[f'{c}_{t}_sample'])*uplift_imp[f'{t}_values'][i] + \n",
    "            uplift_imp[f'{c}_sample']/(uplift_imp[f'{c}_{t}_sample'])*uplift_imp[f'{c}_values'][i] for i in range(len(uplift_imp[f'{t}_values']))]\n",
    "\n",
    "            up_importances = [(label, importance) for label, importance in zip(uplift_imp[f'{t}_labels'],uplift_imp[f'{c}_{t}_varimp_values'])]\n",
    "            up_importances = sorted(up_importances, key = lambda x:x[1], reverse = False)\n",
    "            uplift_imp[f'{c}_{t}_varimp_tuples'] = up_importances            \n",
    "            uplift_imp[f'{c}_{t}_varimp_values'] = [up[1] for up in up_importances]\n",
    "            uplift_imp[f'{c}_{t}_varimp_labels'] = [up[0] for up in up_importances]\n",
    "        print(k)   \n",
    "    for g in plotgroups:\n",
    "        freq_series = pd.Series(uplift_imp[g+'_varimp_values'])\n",
    "        y_labels = uplift_imp[g+'_varimp_labels']\n",
    "\n",
    "        # Plot the figure.\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        ax = freq_series.plot(kind='barh')\n",
    "        ssize=uplift_imp[g+'_sample']\n",
    "        ax.set_title(f'Variable Importance Plot for {k},{g.upper()} Sample')\n",
    "        ax.set_xlabel(f'Frequency, sample size = {str(ssize)}')\n",
    "        ax.set_ylabel(f'Variable')\n",
    "        ax.set_yticklabels(y_labels)\n",
    "        #ax.set_xlim(-40, 300) # expand xlim to make labels easier to read\n",
    "\n",
    "        rects = ax.patches\n",
    "\n",
    "        # For each bar: Place a label\n",
    "        for rect in rects:\n",
    "            # Get X and Y placement of label from rect.\n",
    "            x_value = rect.get_width()\n",
    "            y_value = rect.get_y() + rect.get_height() / 2\n",
    "\n",
    "            # Number of points between bar and label. Change to your liking.\n",
    "            space = 3\n",
    "            # Vertical alignment for positive values\n",
    "            ha = 'left'\n",
    "\n",
    "            # Use X value as label and format number with one decimal place\n",
    "            label = \"{:.3f}\".format(x_value)\n",
    "\n",
    "            # Create annotation\n",
    "            plt.annotate(\n",
    "                label,                      # Use `label` as label\n",
    "                (x_value, y_value),         # Place label at end of the bar\n",
    "                xytext=(space, 0),          # Horizontally shift label by `space`\n",
    "                textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "                va='center',                # Vertically center label\n",
    "                ha=ha)                      # Horizontally align label differently for\n",
    "                                            # positive and negative values.\n",
    "        plt.savefig(f'{newpath}\\\\{vartype}_{g}.pdf')\n",
    "        plt.close()\n",
    "        #plt.savefig(\"image.png\")\n",
    "        #plt.savefig(newpath + '\\\\'+ pathend +f'_{i}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Partial dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6.2** Function for uplift 2model partial dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_num_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[], types=['mean'], percentile='none', grid_lower=5, grid_upper=95 ): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor (or multiple predictors) in a random forest uplift 2 model approach.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    y_pred: corresponding predicted values of dependent variable (type: ndarray). \n",
    "            If input types is a list the columns in the array correspond to the chosen types in the same order\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is int):\n",
    "                if f_id > (X_temp.shape[1]-1):\n",
    "                    raise ValueError(f'positional number of {f_id} exceeds array shape')\n",
    "                else:\n",
    "                    X_temp = X_treat.copy()\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str):\n",
    "                if f_id not in feature_ids:\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids is not passed to the function')\n",
    "                else:\n",
    "                    f_id = feature_ids.index(f_id)\n",
    "                    f_id_label = f_id\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('f_id needs to be either an integer or a string')\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(f_id) is int):\n",
    "                raise ValueError(f'if X_cont is specified, then f_id needs to be a string variable')\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str) & ((f_id not in feature_ids_treat) |  (f_id not in feature_ids_cont)):\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids_treat or feature_ids_cont is not passed to the function')\n",
    "            else:\n",
    "                if (sorted(feature_ids_treat)!=sorted(feature_ids_cont)) & (type(X_cont_rbtamt) is not np.ndarray) :\n",
    "                    raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                else:\n",
    "                    X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = np.array(mean_rbt)\n",
    "                    X_labels = list(X.columns)\n",
    "                    X_temp = np.array(X)\n",
    "                    f_id_label = f_id\n",
    "                    f_id = X_labels.index(f_id)             \n",
    "        else: \n",
    "            raise ValueError('Either X_cont or X_treat does not have an array like structure')\n",
    "\n",
    "                       #['age', 'adults', 'PERSLT18'\n",
    "        X_unique = np.array(list(set(X_temp[:, f_id])))\n",
    "        if len(X_unique)*3 > 100:\n",
    "            grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower), np.percentile(X_temp[:, f_id], grid_upper), 100)\n",
    "        else:\n",
    "            grid = np.linspace(np.percentile(X_temp[:, f_id],grid_lower),np.percentile(X_temp[:, f_id],grid_upper), len(X_unique)*2)\n",
    "       \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types.copy()\n",
    "        #column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "\n",
    "        if set(types) <= set(function_labels):\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i]\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif type(percentile) is not list: # 0<=percentile<=100:\n",
    "                    raise ValueError('percentile must be list')\n",
    "                else:\n",
    "                    column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "                    if len(percentile)>1:\n",
    "                        for p in percentile[1:]:\n",
    "                            types = types + ['percentile'] #pass\n",
    "                            nptypes = nptypes + [np.percentile]\n",
    "                            column_labels = column_labels + ['percentile_' +str(p)]\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "        y_pred = np.zeros((len(grid),len(types)))\n",
    "        mpc_pred = np.zeros((len(grid), len(types)))\n",
    "        p_pos = 0\n",
    "        for i, val in enumerate(grid): # i returns the counter, val returns the value at position of counter on grid \n",
    "            X_temp[:, f_id] = val\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            p_pos = 0\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[i,j] = nptypes[j](y_temp,percentile[p_pos])\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp,percentile[p_pos])\n",
    "                    p_pos = p_pos + 1\n",
    "                else:\n",
    "                    y_pred[i,j] = nptypes[j](y_temp)\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp)\n",
    "        for j in range(len(column_labels)):\n",
    "            if column_labels[j] == 'percentile':\n",
    "                column_labels[j] = 'percentile_' + str(percentile)\n",
    "            else:\n",
    "                pass \n",
    "        column_labels_cr = ['cr_'+ lab for lab in column_labels]\n",
    "        column_labels_mpc = ['mpc_' + lab for lab in column_labels]\n",
    "        column_labels = ['grid']+column_labels_cr+column_labels_mpc\n",
    "        column_labels = [str(f_id_label)+ '_' + lab for lab in column_labels]\n",
    "        df = pd.DataFrame(np.c_[grid, y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return df #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for partial dependency of categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_cat_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, feature_ids_treat, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[],  feature_ids_cont=[], types=['mean'], percentile='none'): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor (or multiple predictors) in a random forest uplift 2 model approach.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    y_pred: corresponding predicted values of dependent variable (type: ndarray). \n",
    "            If input types is a list the columns in the array correspond to the chosen types in the same order\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is list):\n",
    "                if (len(f_id)<2):\n",
    "                    raise ValueError(f'{f_id} must be a list of hot-encoding features a hence neither empty nor have a length of 1')\n",
    "                else:\n",
    "                    cat = dict()\n",
    "                    positions = []\n",
    "                    for f in f_id:\n",
    "                        if type(f) is not str:\n",
    "                            raise ValueError('features in f_id list must be variable names of string type')\n",
    "                        else:\n",
    "                            if f not in feature_ids_treat:\n",
    "                                raise ValueError(f'categorical variable {f_id} is not a varibale of data frame')                                     \n",
    "                            else:\n",
    "                                cat[f+'_id'] = feature_ids_treat.index(f)\n",
    "                                cat[f+'_id_label'] = f\n",
    "                                positions = positions + [cat[f+'_id']]\n",
    "                    f_tuple = list(zip(f_id,positions))\n",
    "                    f_tuple = sorted(f_tuple, key = lambda x:x[1], reverse = False)\n",
    "                    f_id = [i[0] for i in f_tuple]\n",
    "                    positions = [i[1] for i in f_tuple]\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('Either X_treat or f_id is not correctly specified')\n",
    "        elif (type(X_treat) is np.ndarray) & (type(X_treat) is np.ndarray) & (type(f_id) is list):\n",
    "            if (len(f_id)<2):\n",
    "                raise ValueError(f'{f_id} must be a list of hot-encoding features a hence neither empty nor have a length of 1')                            \n",
    "            else:\n",
    "                cat = dict()\n",
    "                positions = []\n",
    "                for f in f_id:\n",
    "                    if type(f) is not str:\n",
    "                        raise ValueError('features in f_id list must be variable names of string type')\n",
    "                    else:\n",
    "                        if (f not in feature_ids_treat) | (f not in feature_ids_cont):\n",
    "                            raise ValueError(f'categorical variable {f_id} is not a varibale of cont or treat data frame')                                     \n",
    "                        elif (sorted(feature_ids_treat)!=sorted(feature_ids_cont)) & (type(X_cont_rbtamt) is not np.ndarray):\n",
    "                            raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                        else:\n",
    "                            cat[f+'_id'] = feature_ids_treat.index(f)\n",
    "                            cat[f+'_id_label'] = f\n",
    "                            #cat[f+'_tuple'] = listzip\n",
    "                            positions = positions + [cat[f+'_id']]\n",
    "                f_tuple = list(zip(f_id,positions))\n",
    "                f_tuple = sorted(f_tuple, key = lambda x:x[1], reverse = False)\n",
    "                f_id = [i[0] for i in f_tuple]\n",
    "                positions = [i[1] for i in f_tuple]\n",
    "                X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                mean_rbt = np.array(mean_rbt)\n",
    "                X_labels = list(X.columns)\n",
    "                X_temp = np.array(X)\n",
    "        else:\n",
    "            raise ValueError('X_treat and X_cont (if specified) need to be array types, f_id has to be a list of hot-encoded categorical variables')\n",
    "        \n",
    "        for f in f_id:\n",
    "            if (np.max(X_temp[:,cat[f+'_id']])!=1) | (np.min(X_temp[:,cat[f+'_id']])!=0):\n",
    "                raise ValueError('hot encoded variable is not of binary classification')\n",
    "            else:\n",
    "                pass\n",
    "        #grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower),\n",
    "        #np.percentile(X_temp[:, f_id], grid_upper),\n",
    "        \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types.copy()\n",
    "        #column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "\n",
    "        if set(types) <= set(function_labels):\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i]\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif type(percentile) is not list: # 0<=percentile<=100:\n",
    "                    raise ValueError('percentile must be list')\n",
    "                else:\n",
    "                    column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "                    if len(percentile)>1:\n",
    "                        for p in percentile[1:]:\n",
    "                            types = types + ['percentile'] #pass\n",
    "                            nptypes = nptypes + [np.percentile]\n",
    "                            column_labels = column_labels + ['percentile_' +str(p)]\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "        #y_pred = np.zeros((len(grid),len(types)))\n",
    "        #mpc_pred = np.zeros((len(grid), len(types)))        \n",
    "        y_pred = np.zeros((1, len(types)*len(f_id)))\n",
    "        mpc_pred = np.zeros((1, len(types)*len(f_id)))\n",
    "        grid_row = np.identity(len(f_id))\n",
    "        k=0\n",
    "        \n",
    "        column_labels_cr=[]\n",
    "        column_labels_mpc=[]\n",
    "        for f in range(len(f_id)): # i returns the counter, val returns the value at position of counter on grid\n",
    "            p_pos = 0\n",
    "            A = np.array([list(grid_row[f]) for _ in range(len(X_temp))])\n",
    "            X_temp[:, positions] = A\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[0,k] = nptypes[j](y_temp,percentile[p_pos])\n",
    "                    mpc_pred[0,k] = nptypes[j](mean_rbt_temp,percentile[p_pos])\n",
    "                    column_labels_cr = column_labels_cr + ['cr_'+ f_id[f] +'_'+ types[j] + str(percentile[p_pos])]\n",
    "                    column_labels_mpc= column_labels_mpc + ['mpc_'+ f_id[f] +'_'+ types[j] + str(percentile[p_pos])]\n",
    "                    p_pos = p_pos+1\n",
    "                else:\n",
    "                    y_pred[0,k] = nptypes[j](y_temp)\n",
    "                    mpc_pred[0,k] = nptypes[j](mean_rbt_temp)\n",
    "                    column_labels_cr = column_labels_cr + ['cr_'+ f_id[f] +'_'+ types[j]]\n",
    "                    column_labels_mpc= column_labels_mpc + ['mpc_'+ f_id[f] +'_'+ types[j]]\n",
    "                k = k+1\n",
    "        column_labels = column_labels_cr + column_labels_mpc\n",
    "        df = pd.DataFrame(np.c_[y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return df #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean', 'std']\n",
      "[[0.94817251 9.12532274 0.94842077 9.02684461 1.01159811 9.27271531]]\n",
      "[[ 643.92868553 1525.99264346  644.09727911 1506.71507605  687.00266092\n",
      "  1537.45160506]]\n",
      "['mean', 'std']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cr_educ_nodegree_mean</th>\n",
       "      <th>cr_educ_nodegree_std</th>\n",
       "      <th>cr_educ_highschool_mean</th>\n",
       "      <th>cr_educ_highschool_std</th>\n",
       "      <th>cr_educ_higher_mean</th>\n",
       "      <th>cr_educ_higher_std</th>\n",
       "      <th>mpc_educ_nodegree_mean</th>\n",
       "      <th>mpc_educ_nodegree_std</th>\n",
       "      <th>mpc_educ_highschool_mean</th>\n",
       "      <th>mpc_educ_highschool_std</th>\n",
       "      <th>mpc_educ_higher_mean</th>\n",
       "      <th>mpc_educ_higher_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>643.928686</td>\n",
       "      <td>1525.992643</td>\n",
       "      <td>644.097279</td>\n",
       "      <td>1506.715076</td>\n",
       "      <td>687.002661</td>\n",
       "      <td>1537.451605</td>\n",
       "      <td>0.948173</td>\n",
       "      <td>9.125323</td>\n",
       "      <td>0.948421</td>\n",
       "      <td>9.026845</td>\n",
       "      <td>1.011598</td>\n",
       "      <td>9.272715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cr_educ_nodegree_mean  cr_educ_nodegree_std  cr_educ_highschool_mean  \\\n",
       "0             643.928686           1525.992643               644.097279   \n",
       "\n",
       "   cr_educ_highschool_std  cr_educ_higher_mean  cr_educ_higher_std  \\\n",
       "0             1506.715076           687.002661         1537.451605   \n",
       "\n",
       "   mpc_educ_nodegree_mean  mpc_educ_nodegree_std  mpc_educ_highschool_mean  \\\n",
       "0                0.948173               9.125323                  0.948421   \n",
       "\n",
       "   mpc_educ_highschool_std  mpc_educ_higher_mean  mpc_educ_higher_std  \n",
       "0                 9.026845              1.011598             9.272715  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#f_id = ['educ_nodegree', 'educ_highschool','educ_higher']\n",
    "#display(uplift_cat_2m_partial_dependency_mpc(rfdicts['DUR_fin_baseline']['treat1_rf'], rfdicts['DUR_fin_baseline']['cont1_rf'], f_id, rfdicts['DUR_fin_baseline']['cont1_X_labels'], rfdicts['DUR_fin_baseline']['treat1_X'], rfdicts['DUR_fin_baseline']['treat1_rbtamt'], X_cont=rfdicts['DUR_fin_baseline']['cont1_X'], X_cont_rbtamt=rfdicts['DUR_fin_baseline']['cont1_rbtamt'],  feature_ids_cont=rfdicts['DUR_fin_baseline']['treat1_X_labels'], types=['mean','percentile','std'],percentile=[25,75]))\n",
    "#f_id = ['educ_highschool','educ_higher','educ_nodegree']\n",
    "#display(uplift_cat_2m_partial_dependency_mpc(rfdicts['TOT_fin']['treat1_rf'], rfdicts['TOT_fin']['cont1_rf'], f_id, rfdicts['TOT_fin']['cont1_X_labels'], rfdicts['TOT_fin']['treat1_X'], rfdicts['TOT_fin']['treat1_rbtamt'], X_cont=rfdicts['TOT_fin']['cont1_X'], X_cont_rbtamt=rfdicts['TOT_fin']['cont1_rbtamt'],  feature_ids_cont=rfdicts['TOT_fin']['treat1_X_labels'], types=['mean','std'])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run partial dependence function for given sample and explanatory variables. this may take a while. Hence, save as later as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "\n",
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "print(list(rfdicts[rfdicts_keys[0]]))\n",
    "print(list(rfdicts))\n",
    "#rfdicts['DUR_fin']['cont1_X_labels']\n",
    "INCOME = ['FINCBTAX','FSALARYM','FINCBTXM']\n",
    "CONTROL = ['adults', 'PERSLT18']\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'orgmrtx_sum', 'qescrowx_sum', 'timeleft']\n",
    "CAT = [['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5'],['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'],['educ_nodegree','educ_highschool','educ_higher']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FD_fin', 'FD_finit', 'FD_nofin', 'ND_fin', 'ND_finit', 'ND_nofin', 'SND_fin', 'SND_finit', 'SND_nofin', 'TOT_fin', 'TOT_finit', 'TOT_nofin']\n",
      "FD_fin:\n",
      "FD_fin treat1 cont1 FINCBTAX\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n",
      "FD_fin treat1 cont1 finassets\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n",
      "FD_fin treat1 cont1 morgpayment\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n",
      "FD_fin treat1 cont1 qblncm1x_sum\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n",
      "FD_fin treat1 cont1 orgmrtx_sum\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n",
      "FD_fin treat1 cont1 qescrowx_sum\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n",
      "FD_fin treat1 cont1 timeleft\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n",
      "FD_fin treat1 cont1 age\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n",
      "FD_fin treat1 cont1 adults\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n",
      "FD_fin treat1 cont1 PERSLT18\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n",
      "cont1\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n",
      "[[ 0.15922427 -0.13689051  2.45802832  0.10932659  0.3766382   0.15922427\n",
      "  -0.13689051  2.45802832  0.10932659  0.3766382   0.15922427 -0.13689051\n",
      "   2.45802832  0.10932659  0.3766382   0.15922427 -0.13689051  2.45802832\n",
      "   0.10932659  0.3766382 ]]\n",
      "[[ 108.13335334 -144.0665974   431.33743533  114.16017148  383.92221296\n",
      "   108.13335334 -144.0665974   431.33743533  114.16017148  383.92221296\n",
      "   108.13335334 -144.0665974   431.33743533  114.16017148  383.92221296\n",
      "   108.13335334 -144.0665974   431.33743533  114.16017148  383.92221296]]\n",
      "['mean', 'percentile_25', 'std', 'median', 'percentile_75']\n",
      "cont1\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n",
      "[[ 0.22510579 -0.08776004  2.55405194  0.16398989  0.43349633  0.09630013\n",
      "  -0.15406702  2.16475431  0.06653176  0.30276998  0.1602706  -0.11088337\n",
      "   2.20053541  0.10866829  0.34526923  0.09630013 -0.15406702  2.16475431\n",
      "   0.06653176  0.30276998]]\n",
      "[[ 152.8752157   -93.30751574  436.90398822  184.87379576  440.26080457\n",
      "    65.39992862 -164.0552901   385.69041116   75.36678669  313.97549557\n",
      "   108.84394686 -123.48877347  383.16170953  122.62738348  360.73711777\n",
      "    65.39992862 -164.0552901   385.69041116   75.36678669  313.97549557]]\n",
      "['mean', 'percentile_25', 'std', 'median', 'percentile_75']\n",
      "cont1\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n",
      "[[ 0.17419413 -0.11467787  2.4422193   0.12260106  0.38635043  0.11820247\n",
      "  -0.15538939  2.3398715   0.0797813   0.3433669   0.22750569 -0.08072578\n",
      "   2.49765638  0.15024928  0.42297564]]\n",
      "[[ 118.29977407 -127.30953037  425.89645796  137.25100597  400.21414912\n",
      "    80.27438149 -164.55894733  415.27494968   87.86942127  352.49446218\n",
      "   154.50504526  -94.82389335  425.59865881  168.91639326  433.82229364]]\n",
      "['mean', 'percentile_25', 'std', 'median', 'percentile_75']\n",
      "FD_finit:\n",
      "FD_finit treat1 cont1 FINCBTAX\n",
      "['mean', 'percentile', 'std', 'median', 'percentile']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-b611604428dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m                                                                \u001b[0mX_cont\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrfdicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_cont_rbtamt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrfdicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_rbtamt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                                                                \u001b[0mfeature_ids_treat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrfdicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_X_labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_ids_cont\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrfdicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_X_labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                                                                types=['mean','percentile','std','median'], percentile=[25,75])\n\u001b[0m\u001b[0;32m     31\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                     df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
      "\u001b[1;32m<ipython-input-21-2f12f87aa9f8>\u001b[0m in \u001b[0;36muplift_num_2m_partial_dependency_mpc\u001b[1;34m(rf_treat, rf_cont, f_id, X_treat, X_treat_rbtamt, X_cont, X_cont_rbtamt, feature_ids_treat, feature_ids_cont, types, percentile, grid_lower, grid_upper)\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                     \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnptypes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m                     \u001b[0mmpc_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnptypes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_rbt_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcolumn_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'percentile'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[1;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[0;32m   3495\u001b[0m     \"\"\"\n\u001b[0;32m   3496\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[1;32m-> 3497\u001b[1;33m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[0;32m   3498\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3499\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[1;34m(a, func, **kwargs)\u001b[0m\n\u001b[0;32m   3403\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3405\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3406\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_median\u001b[1;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[0;32m   3528\u001b[0m             \u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3529\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3530\u001b[1;33m         \u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"K\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#def vimp_plot_uplift()\n",
    "rfdicts_keys = list(rfdicts)\n",
    "#rfdicts_keys = rfdicts_keys[:1]\n",
    "print(rfdicts_keys)\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\pdp\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "    for t in treat:\n",
    "        for c in ['cont1']:\n",
    "            expvars = rfdicts[k][c+'_X_labels']\n",
    "            INCOME = ['FINCBTAX','FSALARYM','FINCBTXM']\n",
    "            if 'finassets' in expvars:\n",
    "                INCOME = INCOME + ['finassets']\n",
    "            if 'finassets_it' in expvars:\n",
    "                INCOME = INCOME + ['finassets_it']\n",
    "            for v in INCOME:\n",
    "                print(k,t,c,v)\n",
    "                if INCOME.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                               feature_ids_treat=rfdicts[k][t+'_X_labels'], feature_ids_cont=rfdicts[k][t+'_X_labels'], \n",
    "                                                               types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                                        feature_ids_treat=rfdicts[k][t+'_X_labels'],feature_ids_cont=rfdicts[k][t+'_X_labels'], \n",
    "                                                                        types=['mean','percentile','std','median'], percentile=[25,75])) \n",
    "            rfdicts[k][f'{c}_{t}_pdp_INCOME'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_INCOME.csv')\n",
    "            for v in MORTGAGE:\n",
    "                print(k,t,c,v)\n",
    "                if MORTGAGE.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],expvars[0],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'], \n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                                        feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75]))    \n",
    "            rfdicts[k][f'{c}_{t}_pdp_MORTGAGE'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_MORTGAGE.csv')\n",
    "            \n",
    "            for v in ['age']:\n",
    "                print(k,t,c,v)\n",
    "                df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],expvars[0],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "            rfdicts[k][f'{c}_{t}_pdp_age'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_age.csv')\n",
    "            \n",
    "            for v in CONTROL:\n",
    "                print(k,t,c,v)\n",
    "                if CONTROL.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],expvars[0],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                                        feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75]))    \n",
    "            rfdicts[k][f'{c}_{t}_pdp_MORTGAGE'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_CONTROL.csv')\n",
    "            \n",
    "            i = 1\n",
    "            for cat in CAT:\n",
    "                print(c)\n",
    "                df = uplift_cat_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],cat,rfdicts[k][t+'_X_labels'],rfdicts[k][t+'_X'],\n",
    "                                                           rfdicts[k][t+'_rbtamt'],X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                           feature_ids_cont=rfdicts[k][c+'_X_labels'],types=['mean','percentile','std','median'],\n",
    "                                                           percentile=[25,75])\n",
    "                rfdicts[k][f'{c}_{t}_pdp_CAT_{i}'] = df\n",
    "                df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_CAT_{i}.csv')\n",
    "                i = i+1\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot partial dependeny as comparison between the different specifications for a given control group and type of consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7** Visualize tree (not done yet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = ['FD_','SND']\n",
    "check = [key for key in pds_keys if key[0:7] == 'pdp_' + cons[0]]\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull out one tree from forest\n",
    "tree = rf.estimators_[5]\n",
    "\n",
    "#export image to a dot file\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = x_list, rounded = True, precision = 1 )\n",
    "(graph, )= pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree_check.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
