{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import PercentFormatter #plot as percentage\n",
    "import seaborn #plot density and histogram at the same time\n",
    "# Set directory where files are downloaded to. Chdir has to be changed in order to run on another computer\n",
    "os.chdir('C:\\\\Users\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE') #change this to the folder where the data set is stored, all the results will be saved in the same folder\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Data** (necessary for part1 and part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08 = pd.read_csv(os.getcwd()+'\\\\fs08.csv').set_index('CustID')\n",
    "#identifier\n",
    "TIME = ['QINTRVMO', 'QINTRVYR', 'rbtmo_1', 'rbtmo_2', 'diff_1', 'diff_2']\n",
    "ID = ['NEWID']\n",
    "\n",
    "#dependent variables\n",
    "CONS = ['FD','SND','ND','DUR','TOT']\n",
    "FUTCONS = ['fut_' + c for c in CONS]\n",
    "LRUNCONS = ['lrun_' + c for c in CONS]\n",
    "\n",
    "for i in range(len(LRUNCONS)):\n",
    "    fs08[LRUNCONS[i]] = fs08[[CONS[i],FUTCONS[i]]].sum(axis=1)\n",
    "\n",
    "#explanatory variables\n",
    "DEMO = ['age', 'adults', 'PERSLT18', 'MARITAL1', 'CUTENURE'] #exclude , 'FINCBTAX'\n",
    "    #age; number of adults; people below 18; marital status; housing tenure; income in the last 12 months\n",
    "DEMO2 = ['FSALARYM', 'FINCBTXM'] \n",
    "    #FSALARYM: income from salary and wages, CKBKACTX: balance/market value in balance accounts/brookerage accounts;    \n",
    "    #FINCBTXM: Total amount of family income before taxes (Imputed or collected data); (relevant demographics available for the second stimulus only)\n",
    "ASSETS = ['valid_finassets','finassets']\n",
    "    # finassets: sum of 1) SAVACCTX (Total balance/market value (including interest earned) CU had in savings accounts in banks, savings and loans,\n",
    "                         #credit unions, etc., as of the last day of previous month;)\n",
    "                # and    2)CKBKACTX (Total balance or market value (including interest earned) CU had in checking accounts, brokerage accounts, \n",
    "                            #and other similar accounts as of the last day of the previous month\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'qescrowx_sum', 'timeleft'] #exclude , 'orgmrtx_sum',\n",
    "    #morgpayment: morgage payment per month; qblncm1x_sum: sum of principal balances outstanding at the beginning of month M1; orgmrtx_sum: sum of mortgage amounts;\n",
    "    #qescrowx_sum: sum of last regular escrow payments; timeleft: maximum time left on mortgage payment\n",
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #\n",
    "#sample split\n",
    "RBT = ['rbtamt', 'rbtamt_chk', 'rbtamt_e']\n",
    "LAGRBT = ['last_' + var for var in RBT] #lagged variables\n",
    "FUTRBT = ['fut_' + var for var in RBT] #future variables\n",
    "\n",
    "for m in MORTGAGE:\n",
    "    fs08.loc[fs08[m].isna(),m]=0\n",
    "        \n",
    "\n",
    "\n",
    "fs08 = fs08[TIME + ID + CONS + DEMO + DEMO2 + ASSETS + MORTGAGE + RBT + ['rbtamt_1','rbtamt_2'] + LAGRBT + FUTRBT + FUTCONS + LRUNCONS + EDUC] #+ CHGCONS + LAGCONS \n",
    "#fs08 = fs08.loc[fs08['timeleft']>0,:]\n",
    "fs08 = pd.get_dummies(fs08, columns=['CUTENURE','MARITAL1']) #change categorical variables to dummy variables\n",
    "\n",
    "DEMO = [s for s in DEMO if s!='CUTENURE' if s!='MARITAL1'] + ['CUTENURE' + f'_{j}' for j in list(range(1,6)) if j!=3] +['MARITAL1' + f'_{j}' for j in list(range(1,5))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the average rebate amount per individual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08['rbtamt_idmean'] = 0\n",
    "fs08['rbtamt_idmean'] = fs08.groupby('CustID')['rbtamt'].transform('mean')\n",
    "fs08['rbt_count'] = 0\n",
    "fs08['rbt_count'] = fs08.groupby('CustID')['rbtamt'].transform('count')\n",
    "\n",
    "#\n",
    "#sometimes individuals give information of rebate receipt preceding (following) three months of the first (last) interview.\n",
    "#Wherever this is the case, the average rebate should be the weighted mean of rebates received before (after) the relevant time and the actual rebate \n",
    "\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['last_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) #weighted mean,  & (fs08['rbtamt_idmean']>0)\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist() # & (fs08['rbtamt_idmean']>0)\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first') #change for all entries for a given individual\n",
    "\n",
    "\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['fut_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) #weighted mean  & (fs08['rbtamt_idmean']>0)\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0)  & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() #& (fs08['rbtamt_idmean']>0)\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last')  #change for all entries for a given individual\n",
    "#display(fs08.loc[index, ['rbtamt_idmean', 'rbtamt', 'fut_rbtamt', 'last_rbtamt']])\n",
    "\n",
    "#wherever there is no entry for rebates received in the relevant time period but when there were rebates receivde in the past (future) change mean to the value\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() #                                                                               \n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 fs08['fut_rbtamt'], fs08['rbtamt_idmean'])\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last')\n",
    "\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist()\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())), \n",
    "                                fs08['last_rbtamt'], fs08['rbtamt_idmean'])\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first')\n",
    "\n",
    "\n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[(fs08['rbtamt']>0) | (fs08['fut_rbtamt']>0) | (fs08['last_rbtamt']>0) ,'rbt_flag'] = 1\n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('sum')\n",
    "fs08.loc[fs08['rbt_flag']>0, 'rbt_flag'] = 1\n",
    "fs08 = fs08.loc[fs08['rbt_flag']==1]\n",
    "\n",
    "\n",
    "index = fs08.index[fs08['rbtamt_idmean'].isna()].tolist()\n",
    "index = list(set(index))\n",
    "fs08.loc[index, 'rbtamt_idmean'] = fs08.loc[index,'fut_rbtamt']\n",
    "fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'fut_rbtamt' ]\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'last_rbtamt' ]\n",
    "\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations, impute values for financial liquidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(RBT)):\n",
    "    fs08.loc[fs08[RBT[i]]==0, RBT[i]] = np.nan\n",
    "    fs08.loc[fs08[LAGRBT[i]]==0, LAGRBT[i]] = np.nan\n",
    "    fs08.loc[fs08[FUTRBT[i]]==0, FUTRBT[i]] = np.nan\n",
    "    #fs08.loc[(fs08[LAGRBT[i]]==0) | (fs08[LAGRBT[i]].isna()), LAGRBT[i]] =  fs08.loc[(fs08[LAGRBT[i]]==0) | (fs08[LAGRBT[i]].isna())].groupby('CustID')[RBT[i]].shift(-1) \n",
    "    fs08.loc[(fs08[FUTRBT[i]]==0) | (fs08[FUTRBT[i]].isna()), FUTRBT[i]] =  fs08.loc[(fs08[FUTRBT[i]]==0) | (fs08[FUTRBT[i]].isna())].groupby('CustID')[RBT[i]].shift(-1) \n",
    "\n",
    "\n",
    "\n",
    "fs08 = fs08.reset_index()\n",
    "\n",
    "#Iterative imputation for financial liquidity\n",
    "#explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "fs08 = fs08.dropna(subset=CONS+DEMO+DEMO2+MORTGAGE) #Keep only observations that have all info on explanatory variables, dropping missing values on mortgage lowers the sample to half \n",
    "\n",
    "\n",
    "fs08_finit = fs08.copy()\n",
    "fs08_finit = fs08_finit[ ID + CONS + DEMO + DEMO2 + ASSETS + MORTGAGE +  LRUNCONS + EDUC]\n",
    "#fs08_finit = fs08_finit.loc[:,CONS+DEMO+DEMO2+MORTGAGE+['CustID','NEWID','finassets']]\n",
    "labels = list(fs08_finit.columns)\n",
    "imp_mean = IterativeImputer(random_state=0) #use python package iterative imputer\n",
    "imp_mean.fit(fs08_finit[2:])\n",
    "fs08_finit = pd.DataFrame(imp_mean.transform(fs08_finit),columns=labels)\n",
    "fs08_finit = fs08_finit.loc[:,['finassets','NEWID']]\n",
    "fs08_finit = fs08_finit.rename(columns={'finassets':'finassets_it'})\n",
    "\n",
    "fs08 = pd.merge(fs08.sort_values(by = ['NEWID']).reset_index(), fs08_finit.sort_values(by = ['NEWID']).reset_index(), how = 'left', on = 'NEWID', validate = '1:1')\n",
    "fs08 = fs08.drop(columns=['index_x','index_y']) \n",
    "ASSETS = ['valid_finassets','finassets', 'finassets_it' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate treatment and control groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate treatment group:\n",
    "fs08['treat1'] = 0 \n",
    "fs08.loc[fs08['rbtamt'].notna(),'treat1'] = 1 #all entries with actual info on rebate are in the treatment group\n",
    "\n",
    "#three different control groups:\n",
    "#control group 1: those who didn't receive the rebate in the given month\n",
    "fs08['cont1'] = 0\n",
    "fs08.loc[fs08['rbtamt'].isna(), 'cont1'] = 1\n",
    "\n",
    "#control group 2: drop one time period after receipt of rebate, as part of rebate might've been consumed one time period after\n",
    "fs08['cont2'] = 0 \n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[fs08['rbtamt']>0,'rbt_flag'] = 1 #identifier for rebate\n",
    "fs08['rbt_flag_lag'] = fs08.groupby('CustID')['rbt_flag'].shift(1) #identifier for rebate a period before (lag)\n",
    "fs08.loc[fs08['rbt_flag_lag']==1,'rbt_flag']=1 #change rebate identifier so it capture now if a rebate was received this period or the period before\n",
    "fs08.loc[(fs08['rbtamt'].isna()) & (fs08['rbt_flag']==0), 'cont2'] = 1 #those who didn't receive a rebate now or a period before are in the control group\n",
    "\n",
    "#treatment 2: all individuals who received a rebate last time period. This group should be compared to control group 2 only\n",
    "fs08['treat2'] = 0\n",
    "fs08.loc[(fs08['cont2']==0) & (fs08['treat1']==0),'treat2'] = 1\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt'].isna()), 'treat2'] = 1\n",
    "\n",
    "#treatment 3: long run consumption response: all individuals who received a rebate in this period and where the rebate interview is not the last\n",
    "fs08['treat3'] = 0\n",
    "fs08.loc[(fs08['rbtamt']>0) & (fs08[FUTCONS[0]].notna()), 'treat3'] = 1\n",
    "\n",
    "#control 3: long-run consumption: those who haven't received a rebate two periods from current period and have information on consumption for next period as well\n",
    "fs08['cont4'] = 0\n",
    "fs08.loc[(fs08['cont2']==1) & (fs08[FUTCONS[0]].notna()),'cont4'] = 1\n",
    "\n",
    "#control 4: those who haven't received the rebate yet\n",
    "fs08['cont3'] = 0 \n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('cumsum') #starts counting from the point on which the first rebate was received\n",
    "fs08.loc[(fs08['rbtamt'].isna()) & (fs08['rbt_flag']==0),'cont3'] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08_cap = fs08[(np.abs(stats.zscore(fs08.loc[:,CONS+LRUNCONS])) < 3).all(axis=1)] #drop outliers\n",
    "fs08_cap = fs08_cap.loc[fs08_cap['FD']>0] #there are still two observations where food consumption is zero; drop bc of common sense\n",
    "fs08_cap = fs08_cap.loc[fs08['ND']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Descriptive statistics** (part 1 and part 2 cn be run seperately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Take a look at the explanatory variables used for random forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4028.000000\n",
       "mean       49.705313\n",
       "std        15.572740\n",
       "min        21.000000\n",
       "25%        37.500000\n",
       "50%        49.000000\n",
       "75%        61.500000\n",
       "max        84.500000\n",
       "Name: age, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{FSALARYM} & \\multicolumn{2}{c}{FINCBTXM} & \\multicolumn{2}{c}{finassets} & \\multicolumn{2}{c}{finassets\\_it} \\\\\n",
      "{} &   treat1 &    cont1 &   treat1 &    cont1 &    treat1 &     cont1 &       treat1 &    cont1 \\\\\n",
      "\\midrule\n",
      "count &  4,028.0 & 11,195.0 &  4,028.0 & 11,195.0 &     723.0 &   1,458.0 &      4,028.0 & 11,195.0 \\\\\n",
      "mean  & 49,829.5 & 49,856.5 & 61,456.0 & 62,444.7 &  36,182.4 &  44,898.0 &     48,045.8 & 50,124.9 \\\\\n",
      "std   & 47,253.7 & 47,323.9 & 46,710.9 & 47,227.1 & 144,797.3 & 214,301.3 &     78,665.0 & 94,903.9 \\\\\n",
      "25\\%   & 12,977.9 & 11,953.5 & 28,149.5 & 29,000.0 &     225.0 &     200.0 &      9,616.5 & 11,342.8 \\\\\n",
      "50\\%   & 42,000.0 & 41,930.2 & 50,780.5 & 52,056.0 &   2,200.0 &   2,600.0 &     30,743.0 & 31,556.1 \\\\\n",
      "75\\%   & 73,000.0 & 74,000.0 & 81,927.0 & 82,553.5 &  14,000.0 &  16,000.0 &     61,857.0 & 62,810.2 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{morgpayment} & \\multicolumn{2}{c}{qblncm1x\\_sum} & \\multicolumn{2}{c}{qescrowx\\_sum} & \\multicolumn{2}{c}{timeleft} \\\\\n",
      "{} &      treat1 &   cont1 &       treat1 &     cont1 &       treat1 &   cont1 &   treat1 &   cont1 \\\\\n",
      "\\midrule\n",
      "count &     1,854.0 & 5,258.0 &      1,861.0 &   5,266.0 &      1,562.0 & 4,326.0 &  1,870.0 & 5,299.0 \\\\\n",
      "mean  &        39.8 &    39.5 &    125,795.3 & 123,429.3 &        359.7 &   364.1 &     19.5 &    19.4 \\\\\n",
      "std   &        54.4 &    57.7 &    107,836.2 & 108,120.2 &        315.3 &   338.7 &      8.7 &     8.8 \\\\\n",
      "25\\%   &        20.0 &    19.6 &     54,423.0 &  53,027.5 &        154.0 &   155.2 &     12.5 &    12.2 \\\\\n",
      "50\\%   &        29.5 &    28.9 &     99,183.0 &  96,212.0 &        275.0 &   273.0 &     21.8 &    21.9 \\\\\n",
      "75\\%   &        45.5 &    44.5 &    165,662.0 & 159,485.8 &        478.0 &   464.0 &     26.9 &    26.9 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'qescrowx_sum', 'timeleft'] #'orgmrtx_sum',\n",
    "expvars = DEMO+DEMO2+ASSETS+MORTGAGE+EDUC+['rbtamt','rbtamt_idmean']\n",
    "\n",
    "#income table\n",
    "for exp in expvars:\n",
    "    if exp == DEMO[0]:\n",
    "        des = fs08_cap.loc[fs08_cap['treat1']==1,exp].describe()\n",
    "        display(des)\n",
    "        des = pd.concat([des, fs08_cap.loc[fs08_cap['cont1']==1,exp].describe()], axis=1, join='inner')\n",
    "    else:\n",
    "        for g in ['treat1','cont1']:\n",
    "            if exp in MORTGAGE:\n",
    "                des = pd.concat([des, fs08_cap.loc[(fs08[g]==1)&(fs08[exp]>0),exp].describe()], axis=1, join='inner')\n",
    "            else:\n",
    "                des = pd.concat([des, fs08_cap.loc[fs08[g]==1,exp].describe()], axis=1, join='inner')\n",
    "\n",
    "index1 = [ i for i in expvars for reps in range(2) ]\n",
    "index2 = ['treat1','cont1']*int((len(index1)/2))\n",
    "tuples = list(zip(index1,index2))\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples) \n",
    "des_cols = list(des)\n",
    "des_cols=[i for i in des_cols if i[0] in ['FSALARYM', 'FINCBTXM','finassets','finassets_it']]\n",
    "print(des.iloc[[0,1,2,4,5,6]].to_latex(float_format=\"{:,.1f}\".format, columns=des_cols,multicolumn_format='c')) #table1\n",
    "des_cols = list(des)\n",
    "des_cols = [i for i in des_cols if i[0] in ['morgpayment', 'qblncm1x_sum','qescrowx_sum','timeleft']]\n",
    "print(des.iloc[[0,1,2,4,5,6]].to_latex(float_format=\"{:,.1f}\".format, columns=des_cols,multicolumn_format='c')) #table2\n",
    "des_cols = list(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average amount of rebate received per household\n",
      "\\begin{tabular}{llrrrrrrr}\n",
      "\\toprule\n",
      "       &             &  count &  mean &   max &  min &  25\\% &   75\\% &  std \\\\\n",
      "\\midrule\n",
      "cont1 & rbtamt\\_mean & 11,195 &   959 & 3,660 &    6 &  600 & 1,200 &  507 \\\\\n",
      "treat1 & rbtamt &  4,028 &   944 & 3,660 &    1 &  600 & 1,200 &  508 \\\\\n",
      "       & rbtamt\\_mean &  4,028 &   945 & 3,660 &    6 &  600 & 1,200 &  502 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Average amount of rebate received per household')\n",
    "des = fs08_cap.loc[fs08_cap['cont1']==1,'rbtamt_idmean'].describe()\n",
    "des = pd.concat([des, fs08_cap.loc[fs08_cap['treat1']==1,'rbtamt_idmean'].describe()], axis=1, join='inner',names=['cont1','treat1'])\n",
    "des = pd.concat([des, fs08_cap.loc[fs08['rbtamt']>0,'rbtamt'].describe()], axis=1, join='inner',names=['cont1','treat1','treat1'])\n",
    "tuples = [('rbtamt_mean', 'cont1'), ('rbtamt_mean', 'treat1'), ('rbtamt', 'treat1')]\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples)\n",
    "print(des.stack().unstack(level=0).stack(level=0).to_latex(float_format=\"{:,.0f}\".format,  columns=['count','mean','max','min','25%','75%','std'])) #table 3\n",
    "#print(des1.loc[['count', 'mean', 'std', 'min','50%','max'],:].to_latex(float_format=\"{:,.1f}\".format))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Descriptives for dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{FD} & \\multicolumn{2}{c}{SND} & \\multicolumn{2}{c}{ND} & \\multicolumn{2}{c}{TOT} \\\\\n",
      "{} &  treat1 &    cont1 &   treat1 &    cont1 &   treat1 &    cont1 &   treat1 &    cont1 \\\\\n",
      "\\midrule\n",
      "count & 4,028.0 & 11,195.0 &  4,028.0 & 11,195.0 &  4,028.0 & 11,195.0 &  4,028.0 & 11,195.0 \\\\\n",
      "mean  & 1,867.7 &  1,795.1 &  4,264.4 &  4,010.1 &  5,318.6 &  5,082.2 &  9,432.4 &  9,119.5 \\\\\n",
      "std   &   981.4 &    943.9 &  1,999.9 &  1,871.9 &  2,532.5 &  2,392.4 &  4,766.9 &  4,645.6 \\\\\n",
      "min   &    65.0 &     26.0 &    250.0 &    133.0 &    372.0 &    444.0 &    372.0 &    747.0 \\\\\n",
      "25\\%   & 1,154.0 &  1,105.0 &  2,768.8 &  2,620.0 &  3,386.8 &  3,290.7 &  5,807.2 &  5,696.3 \\\\\n",
      "50\\%   & 1,695.0 &  1,635.0 &  4,002.5 &  3,736.0 &  4,960.0 &  4,712.0 &  8,568.3 &  8,248.4 \\\\\n",
      "75\\%   & 2,425.0 &  2,320.0 &  5,485.0 &  5,119.0 &  6,844.9 &  6,480.8 & 12,243.1 & 11,653.0 \\\\\n",
      "max   & 5,580.0 &  5,617.0 & 11,881.5 & 11,803.3 & 14,759.7 & 14,799.7 & 31,505.9 & 31,664.5 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "treat = ['treat1','treat2','treat3']\n",
    "cont = ['cont1','cont2','cont3','cont4']\n",
    "#baseline:\n",
    "CONS=['FD','SND','ND','TOT']\n",
    "treat = ['treat1']\n",
    "cont = ['cont1']\n",
    "\n",
    "for i in CONS:\n",
    "    if i=='FD':\n",
    "        des = fs08_cap.loc[fs08_cap['treat1']==1,i].describe()\n",
    "        des = pd.concat([des,fs08_cap.loc[fs08_cap['cont1']==1,i].describe()],join='inner',axis=1)\n",
    "    else:\n",
    "        for g in ['treat1','cont1']:\n",
    "            des = pd.concat([des, fs08_cap.loc[fs08_cap[g]==1,i].describe()],join='inner', axis=1)\n",
    "\n",
    "index1 = ['FD']*2 + ['SND']*2 + ['ND']*2 + ['TOT']*2\n",
    "index2 = ['treat1','cont1']*8\n",
    "tuples = list(zip(index1,index2))\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples)\n",
    "print(des.to_latex(float_format=\"{:,.1f}\".format ,multicolumn_format='c')) #table 4\n",
    "#des.stack().unstack(level=0).stack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare histograms of treatment group with different control groups\n",
    "\n",
    "for i in range(len(CONS)):\n",
    "    for t in treat[0:1]:\n",
    "        for c in cont[0:1]:\n",
    "            plt.figure(figsize=(3.2,2.4))\n",
    "            plt.title(f'{CONS[i]}')\n",
    "            plt.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            #plt.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, density=True, bins=30, color='red')\n",
    "            plt.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            #plt.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= t, density=True, bins=30, color='green')\n",
    "            #plt.set_title(f'{CONS[i]}:')\n",
    "            #plt.tight_layout()\n",
    "            plt.xticks(fontsize = 6)\n",
    "            plt.yticks(fontsize=6)\n",
    "            plt.legend(loc='upper right', frameon=True, fontsize=6)\n",
    "            plt.savefig(os.getcwd() + f'\\\\descriptives\\\\{CONS[i]}_pattern.pdf')\n",
    "            plt.close()\n",
    "CONS=['SND','TOT']\n",
    "fig=plt.figure(figsize=(6.4,2.4))\n",
    "for i in range(len(CONS)):\n",
    "    #plt.figure(figsize=(6.4,2.4))\n",
    "    plot = plt.subplot(1,2,i+1)\n",
    "    for t in treat[0:1]:\n",
    "        for c in cont[0:1]:     \n",
    "            #plot = plt.figure(figsize=(10,3))\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            plot.set_title(f'{CONS[i]}:')\n",
    "            plt.tight_layout()\n",
    "            plt.xticks(fontsize = 8)\n",
    "            plt.yticks(fontsize=8)\n",
    "            plt.legend(loc='upper right', frameon=False, fontsize=8)\n",
    "            #plot_count = plot_count+1\n",
    "            plt.savefig(os.getcwd() + f'\\\\descriptives\\\\SND_TOT_pattern_group1_treat1.pdf') #figure 1\n",
    "plt.close()\n",
    "\n",
    "\n",
    "CONS=['FD','SND','ND','TOT']        \n",
    "plot_count = 1\n",
    "for i in range(len(CONS)):\n",
    "    #plt.figure(figsize=(6.4,2.4))\n",
    "    for t in treat[0:1]:\n",
    "        fig=plt.figure(figsize=(6.4,2.4))\n",
    "        for c in cont[0:3]:     \n",
    "            #plot = plt.figure(figsize=(10,3))\n",
    "            plot = plt.subplot(1,3,cont.index(c)+1)\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            plot.set_title(f'{CONS[i]}:')\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc='upper right', frameon=False)\n",
    "            plot_count = plot_count+1\n",
    "            #plt.savefig(os.getcwd() + f'\\\\descriptives\\\\{CONS[i]}_pattern_groupcomp.pdf')\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "descriptives for rebate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: Machine learning approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Define sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Run random forest algorithm seperately for treatment and control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Out of bag estimation only available if bootstrap=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-569ef3799cda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mrf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_rf'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExtraTreesRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moob_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.33\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mrf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_rf'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mf'\\\\rf_dicts\\\\{depvar}_finit.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbootstrap\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moob_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m             raise ValueError(\"Out of bag estimation only available\"\n\u001b[0m\u001b[0;32m    291\u001b[0m                              \" if bootstrap=True\")\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Out of bag estimation only available if bootstrap=True"
     ]
    }
   ],
   "source": [
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #'educ_bachelor','educ_master','educ_doctorate'\n",
    "DEMO = ['age', 'adults', 'PERSLT18','CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5', 'MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'] # exclude 'FINCBTAX',\n",
    "DEMO2 = [ 'FINCBTXM','FSALARYM' ] # exclude 'FSALARYM',\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum','qescrowx_sum', 'timeleft'] # 'orgmrtx_sum', \n",
    "CONS = ['FD', 'SND', 'ND', 'TOT']\n",
    "CONT = ['cont1', 'cont2', 'cont3']\n",
    "TREAT = 'treat1'\n",
    "treatgroup = TREAT\n",
    "trees = 1000\n",
    "\n",
    "#Random Forest for short term consumption: treatment group 1 with imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC + ['finassets_it']  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_finit.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_nofin.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group with just the observations where financial assets are included\n",
    "TREAT = 'treat1'\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + ['finassets'] + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1) & (fs08_cap['valid_finassets']==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[(fs08_cap[con]==1) & (fs08_cap['valid_finassets']==1) , [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_fin.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "#pkl_file = open('myfile.pkl', 'rb')\n",
    "#rf2 = pickle.load(pkl_file)\n",
    "#pkl_file.close()\n",
    "#\n",
    "#print(rf)\n",
    "#print(rf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Predict Outcomes for overall consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_predicitons_rbt(rf_treat, rf_cont, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if (type(X_cont) is not np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_treat_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('X_treat_rbamt needs to have an array like structure')\n",
    "            else:\n",
    "                X_temp = X_treat.copy()\n",
    "                rbtamt_temp = X_treat_rbtamt.copy()\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_cont_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('if X_cont is specified, X_cont_rbamt needs to have an array like structure')\n",
    "            if sorted(feature_ids_treat)!=sorted(feature_ids_cont):\n",
    "                raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont')\n",
    "            elif (len(feature_ids_treat)==0) | (len(feature_ids_cont)==0):\n",
    "                raise ValueError(f'if X_treat and X_cont are specified, feature_ids must not be empty')\n",
    "            else:\n",
    "                X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                rbtamt_temp = pd.concat([pd.DataFrame(X_treat_rbtamt), pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                X_labels = list(X.columns)\n",
    "                X_temp = np.array(X)\n",
    "                rbtamt_temp = np.array(rbtamt_temp)\n",
    "        else: \n",
    "            raise ValueError('X_treat does not have an array like structure')\n",
    "        y = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "        mpc = y/rbtamt_temp[:,0]\n",
    "        return y,mpc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD_fin:\n",
      "cont2_treat1_y_pred (1479,)\n",
      "cont2_treat1_mpc_pred (1479,)\n",
      "cont1_treat1_y_pred (2181,)\n",
      "cont1_treat1_mpc_pred (2181,)\n",
      "cont3_treat1_y_pred (798,)\n",
      "cont3_treat1_mpc_pred (798,)\n",
      "FD_finit:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "FD_nofin:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "ND_fin:\n",
      "cont2_treat1_y_pred (1479,)\n",
      "cont2_treat1_mpc_pred (1479,)\n",
      "cont1_treat1_y_pred (2181,)\n",
      "cont1_treat1_mpc_pred (2181,)\n",
      "cont3_treat1_y_pred (798,)\n",
      "cont3_treat1_mpc_pred (798,)\n",
      "ND_finit:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "ND_nofin:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "SND_fin:\n",
      "cont2_treat1_y_pred (1479,)\n",
      "cont2_treat1_mpc_pred (1479,)\n",
      "cont1_treat1_y_pred (2181,)\n",
      "cont1_treat1_mpc_pred (2181,)\n",
      "cont3_treat1_y_pred (798,)\n",
      "cont3_treat1_mpc_pred (798,)\n",
      "SND_finit:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "SND_nofin:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "TOT_fin:\n",
      "cont2_treat1_y_pred (1479,)\n",
      "cont2_treat1_mpc_pred (1479,)\n",
      "cont1_treat1_y_pred (2181,)\n",
      "cont1_treat1_mpc_pred (2181,)\n",
      "cont3_treat1_y_pred (798,)\n",
      "cont3_treat1_mpc_pred (798,)\n",
      "TOT_finit:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "TOT_nofin:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n"
     ]
    }
   ],
   "source": [
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\condistr\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for t in treat:\n",
    "        for c in cont:\n",
    "            y,mpc = uplift_predicitons_rbt(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                          rfdicts[k][c+'_X'],rfdicts[k][c+'_rbtamt'],rfdicts[k][t+'_X_labels'],rfdicts[k][c+'_X_labels'])\n",
    "            rfdicts[k][f'{c}_{t}_y_pred'] = y\n",
    "            print(f'{c}_{t}_y_pred', y.shape)\n",
    "            rfdicts[k][f'{c}_{t}_mpc_pred'] = mpc\n",
    "            print(f'{c}_{t}_mpc_pred', mpc.shape)\n",
    "            #plt.hist(y, bins=40,  edgecolor='black')\n",
    "            #lower = round((min(y)/100),1)*100\n",
    "            #upper = round((max(y)/100),1)*100+1\n",
    "            #plt.xticks(np.arange(lower, upper, 1000))\n",
    "            #plt.title(f'Pred cons resp distr., {vartype},{c},{t}')\n",
    "            #plt.xlabel(f'consumption in {cons}, number of observations {y.shape[0]}')\n",
    "            #plt.ylabel(f'number of individuals in bin')\n",
    "            #plt.savefig(newpath + f'\\\\{vartype}_{c}_{t}_y_pred.pdf')\n",
    "            #plt.close()         \n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD_fin\n",
      "FD_finit\n",
      "FD_nofin\n",
      "ND_fin\n",
      "ND_finit\n",
      "ND_nofin\n",
      "SND_fin\n",
      "SND_finit\n",
      "SND_nofin\n",
      "TOT_fin\n",
      "TOT_finit\n",
      "TOT_nofin\n"
     ]
    }
   ],
   "source": [
    "for k in rfdicts_keys:\n",
    "    print(f'{k}')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\condistr\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for t in treat:\n",
    "        for c in cont:\n",
    "            y = rfdicts[k][f'{c}_{t}_y_pred']\n",
    "            mpc = rfdicts[k][f'{c}_{t}_mpc_pred']\n",
    "            plt.hist(y, bins=40,  edgecolor='black')\n",
    "            lower = round((min(y)/100),1)*100\n",
    "            upper = round((max(y)/100),1)*100+1\n",
    "            plt.xticks(np.arange(lower, upper, 1000))\n",
    "            plt.title(f'Pred cons resp distr., {vartype},{c},{t}')\n",
    "            plt.xlabel(f'consumption in {cons}, number of observations {y.shape[0]}')\n",
    "            plt.ylabel(f'number of individuals in bin')\n",
    "            plt.savefig(newpath + f'\\\\{vartype}_{c}_{t}_y_pred.pdf')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Figure' object has no attribute 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-27097a67a8b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#plt.xlabel(f'consumption in {cons}, number of observations {y.shape[0]}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#plt.ylabel(f'number of individuals in bin')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Check'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Figure' object has no attribute 'title'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x172.8 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "CONS = ['SND','TOT']\n",
    "\n",
    "rfdicts_subplot = [k for k in rfdicts_keys if k.split('_')[0] in CONS if k.split('_')[1]=='nofin' ]\n",
    "\n",
    "fig=plt.figure(figsize=(6.4,2.4))\n",
    "i=0\n",
    "for k in rfdicts_subplot:\n",
    "    c = 'cont1'\n",
    "    t= 'treat1'\n",
    "    plot = plt.subplot(1,2,i+1)    \n",
    "    y = rfdicts[k][f'{c}_{t}_y_pred']\n",
    "    plot.hist(y, bins=40,  edgecolor='black')\n",
    "    lower = round((min(y)/100),1)*100\n",
    "    upper = round((max(y)/100),1)*100+1\n",
    "    plt.xticks(np.arange(lower, upper, 2000), fontsize = 8)\n",
    "    #plt.title(f'Pred cons resp distr., {vartype},{c},{t}')\n",
    "    plt.tight_layout()\n",
    "    plt.yticks(fontsize=8)\n",
    "    #plt.savefig(os.getcwd() + f'\\\\condistr\\\\SND_TOT_consreso_{c}_{t}.pdf') #figure 1\n",
    "    i=i+1\n",
    "#plt.xlabel(f'consumption in {cons}, number of observations {y.shape[0]}')\n",
    "#plt.ylabel(f'number of individuals in bin')    \n",
    "plt.show    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** Plot distribution of consumption response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5** Variable importance plot for treatment and control group separately and as a weighted sum for the whole sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfdicts_keys = list(rfdicts)\n",
    "#print(rfdicts_keys)\n",
    "#print(list(rfdicts[rfdicts_keys[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "FD_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "FD_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "FD_nofin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "ND_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "ND_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "ND_nofin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "SND_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "SND_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "SND_nofin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "TOT_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "TOT_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "TOT_nofin\n"
     ]
    }
   ],
   "source": [
    "#def vimp_plot_uplift()\n",
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "for k in rfdicts_keys:\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\varimp\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    print(treat+cont)\n",
    "    uplift_imp = dict()\n",
    "    for i in treat+cont:\n",
    "        importances = (rfdicts[k][i+'_rf'].feature_importances_)\n",
    "        X_importances = [(label, importance) for label, importance in zip(rfdicts[k][i+'_X_labels'],importances)]\n",
    "        #X_importances = [(round(importance,2), label) for importance, label in zip(importances, rf[i+'_X_labels'])]\n",
    "        X_importances = sorted(X_importances, key = lambda x:x[1], reverse = False)\n",
    "        uplift_imp[i+'_varimp_values'] = [x[1] for x in X_importances]\n",
    "        uplift_imp[i+'_varimp_labels'] = [x[0] for x in X_importances]\n",
    "        uplift_imp[i+'_varimp_tuples'] = X_importances\n",
    "        \n",
    "    \n",
    "    for i in treat + cont:\n",
    "        X_importances = sorted(X_importances, key = lambda x:x[0].upper(), reverse = False) #sort in alphabetical order\n",
    "        uplift_imp[i+'_values'] = [x[1] for x in X_importances] #importances \n",
    "        uplift_imp[i+'_labels'] = [x[0] for x in X_importances] \n",
    "        shape = rfdicts[k][i+'_X'].shape\n",
    "        uplift_imp[i+'_sample'] = shape[0] \n",
    "    \n",
    "    for t in treat:\n",
    "        plotgroups = [t]\n",
    "        for c in cont:\n",
    "            plotgroups = plotgroups + [c] + [c+'_'+t]\n",
    "            uplift_imp[f'{c}_{t}_sample'] = uplift_imp[f'{t}_sample'] + uplift_imp[f'{c}_sample'] \n",
    "            uplift_imp[f'{c}_{t}_varimp_values'] = [uplift_imp[f'{t}_sample']/(uplift_imp[f'{c}_{t}_sample'])*uplift_imp[f'{t}_values'][i] + \n",
    "            uplift_imp[f'{c}_sample']/(uplift_imp[f'{c}_{t}_sample'])*uplift_imp[f'{c}_values'][i] for i in range(len(uplift_imp[f'{t}_values']))]\n",
    "\n",
    "            up_importances = [(label, importance) for label, importance in zip(uplift_imp[f'{t}_labels'],uplift_imp[f'{c}_{t}_varimp_values'])]\n",
    "            up_importances = sorted(up_importances, key = lambda x:x[1], reverse = False)\n",
    "            uplift_imp[f'{c}_{t}_varimp_tuples'] = up_importances            \n",
    "            uplift_imp[f'{c}_{t}_varimp_values'] = [up[1] for up in up_importances]\n",
    "            uplift_imp[f'{c}_{t}_varimp_labels'] = [up[0] for up in up_importances]\n",
    "        print(k)   \n",
    "    for g in plotgroups:\n",
    "        freq_series = pd.Series(uplift_imp[g+'_varimp_values'])\n",
    "        y_labels = uplift_imp[g+'_varimp_labels']\n",
    "\n",
    "        # Plot the figure.\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        ax = freq_series.plot(kind='barh')\n",
    "        ssize=uplift_imp[g+'_sample']\n",
    "        ax.set_title(f'Variable Importance Plot for {k},{g.upper()} Sample')\n",
    "        ax.set_xlabel(f'Frequency, sample size = {str(ssize)}')\n",
    "        ax.set_ylabel(f'Variable')\n",
    "        ax.set_yticklabels(y_labels)\n",
    "        #ax.set_xlim(-40, 300) # expand xlim to make labels easier to read\n",
    "\n",
    "        rects = ax.patches\n",
    "\n",
    "        # For each bar: Place a label\n",
    "        for rect in rects:\n",
    "            # Get X and Y placement of label from rect.\n",
    "            x_value = rect.get_width()\n",
    "            y_value = rect.get_y() + rect.get_height() / 2\n",
    "\n",
    "            # Number of points between bar and label. Change to your liking.\n",
    "            space = 3\n",
    "            # Vertical alignment for positive values\n",
    "            ha = 'left'\n",
    "\n",
    "            # Use X value as label and format number with one decimal place\n",
    "            label = \"{:.3f}\".format(x_value)\n",
    "\n",
    "            # Create annotation\n",
    "            plt.annotate(\n",
    "                label,                      # Use `label` as label\n",
    "                (x_value, y_value),         # Place label at end of the bar\n",
    "                xytext=(space, 0),          # Horizontally shift label by `space`\n",
    "                textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "                va='center',                # Vertically center label\n",
    "                ha=ha)                      # Horizontally align label differently for\n",
    "                                            # positive and negative values.\n",
    "        plt.savefig(f'{newpath}\\\\{vartype}_{g}.pdf')\n",
    "        plt.close()\n",
    "        #plt.savefig(\"image.png\")\n",
    "        #plt.savefig(newpath + '\\\\'+ pathend +f'_{i}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Partial dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6.2** Function for uplift 2model partial dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_num_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[], types=['mean'], percentile='none', grid_lower=5, grid_upper=95 ): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor (or multiple predictors) in a random forest uplift 2 model approach.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    y_pred: corresponding predicted values of dependent variable (type: ndarray). \n",
    "            If input types is a list the columns in the array correspond to the chosen types in the same order\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is int):\n",
    "                if f_id > (X_temp.shape[1]-1):\n",
    "                    raise ValueError(f'positional number of {f_id} exceeds array shape')\n",
    "                else:\n",
    "                    X_temp = X_treat.copy()\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str):\n",
    "                if f_id not in feature_ids:\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids is not passed to the function')\n",
    "                else:\n",
    "                    f_id = feature_ids.index(f_id)\n",
    "                    f_id_label = f_id\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('f_id needs to be either an integer or a string')\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(f_id) is int):\n",
    "                raise ValueError(f'if X_cont is specified, then f_id needs to be a string variable')\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str) & ((f_id not in feature_ids_treat) |  (f_id not in feature_ids_cont)):\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids_treat or feature_ids_cont is not passed to the function')\n",
    "            else:\n",
    "                if (sorted(feature_ids_treat)!=sorted(feature_ids_cont)) & (type(X_cont_rbtamt) is not np.ndarray) :\n",
    "                    raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                else:\n",
    "                    X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = np.array(mean_rbt)\n",
    "                    X_labels = list(X.columns)\n",
    "                    X_temp = np.array(X)\n",
    "                    f_id_label = f_id\n",
    "                    f_id = X_labels.index(f_id)             \n",
    "        else: \n",
    "            raise ValueError('Either X_cont or X_treat does not have an array like structure')\n",
    "\n",
    "                       #['age', 'adults', 'PERSLT18'\n",
    "        X_unique = np.array(list(set(X_temp[:, f_id])))\n",
    "        if len(X_unique)*3 > 100:\n",
    "            grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower), np.percentile(X_temp[:, f_id], grid_upper), 100)\n",
    "        else:\n",
    "            grid = np.linspace(np.percentile(X_temp[:, f_id],grid_lower),np.percentile(X_temp[:, f_id],grid_upper), len(X_unique)*2)\n",
    "       \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types.copy()\n",
    "        #column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "\n",
    "        if set(types) <= set(function_labels):\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i]\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif type(percentile) is not list: # 0<=percentile<=100:\n",
    "                    raise ValueError('percentile must be list')\n",
    "                else:\n",
    "                    column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "                    if len(percentile)>1:\n",
    "                        for p in percentile[1:]:\n",
    "                            types = types + ['percentile'] #pass\n",
    "                            nptypes = nptypes + [np.percentile]\n",
    "                            column_labels = column_labels + ['percentile_' +str(p)]\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "        y_pred = np.zeros((len(grid),len(types)))\n",
    "        mpc_pred = np.zeros((len(grid), len(types)))\n",
    "        p_pos = 0\n",
    "        for i, val in enumerate(grid): # i returns the counter, val returns the value at position of counter on grid \n",
    "            X_temp[:, f_id] = val\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            p_pos = 0\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[i,j] = nptypes[j](y_temp,percentile[p_pos])\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp,percentile[p_pos])\n",
    "                    p_pos = p_pos + 1\n",
    "                else:\n",
    "                    y_pred[i,j] = nptypes[j](y_temp)\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp)\n",
    "        for j in range(len(column_labels)):\n",
    "            if column_labels[j] == 'percentile':\n",
    "                column_labels[j] = 'percentile_' + str(percentile)\n",
    "            else:\n",
    "                pass \n",
    "        column_labels_cr = ['cr_'+ lab for lab in column_labels]\n",
    "        column_labels_mpc = ['mpc_' + lab for lab in column_labels]\n",
    "        column_labels = ['grid']+column_labels_cr+column_labels_mpc\n",
    "        column_labels = [str(f_id_label)+ '_' + lab for lab in column_labels]\n",
    "        df = pd.DataFrame(np.c_[grid, y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return df #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for partial dependency of categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_cat_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, feature_ids_treat, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[],  feature_ids_cont=[], types=['mean'], percentile='none'): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor (or multiple predictors) in a random forest uplift 2 model approach.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    y_pred: corresponding predicted values of dependent variable (type: ndarray). \n",
    "            If input types is a list the columns in the array correspond to the chosen types in the same order\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is list):\n",
    "                if (len(f_id)<2):\n",
    "                    raise ValueError(f'{f_id} must be a list of hot-encoding features a hence neither empty nor have a length of 1')\n",
    "                else:\n",
    "                    cat = dict()\n",
    "                    positions = []\n",
    "                    for f in f_id:\n",
    "                        if type(f) is not str:\n",
    "                            raise ValueError('features in f_id list must be variable names of string type')\n",
    "                        else:\n",
    "                            if f not in feature_ids_treat:\n",
    "                                raise ValueError(f'categorical variable {f_id} is not a varibale of data frame')                                     \n",
    "                            else:\n",
    "                                cat[f+'_id'] = feature_ids_treat.index(f)\n",
    "                                cat[f+'_id_label'] = f\n",
    "                                positions = positions + [cat[f+'_id']]\n",
    "                    f_tuple = list(zip(f_id,positions))\n",
    "                    f_tuple = sorted(f_tuple, key = lambda x:x[1], reverse = False)\n",
    "                    f_id = [i[0] for i in f_tuple]\n",
    "                    positions = [i[1] for i in f_tuple]\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('Either X_treat or f_id is not correctly specified')\n",
    "        elif (type(X_treat) is np.ndarray) & (type(X_treat) is np.ndarray) & (type(f_id) is list):\n",
    "            if (len(f_id)<2):\n",
    "                raise ValueError(f'{f_id} must be a list of hot-encoding features a hence neither empty nor have a length of 1')                            \n",
    "            else:\n",
    "                cat = dict()\n",
    "                positions = []\n",
    "                for f in f_id:\n",
    "                    if type(f) is not str:\n",
    "                        raise ValueError('features in f_id list must be variable names of string type')\n",
    "                    else:\n",
    "                        if (f not in feature_ids_treat) | (f not in feature_ids_cont):\n",
    "                            raise ValueError(f'categorical variable {f_id} is not a varibale of cont or treat data frame')                                     \n",
    "                        elif (sorted(feature_ids_treat)!=sorted(feature_ids_cont)) & (type(X_cont_rbtamt) is not np.ndarray):\n",
    "                            raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                        else:\n",
    "                            cat[f+'_id'] = feature_ids_treat.index(f)\n",
    "                            cat[f+'_id_label'] = f\n",
    "                            #cat[f+'_tuple'] = listzip\n",
    "                            positions = positions + [cat[f+'_id']]\n",
    "                f_tuple = list(zip(f_id,positions))\n",
    "                f_tuple = sorted(f_tuple, key = lambda x:x[1], reverse = False)\n",
    "                f_id = [i[0] for i in f_tuple]\n",
    "                positions = [i[1] for i in f_tuple]\n",
    "                X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                mean_rbt = np.array(mean_rbt)\n",
    "                X_labels = list(X.columns)\n",
    "                X_temp = np.array(X)\n",
    "        else:\n",
    "            raise ValueError('X_treat and X_cont (if specified) need to be array types, f_id has to be a list of hot-encoded categorical variables')\n",
    "        \n",
    "        for f in f_id:\n",
    "            print(np.max(X_temp[:,cat[f+'_id']]))\n",
    "            print(np.min(X_temp[:,cat[f+'_id']]))\n",
    "            if (np.max(X_temp[:,cat[f+'_id']])!=1) | (np.min(X_temp[:,cat[f+'_id']])!=0):\n",
    "                raise ValueError('hot encoded variable is not of binary classification')\n",
    "            else:\n",
    "                pass\n",
    "        #grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower),\n",
    "        #np.percentile(X_temp[:, f_id], grid_upper),\n",
    "        \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types.copy()\n",
    "        #column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "\n",
    "        if set(types) <= set(function_labels):\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i]\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif type(percentile) is not list: # 0<=percentile<=100:\n",
    "                    raise ValueError('percentile must be list')\n",
    "                else:\n",
    "                    column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "                    if len(percentile)>1:\n",
    "                        for p in percentile[1:]:\n",
    "                            types = types + ['percentile'] #pass\n",
    "                            nptypes = nptypes + [np.percentile]\n",
    "                            column_labels = column_labels + ['percentile_' +str(p)]\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "        #y_pred = np.zeros((len(grid),len(types)))\n",
    "        #mpc_pred = np.zeros((len(grid), len(types)))        \n",
    "        y_pred = np.zeros((1, len(types)*len(f_id)))\n",
    "        mpc_pred = np.zeros((1, len(types)*len(f_id)))\n",
    "        grid_row = np.identity(len(f_id))\n",
    "        k=0\n",
    "        \n",
    "        column_labels_cr=[]\n",
    "        column_labels_mpc=[]\n",
    "        for f in range(len(f_id)): # i returns the counter, val returns the value at position of counter on grid\n",
    "            p_pos = 0\n",
    "            A = np.array([list(grid_row[f]) for _ in range(len(X_temp))])\n",
    "            X_temp[:, positions] = A\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[0,k] = nptypes[j](y_temp,percentile[p_pos])\n",
    "                    mpc_pred[0,k] = nptypes[j](mean_rbt_temp,percentile[p_pos])\n",
    "                    column_labels_cr = column_labels_cr + ['cr_'+ f_id[f] +'_'+ types[j] + str(percentile[p_pos])]\n",
    "                    column_labels_mpc= column_labels_mpc + ['mpc_'+ f_id[f] +'_'+ types[j] + str(percentile[p_pos])]\n",
    "                    p_pos = p_pos+1\n",
    "                else:\n",
    "                    y_pred[0,k] = nptypes[j](y_temp)\n",
    "                    mpc_pred[0,k] = nptypes[j](mean_rbt_temp)\n",
    "                    column_labels_cr = column_labels_cr + ['cr_'+ f_id[f] +'_'+ types[j]]\n",
    "                    column_labels_mpc= column_labels_mpc + ['mpc_'+ f_id[f] +'_'+ types[j]]\n",
    "                k = k+1\n",
    "        column_labels = column_labels_cr + column_labels_mpc\n",
    "        df = pd.DataFrame(np.c_[y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return df #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_id = ['educ_nodegree', 'educ_highschool','educ_higher']\n",
    "#display(uplift_cat_2m_partial_dependency_mpc(rfdicts['DUR_fin_baseline']['treat1_rf'], rfdicts['DUR_fin_baseline']['cont1_rf'], f_id, rfdicts['DUR_fin_baseline']['cont1_X_labels'], rfdicts['DUR_fin_baseline']['treat1_X'], rfdicts['DUR_fin_baseline']['treat1_rbtamt'], X_cont=rfdicts['DUR_fin_baseline']['cont1_X'], X_cont_rbtamt=rfdicts['DUR_fin_baseline']['cont1_rbtamt'],  feature_ids_cont=rfdicts['DUR_fin_baseline']['treat1_X_labels'], types=['mean','percentile','std'],percentile=[25,75]))\n",
    "#f_id = ['educ_highschool','educ_higher','educ_nodegree']\n",
    "#display(uplift_cat_2m_partial_dependency_mpc(rfdicts['TOT_fin']['treat1_rf'], rfdicts['TOT_fin']['cont1_rf'], f_id, rfdicts['TOT_fin']['cont1_X_labels'], rfdicts['TOT_fin']['treat1_X'], rfdicts['TOT_fin']['treat1_rbtamt'], X_cont=rfdicts['TOT_fin']['cont1_X'], X_cont_rbtamt=rfdicts['TOT_fin']['cont1_rbtamt'],  feature_ids_cont=rfdicts['TOT_fin']['treat1_X_labels'], types=['mean','std'])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run partial dependence function for given sample and explanatory variables. this may take a while. Hence, save as later as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['treat1', 'cont1', 'cont2', 'cont3', 'treat1_X', 'treat1_X_labels', 'treat1_rbtamt', 'treat1_rf', 'cont1_X', 'cont1_X_labels', 'cont1_rbtamt', 'cont1_rf', 'cont2_X', 'cont2_X_labels', 'cont2_rbtamt', 'cont2_rf', 'cont3_X', 'cont3_X_labels', 'cont3_rbtamt', 'cont3_rf', 'cont2_treat1_y_pred', 'cont2_treat1_mpc_pred', 'cont1_treat1_y_pred', 'cont1_treat1_mpc_pred', 'cont3_treat1_y_pred', 'cont3_treat1_mpc_pred']\n",
      "['FD_fin', 'FD_finit', 'FD_nofin', 'ND_fin', 'ND_finit', 'ND_nofin', 'SND_fin', 'SND_finit', 'SND_nofin', 'TOT_fin', 'TOT_finit', 'TOT_nofin']\n"
     ]
    }
   ],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "\n",
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "print(list(rfdicts[rfdicts_keys[0]]))\n",
    "print(list(rfdicts))\n",
    "#rfdicts['DUR_fin']['cont1_X_labels']\n",
    "INCOME = ['FINCBTAX','FSALARYM','FINCBTXM']\n",
    "CONTROL = ['adults', 'PERSLT18']\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'orgmrtx_sum', 'qescrowx_sum', 'timeleft']\n",
    "CAT = [['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5'],['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'],['educ_nodegree','educ_highschool','educ_higher']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FD_fin', 'FD_finit', 'FD_nofin', 'ND_fin', 'ND_finit', 'ND_nofin', 'SND_fin', 'SND_finit', 'SND_nofin', 'TOT_fin', 'TOT_finit', 'TOT_nofin']\n",
      "FD_fin:\n",
      "FD_fin treat1 cont2 FINCBTAX\n",
      "FD_fin treat1 cont2 FSALARYM\n",
      "FD_fin treat1 cont2 FINCBTXM\n",
      "FD_fin treat1 cont2 finassets\n",
      "FD_fin treat1 cont2 morgpayment\n",
      "FD_fin treat1 cont2 qblncm1x_sum\n",
      "FD_fin treat1 cont2 orgmrtx_sum\n",
      "FD_fin treat1 cont2 qescrowx_sum\n",
      "FD_fin treat1 cont2 timeleft\n",
      "FD_fin treat1 cont2 age\n",
      "FD_fin treat1 cont2 adults\n",
      "FD_fin treat1 cont2 PERSLT18\n",
      "FD_fin treat1 cont2 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "FD_fin treat1 cont2 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "FD_finit:\n",
      "FD_finit treat1 cont2 FINCBTAX\n",
      "FD_finit treat1 cont2 FSALARYM\n",
      "FD_finit treat1 cont2 FINCBTXM\n",
      "FD_finit treat1 cont2 finassets_it\n",
      "FD_finit treat1 cont2 morgpayment\n",
      "FD_finit treat1 cont2 qblncm1x_sum\n",
      "FD_finit treat1 cont2 orgmrtx_sum\n",
      "FD_finit treat1 cont2 qescrowx_sum\n",
      "FD_finit treat1 cont2 timeleft\n",
      "FD_finit treat1 cont2 age\n",
      "FD_finit treat1 cont2 adults\n",
      "FD_finit treat1 cont2 PERSLT18\n",
      "FD_finit treat1 cont2 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "FD_finit treat1 cont2 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "FD_finit treat1 cont2 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "FD_nofin:\n",
      "FD_nofin treat1 cont2 FINCBTAX\n",
      "FD_nofin treat1 cont2 FSALARYM\n",
      "FD_nofin treat1 cont2 FINCBTXM\n",
      "FD_nofin treat1 cont2 morgpayment\n",
      "FD_nofin treat1 cont2 qblncm1x_sum\n",
      "FD_nofin treat1 cont2 orgmrtx_sum\n",
      "FD_nofin treat1 cont2 qescrowx_sum\n",
      "FD_nofin treat1 cont2 timeleft\n",
      "FD_nofin treat1 cont2 age\n",
      "FD_nofin treat1 cont2 adults\n",
      "FD_nofin treat1 cont2 PERSLT18\n",
      "FD_nofin treat1 cont2 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "FD_nofin treat1 cont2 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "FD_nofin treat1 cont2 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "ND_fin:\n",
      "ND_fin treat1 cont2 FINCBTAX\n",
      "ND_fin treat1 cont2 FSALARYM\n",
      "ND_fin treat1 cont2 FINCBTXM\n",
      "ND_fin treat1 cont2 finassets\n",
      "ND_fin treat1 cont2 morgpayment\n",
      "ND_fin treat1 cont2 qblncm1x_sum\n",
      "ND_fin treat1 cont2 orgmrtx_sum\n",
      "ND_fin treat1 cont2 qescrowx_sum\n",
      "ND_fin treat1 cont2 timeleft\n",
      "ND_fin treat1 cont2 age\n",
      "ND_fin treat1 cont2 adults\n",
      "ND_fin treat1 cont2 PERSLT18\n",
      "ND_fin treat1 cont2 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "ND_fin treat1 cont2 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "ND_finit:\n",
      "ND_finit treat1 cont2 FINCBTAX\n",
      "ND_finit treat1 cont2 FSALARYM\n",
      "ND_finit treat1 cont2 FINCBTXM\n",
      "ND_finit treat1 cont2 finassets_it\n",
      "ND_finit treat1 cont2 morgpayment\n",
      "ND_finit treat1 cont2 qblncm1x_sum\n",
      "ND_finit treat1 cont2 orgmrtx_sum\n",
      "ND_finit treat1 cont2 qescrowx_sum\n",
      "ND_finit treat1 cont2 timeleft\n",
      "ND_finit treat1 cont2 age\n",
      "ND_finit treat1 cont2 adults\n",
      "ND_finit treat1 cont2 PERSLT18\n",
      "ND_finit treat1 cont2 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "ND_finit treat1 cont2 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "ND_finit treat1 cont2 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "ND_nofin:\n",
      "ND_nofin treat1 cont2 FINCBTAX\n",
      "ND_nofin treat1 cont2 FSALARYM\n",
      "ND_nofin treat1 cont2 FINCBTXM\n",
      "ND_nofin treat1 cont2 morgpayment\n",
      "ND_nofin treat1 cont2 qblncm1x_sum\n",
      "ND_nofin treat1 cont2 orgmrtx_sum\n",
      "ND_nofin treat1 cont2 qescrowx_sum\n",
      "ND_nofin treat1 cont2 timeleft\n",
      "ND_nofin treat1 cont2 age\n",
      "ND_nofin treat1 cont2 adults\n",
      "ND_nofin treat1 cont2 PERSLT18\n",
      "ND_nofin treat1 cont2 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "ND_nofin treat1 cont2 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "ND_nofin treat1 cont2 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_fin:\n",
      "SND_fin treat1 cont2 FINCBTAX\n",
      "SND_fin treat1 cont2 FSALARYM\n",
      "SND_fin treat1 cont2 FINCBTXM\n",
      "SND_fin treat1 cont2 finassets\n",
      "SND_fin treat1 cont2 morgpayment\n",
      "SND_fin treat1 cont2 qblncm1x_sum\n",
      "SND_fin treat1 cont2 orgmrtx_sum\n",
      "SND_fin treat1 cont2 qescrowx_sum\n",
      "SND_fin treat1 cont2 timeleft\n",
      "SND_fin treat1 cont2 age\n",
      "SND_fin treat1 cont2 adults\n",
      "SND_fin treat1 cont2 PERSLT18\n",
      "SND_fin treat1 cont2 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_fin treat1 cont2 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_finit:\n",
      "SND_finit treat1 cont2 FINCBTAX\n",
      "SND_finit treat1 cont2 FSALARYM\n",
      "SND_finit treat1 cont2 FINCBTXM\n",
      "SND_finit treat1 cont2 finassets_it\n",
      "SND_finit treat1 cont2 morgpayment\n",
      "SND_finit treat1 cont2 qblncm1x_sum\n",
      "SND_finit treat1 cont2 orgmrtx_sum\n",
      "SND_finit treat1 cont2 qescrowx_sum\n",
      "SND_finit treat1 cont2 timeleft\n",
      "SND_finit treat1 cont2 age\n",
      "SND_finit treat1 cont2 adults\n",
      "SND_finit treat1 cont2 PERSLT18\n",
      "SND_finit treat1 cont2 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_finit treat1 cont2 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_finit treat1 cont2 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_nofin:\n",
      "SND_nofin treat1 cont2 FINCBTAX\n",
      "SND_nofin treat1 cont2 FSALARYM\n",
      "SND_nofin treat1 cont2 FINCBTXM\n",
      "SND_nofin treat1 cont2 morgpayment\n",
      "SND_nofin treat1 cont2 qblncm1x_sum\n",
      "SND_nofin treat1 cont2 orgmrtx_sum\n",
      "SND_nofin treat1 cont2 qescrowx_sum\n",
      "SND_nofin treat1 cont2 timeleft\n",
      "SND_nofin treat1 cont2 age\n",
      "SND_nofin treat1 cont2 adults\n",
      "SND_nofin treat1 cont2 PERSLT18\n",
      "SND_nofin treat1 cont2 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_nofin treat1 cont2 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_nofin treat1 cont2 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_fin:\n",
      "TOT_fin treat1 cont2 FINCBTAX\n",
      "TOT_fin treat1 cont2 FSALARYM\n",
      "TOT_fin treat1 cont2 FINCBTXM\n",
      "TOT_fin treat1 cont2 finassets\n",
      "TOT_fin treat1 cont2 morgpayment\n",
      "TOT_fin treat1 cont2 qblncm1x_sum\n",
      "TOT_fin treat1 cont2 orgmrtx_sum\n",
      "TOT_fin treat1 cont2 qescrowx_sum\n",
      "TOT_fin treat1 cont2 timeleft\n",
      "TOT_fin treat1 cont2 age\n",
      "TOT_fin treat1 cont2 adults\n",
      "TOT_fin treat1 cont2 PERSLT18\n",
      "TOT_fin treat1 cont2 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_fin treat1 cont2 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_finit:\n",
      "TOT_finit treat1 cont2 FINCBTAX\n",
      "TOT_finit treat1 cont2 FSALARYM\n",
      "TOT_finit treat1 cont2 FINCBTXM\n",
      "TOT_finit treat1 cont2 finassets_it\n",
      "TOT_finit treat1 cont2 morgpayment\n",
      "TOT_finit treat1 cont2 qblncm1x_sum\n",
      "TOT_finit treat1 cont2 orgmrtx_sum\n",
      "TOT_finit treat1 cont2 qescrowx_sum\n",
      "TOT_finit treat1 cont2 timeleft\n",
      "TOT_finit treat1 cont2 age\n",
      "TOT_finit treat1 cont2 adults\n",
      "TOT_finit treat1 cont2 PERSLT18\n",
      "TOT_finit treat1 cont2 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_finit treat1 cont2 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_finit treat1 cont2 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_nofin:\n",
      "TOT_nofin treat1 cont2 FINCBTAX\n",
      "TOT_nofin treat1 cont2 FSALARYM\n",
      "TOT_nofin treat1 cont2 FINCBTXM\n",
      "TOT_nofin treat1 cont2 morgpayment\n",
      "TOT_nofin treat1 cont2 qblncm1x_sum\n",
      "TOT_nofin treat1 cont2 orgmrtx_sum\n",
      "TOT_nofin treat1 cont2 qescrowx_sum\n",
      "TOT_nofin treat1 cont2 timeleft\n",
      "TOT_nofin treat1 cont2 age\n",
      "TOT_nofin treat1 cont2 adults\n",
      "TOT_nofin treat1 cont2 PERSLT18\n",
      "TOT_nofin treat1 cont2 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_nofin treat1 cont2 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_nofin treat1 cont2 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#def vimp_plot_uplift()\n",
    "rfdicts_keys = list(rfdicts)\n",
    "#rfdicts_keys = rfdicts_keys[:1]\n",
    "print(rfdicts_keys)\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\pdp\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "    for t in treat:\n",
    "        for c in ['cont2']:\n",
    "            expvars = rfdicts[k][c+'_X_labels']\n",
    "            INCOME = ['FINCBTAX','FSALARYM','FINCBTXM']\n",
    "            if 'finassets' in expvars:\n",
    "                INCOME = INCOME + ['finassets']\n",
    "            if 'finassets_it' in expvars:\n",
    "                INCOME = INCOME + ['finassets_it']\n",
    "            for v in INCOME:\n",
    "                print(k,t,c,v)\n",
    "                if INCOME.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                               feature_ids_treat=rfdicts[k][t+'_X_labels'], feature_ids_cont=rfdicts[k][t+'_X_labels'], \n",
    "                                                               types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                                        feature_ids_treat=rfdicts[k][t+'_X_labels'],feature_ids_cont=rfdicts[k][t+'_X_labels'], \n",
    "                                                                        types=['mean','percentile','std','median'], percentile=[25,75])) \n",
    "            rfdicts[k][f'{c}_{t}_pdp_INCOME'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_INCOME.csv')\n",
    "            for v in MORTGAGE:\n",
    "                print(k,t,c,v)\n",
    "                if MORTGAGE.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],expvars[0],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'], \n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                                        feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75]))    \n",
    "            rfdicts[k][f'{c}_{t}_pdp_MORTGAGE'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_MORTGAGE.csv')\n",
    "            \n",
    "            for v in ['age']:\n",
    "                print(k,t,c,v)\n",
    "                df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],expvars[0],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "            rfdicts[k][f'{c}_{t}_pdp_age'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_age.csv')\n",
    "            \n",
    "            for v in CONTROL:\n",
    "                print(k,t,c,v)\n",
    "                if CONTROL.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],expvars[0],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                                        feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75]))    \n",
    "            rfdicts[k][f'{c}_{t}_pdp_MORTGAGE'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_CONTROL.csv')\n",
    "            \n",
    "            CAT = [['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5'],['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'],['educ_nodegree','educ_highschool','educ_higher']]\n",
    "            i = 1\n",
    "            if 'finassets' in expvars:\n",
    "                CAT = [['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'],['educ_nodegree','educ_highschool','educ_higher']]\n",
    "            for cat in CAT:\n",
    "                print(k,t,c,cat)\n",
    "                df = uplift_cat_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],cat,rfdicts[k][t+'_X_labels'],rfdicts[k][t+'_X'],\n",
    "                                                           rfdicts[k][t+'_rbtamt'],X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                           feature_ids_cont=rfdicts[k][c+'_X_labels'],types=['mean','percentile','std','median'],\n",
    "                                                           percentile=[25,75])\n",
    "                rfdicts[k][f'{c}_{t}_pdp_CAT_{i}'] = df\n",
    "                df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_CAT_{i}.csv')\n",
    "                i = i+1\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot partial dependeny as comparison between the different specifications for a given control group and type of consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>educ_nodegree</th>\n",
       "      <th>educ_highschool</th>\n",
       "      <th>educ_higher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>344.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>344.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.063953</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.377907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.245026</td>\n",
       "      <td>0.497332</td>\n",
       "      <td>0.485570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       educ_nodegree  educ_highschool  educ_higher\n",
       "count     344.000000       344.000000   344.000000\n",
       "mean        0.063953         0.558140     0.377907\n",
       "std         0.245026         0.497332     0.485570\n",
       "min         0.000000         0.000000     0.000000\n",
       "25%         0.000000         0.000000     0.000000\n",
       "50%         0.000000         1.000000     0.000000\n",
       "75%         0.000000         1.000000     1.000000\n",
       "max         1.000000         1.000000     1.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs08_cap.loc[(fs08_cap['treat1']==1) & (fs08_cap['finassets'].notna()), CAT[2]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7** Visualize tree (not done yet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = ['FD_','SND']\n",
    "check = [key for key in pds_keys if key[0:7] == 'pdp_' + cons[0]]\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull out one tree from forest\n",
    "tree = rf.estimators_[5]\n",
    "\n",
    "#export image to a dot file\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = x_list, rounded = True, precision = 1 )\n",
    "(graph, )= pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree_check.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
