{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import PercentFormatter #plot as percentage\n",
    "import seaborn #plot density and histogram at the same time\n",
    "# Set directory where files are downloaded to. Chdir has to be changed in order to run on another computer\n",
    "os.chdir('C:\\\\Users\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE') #change this to the folder where the data set is stored, all the results will be saved in the same folder\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Data** (necessary for part1 and part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'adults', 'PERSLT18', 'FINCBTAX', 'CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5', 'MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n"
     ]
    }
   ],
   "source": [
    "fs08 = pd.read_csv(os.getcwd()+'\\\\fs08.csv').set_index('CustID')\n",
    "\n",
    "#identifier\n",
    "TIME = ['QINTRVMO', 'QINTRVYR', 'rbtmo_1', 'rbtmo_2', 'diff_1', 'diff_2']\n",
    "ID = ['NEWID']\n",
    "\n",
    "#dependent variables\n",
    "CONS = ['FD','SND','ND','DUR','TOT']\n",
    "FUTCONS = ['fut_' + c for c in CONS]\n",
    "LRUNCONS = ['lrun_' + c for c in CONS]\n",
    "\n",
    "for i in range(len(LRUNCONS)):\n",
    "    fs08[LRUNCONS[i]] = fs08[[CONS[i],FUTCONS[i]]].sum(axis=1)\n",
    "\n",
    "#explanatory variables\n",
    "DEMO = ['age', 'adults', 'PERSLT18', 'MARITAL1', 'CUTENURE', 'FINCBTAX'] \n",
    "    #age; number of adults; people below 18; marital status; housing tenure; income in the last 12 months\n",
    "DEMO2 = ['FSALARYM', 'FINCBTXM'] \n",
    "    #FSALARYM: income from salary and wages, CKBKACTX: balance/market value in balance accounts/brookerage accounts;    \n",
    "    #FINCBTXM: Total amount of family income before taxes (Imputed or collected data); (relevant demographics available for the second stimulus only)\n",
    "ASSETS = ['valid_finassets','finassets']\n",
    "    # finassets: sum of 1) SAVACCTX (Total balance/market value (including interest earned) CU had in savings accounts in banks, savings and loans,\n",
    "                         #credit unions, etc., as of the last day of previous month;)\n",
    "                # and    2)CKBKACTX (Total balance or market value (including interest earned) CU had in checking accounts, brokerage accounts, \n",
    "                            #and other similar accounts as of the last day of the previous month\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'orgmrtx_sum', 'qescrowx_sum', 'timeleft']\n",
    "    #morgpayment: morgage payment per month; qblncm1x_sum: sum of principal balances outstanding at the beginning of month M1; orgmrtx_sum: sum of mortgage amounts;\n",
    "    #qescrowx_sum: sum of last regular escrow payments; timeleft: maximum time left on mortgage payment\n",
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #\n",
    "#sample split\n",
    "RBT = ['rbtamt', 'rbtamt_chk', 'rbtamt_e']\n",
    "LAGRBT = ['last_' + var for var in RBT] #lagged variables\n",
    "FUTRBT = ['fut_' + var for var in RBT] #future variables\n",
    "\n",
    "fs08 = fs08[TIME + ID + CONS + DEMO + DEMO2 + ASSETS + MORTGAGE + RBT + ['rbtamt_1','rbtamt_2'] + LAGRBT + FUTRBT + FUTCONS + LRUNCONS + EDUC] #+ CHGCONS + LAGCONS \n",
    "fs08 = pd.get_dummies(fs08, columns=['CUTENURE','MARITAL1']) #change categorical variables to dummy variables\n",
    "\n",
    "DEMO = [s for s in DEMO if s!='CUTENURE' if s!='MARITAL1'] + ['CUTENURE' + f'_{j}' for j in list(range(1,6)) if j!=3] +['MARITAL1' + f'_{j}' for j in list(range(1,5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8287701228524802\n",
      "0.8955169267751125\n",
      "0.7627057548315713\n",
      "0.09380920937886113\n",
      "771\n",
      "4339\n"
     ]
    }
   ],
   "source": [
    "print(fs08['FINCBTAX'].corr(fs08['FINCBTXM']))\n",
    "print(fs08['FSALARYM'].corr(fs08['FINCBTXM']))\n",
    "print(fs08['FSALARYM'].corr(fs08['FINCBTAX']))\n",
    "print(fs08['FINCBTAX'].corr(fs08['morgpayment']))\n",
    "print(fs08.loc[(fs08['rbtamt']>0)&(fs08['valid_finassets']>0),'NEWID'].count())\n",
    "print(fs08.loc[fs08['rbtamt']>0,'NEWID'].count())\n",
    "fs08.loc[fs08['rbtamt_2']>0, ['rbtamt_1','rbtamt_2','diff_2', 'diff_1']]\n",
    "fs08['diff'] = np.nan\n",
    "fs08['diff'] = np.where((fs08['diff_1']>0) & (fs08['diff_2']>0), fs08[['diff_1','diff_2']].mean(axis=1),fs08['diff_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the average rebate amount per individual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08['rbtamt_idmean'] = 0\n",
    "fs08['rbtamt_idmean'] = fs08.groupby('CustID')['rbtamt'].transform('mean')\n",
    "fs08['rbt_count'] = 0\n",
    "fs08['rbt_count'] = fs08.groupby('CustID')['rbtamt'].transform('count')\n",
    "\n",
    "#\n",
    "#sometimes individuals give information of rebate receipt preceding (following) three months of the first (last) interview.\n",
    "#Wherever this is the case, the average rebate should be the weighted mean of rebates received before (after) the relevant time and the actual rebate \n",
    "\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['last_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) #weighted mean,  & (fs08['rbtamt_idmean']>0)\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist() # & (fs08['rbtamt_idmean']>0)\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first') #change for all entries for a given individual\n",
    "\n",
    "\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['fut_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) #weighted mean  & (fs08['rbtamt_idmean']>0)\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0)  & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() #& (fs08['rbtamt_idmean']>0)\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last')  #change for all entries for a given individual\n",
    "#display(fs08.loc[index, ['rbtamt_idmean', 'rbtamt', 'fut_rbtamt', 'last_rbtamt']])\n",
    "\n",
    "#wherever there is no entry for rebates received in the relevant time period but when there were rebates receivde in the past (future) change mean to the value\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() #                                                                               \n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 fs08['fut_rbtamt'], fs08['rbtamt_idmean'])\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last')\n",
    "\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist()\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())), \n",
    "                                fs08['last_rbtamt'], fs08['rbtamt_idmean'])\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first')\n",
    "\n",
    "\n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[(fs08['rbtamt']>0) | (fs08['fut_rbtamt']>0) | (fs08['last_rbtamt']>0) ,'rbt_flag'] = 1\n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('sum')\n",
    "fs08.loc[fs08['rbt_flag']>0, 'rbt_flag'] = 1\n",
    "fs08 = fs08.loc[fs08['rbt_flag']==1]\n",
    "\n",
    "\n",
    "index = fs08.index[fs08['rbtamt_idmean'].isna()].tolist()\n",
    "index = list(set(index))\n",
    "fs08.loc[index, 'rbtamt_idmean'] = fs08.loc[index,'fut_rbtamt']\n",
    "fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'fut_rbtamt' ]\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'last_rbtamt' ]\n",
    "\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations, impute values for financial liquidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(RBT)):\n",
    "    fs08.loc[fs08[RBT[i]]==0, RBT[i]] = np.nan\n",
    "    fs08.loc[fs08[LAGRBT[i]]==0, LAGRBT[i]] = np.nan\n",
    "    fs08.loc[fs08[FUTRBT[i]]==0, FUTRBT[i]] = np.nan\n",
    "    #fs08.loc[(fs08[LAGRBT[i]]==0) | (fs08[LAGRBT[i]].isna()), LAGRBT[i]] =  fs08.loc[(fs08[LAGRBT[i]]==0) | (fs08[LAGRBT[i]].isna())].groupby('CustID')[RBT[i]].shift(-1) \n",
    "    fs08.loc[(fs08[FUTRBT[i]]==0) | (fs08[FUTRBT[i]].isna()), FUTRBT[i]] =  fs08.loc[(fs08[FUTRBT[i]]==0) | (fs08[FUTRBT[i]].isna())].groupby('CustID')[RBT[i]].shift(-1) \n",
    "\n",
    "\n",
    "\n",
    "fs08 = fs08.reset_index()\n",
    "\n",
    "#Iterative imputation for financial liquidity\n",
    "#explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "fs08 = fs08.dropna(subset=CONS+DEMO+DEMO2+MORTGAGE) #Keep only observations that have all info on explanatory variables, dropping missing values on mortgage lowers the sample to half \n",
    "\n",
    "\n",
    "fs08_finit = fs08.copy()\n",
    "fs08_finit = fs08_finit[ ID + CONS + DEMO + DEMO2 + ASSETS + MORTGAGE +  LRUNCONS + EDUC]\n",
    "#fs08_finit = fs08_finit.loc[:,CONS+DEMO+DEMO2+MORTGAGE+['CustID','NEWID','finassets']]\n",
    "labels = list(fs08_finit.columns)\n",
    "imp_mean = IterativeImputer(random_state=0) #use python package iterative imputer\n",
    "imp_mean.fit(fs08_finit[2:])\n",
    "fs08_finit = pd.DataFrame(imp_mean.transform(fs08_finit),columns=labels)\n",
    "fs08_finit = fs08_finit.loc[:,['finassets','NEWID']]\n",
    "fs08_finit = fs08_finit.rename(columns={'finassets':'finassets_it'})\n",
    "\n",
    "fs08 = pd.merge(fs08.sort_values(by = ['NEWID']).reset_index(), fs08_finit.sort_values(by = ['NEWID']).reset_index(), how = 'left', on = 'NEWID', validate = '1:1')\n",
    "fs08 = fs08.drop(columns=['index_x','index_y']) \n",
    "ASSETS = ['valid_finassets','finassets', 'finassets_it' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate treatment and control groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate treatment group:\n",
    "fs08['treat1'] = 0 \n",
    "fs08.loc[fs08['rbtamt'].notna(),'treat1'] = 1 #all entries with actual info on rebate are in the treatment group\n",
    "\n",
    "#three different control groups:\n",
    "#control group 1: those who didn't receive the rebate in the given month\n",
    "fs08['cont1'] = 0\n",
    "fs08.loc[fs08['rbtamt'].isna(), 'cont1'] = 1\n",
    "\n",
    "#control group 2: drop one time period after receipt of rebate, as part of rebate might've been consumed one time period after\n",
    "fs08['cont2'] = 0 \n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[fs08['rbtamt']>0,'rbt_flag'] = 1 #identifier for rebate\n",
    "fs08['rbt_flag_lag'] = fs08.groupby('CustID')['rbt_flag'].shift(1) #identifier for rebate a period before (lag)\n",
    "fs08.loc[fs08['rbt_flag_lag']==1,'rbt_flag']=1 #change rebate identifier so it capture now if a rebate was received this period or the period before\n",
    "fs08.loc[(fs08['rbtamt'].isna()) & (fs08['rbt_flag']==0), 'cont2'] = 1 #those who didn't receive a rebate now or a period before are in the control group\n",
    "\n",
    "#treatment 2: all individuals who received a rebate last time period. This group should be compared to control group 2 only\n",
    "fs08['treat2'] = 0\n",
    "fs08.loc[(fs08['cont2']==0) & (fs08['treat1']==0),'treat2'] = 1\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt'].isna()), 'treat2'] = 1\n",
    "\n",
    "#treatment 3: long run consumption response: all individuals who received a rebate in this period and where the rebate interview is not the last\n",
    "fs08['treat3'] = 0\n",
    "fs08.loc[(fs08['rbtamt']>0) & (fs08[FUTCONS[0]].notna()), 'treat3'] = 1\n",
    "\n",
    "#control 3: long-run consumption: those who haven't received a rebate two periods from current period and have information on consumption for next period as well\n",
    "fs08['cont4'] = 0\n",
    "fs08.loc[(fs08['cont2']==1) & (fs08[FUTCONS[0]].notna()),'cont4'] = 1\n",
    "\n",
    "#control 4: those who haven't received the rebate yet\n",
    "fs08['cont3'] = 0 \n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('cumsum') #starts counting from the point on which the first rebate was received\n",
    "fs08.loc[(fs08['rbtamt'].isna()) & (fs08['rbt_flag']==0),'cont3'] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08_cap = fs08[(np.abs(stats.zscore(fs08.loc[:,CONS+LRUNCONS])) < 3).all(axis=1)] #drop outliers\n",
    "fs08_cap = fs08_cap.loc[fs08_cap['FD']>0] #there are still two observations where food consumption is zero; drop bc of common sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Descriptive statistics** (part 1 and part 2 cn be run seperately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Take a look at the explanatory variables used for random forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole sample (capped):\n",
      "age\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1941.000000\n",
       "mean       46.505152\n",
       "std        12.354085\n",
       "min        21.000000\n",
       "25%        37.000000\n",
       "50%        46.000000\n",
       "75%        55.000000\n",
       "max        84.500000\n",
       "Name: age, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{FINCBTAX} & \\multicolumn{2}{c}{FSALARYM} & \\multicolumn{2}{c}{FINCBTXM} & \\multicolumn{2}{c}{finassets} & \\multicolumn{2}{c}{finassets\\_it} \\\\\n",
      "{} &   treat1 &    cont1 &   treat1 &   cont1 &   treat1 &   cont1 &    treat1 &     cont1 &       treat1 &     cont1 \\\\\n",
      "\\midrule\n",
      "count &    1,941 &    5,446 &    1,941 &   5,446 &    1,941 &   5,446 &       344 &       728 &        1,941 &     5,446 \\\\\n",
      "mean  &   67,099 &   67,390 &   72,060 &  70,776 &   81,479 &  81,045 &    40,343 &    37,520 &       37,775 &    38,583 \\\\\n",
      "std   &   52,023 &   50,843 &   53,364 &  50,394 &   54,095 &  50,731 &   188,758 &   167,181 &       88,764 &    76,242 \\\\\n",
      "min   &  -56,860 & -104,854 &        0 &       0 &  -36,146 & -36,146 &         0 &         0 &      -91,991 &  -154,508 \\\\\n",
      "25\\%   &   32,000 &   32,756 &   38,000 &  37,591 &   46,180 &  46,527 &       686 &       800 &        5,496 &     7,753 \\\\\n",
      "50\\%   &   61,100 &   62,000 &   64,799 &  65,000 &   71,211 &  71,461 &     3,954 &     4,500 &       26,497 &    27,995 \\\\\n",
      "75\\%   &   95,000 &   95,296 &   96,321 &  97,000 &  104,000 & 104,340 &    18,000 &    20,000 &       51,120 &    51,982 \\\\\n",
      "max   &  423,096 &  434,017 &  435,782 & 435,782 &  439,195 & 436,595 & 3,080,141 & 3,999,868 &    3,080,141 & 3,999,868 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Whole sample (capped):')\n",
    "print(DEMO[0])\n",
    "expvars = DEMO+DEMO2+ASSETS+MORTGAGE+EDUC\n",
    "for exp in expvars:\n",
    "    if exp == DEMO[0]:\n",
    "        des = fs08_cap.loc[fs08_cap['treat1']==1,exp].describe()\n",
    "        display(des)\n",
    "        des = pd.concat([des, fs08_cap.loc[fs08['cont1']==1,exp].describe()], axis=1, join='inner')\n",
    "    else:\n",
    "        pass\n",
    "        for g in ['treat1','cont1']:\n",
    "            des = pd.concat([des, fs08_cap.loc[fs08[g]==1,exp].describe()], axis=1, join='inner')\n",
    "#index1 = [DEMO*2+DEMO2*2+ASSETS*2+MORTGAGE*2+EDUC*2\n",
    "index1 = [ i for i in expvars for reps in range(2) ]\n",
    "index2 = ['treat1','cont1']*25\n",
    "tuples = list(zip(index1,index2))\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples) \n",
    "des_cols = list(des)\n",
    "des_cols=[i for i in des_cols if i[0] in ['FINCBTAX','FSALARYM', 'FINCBTXM','finassets','finassets_it']]\n",
    "print(des.to_latex(float_format=\"{:,.0f}\".format, columns=des_cols,multicolumn_format='c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average amount of rebate received per household\n",
      "\\begin{tabular}{llrrrrrrr}\n",
      "\\toprule\n",
      "       &             &  count &  mean &   max &  min &  25\\% &   75\\% &  std \\\\\n",
      "\\midrule\n",
      "cont1 & rbtamt\\_mean &  5,446 & 1,108 & 3,660 &    6 &  600 & 1,500 &  528 \\\\\n",
      "treat1 & rbtamt &  1,941 & 1,095 & 3,660 &    1 &  600 & 1,500 &  527 \\\\\n",
      "       & rbtamt\\_mean &  1,941 & 1,097 & 3,660 &    6 &  600 & 1,500 &  521 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Average amount of rebate received per household')\n",
    "des = fs08_cap.loc[fs08_cap['cont1']==1,'rbtamt_idmean'].describe()\n",
    "des = pd.concat([des, fs08_cap.loc[fs08_cap['treat1']==1,'rbtamt_idmean'].describe()], axis=1, join='inner',names=['cont1','treat1'])\n",
    "des = pd.concat([des, fs08_cap.loc[fs08['rbtamt']>0,'rbtamt'].describe()], axis=1, join='inner',names=['cont1','treat1','treat1'])\n",
    "tuples = [('rbtamt_mean', 'cont1'), ('rbtamt_mean', 'treat1'), ('rbtamt', 'treat1')]\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples)\n",
    "print(des.stack().unstack(level=0).stack(level=0).to_latex(float_format=\"{:,.0f}\".format,  columns=['count','mean','max','min','25%','75%','std']))\n",
    "#print(des1.loc[['count', 'mean', 'std', 'min','50%','max'],:].to_latex(float_format=\"{:,.1f}\".format))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Descriptives for dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary stats for full sample on consumption:\n",
      "                 FD           SND            ND           DUR           TOT\n",
      "count   7901.000000   7901.000000   7901.000000   7901.000000   7901.000000\n",
      "mean    2214.125173   5107.249167   6357.100416   6241.762099  12598.862515\n",
      "std     1309.135172   2681.752964   3277.032339   5821.221010   7599.242540\n",
      "min        0.000000    611.000100  -4420.575000    169.499900   1818.500100\n",
      "25%     1371.000000   3415.000200   4204.999800   3099.000200   7865.991000\n",
      "50%     1975.000200   4620.750200   5761.999900   4756.000200  10843.200100\n",
      "75%     2750.000100   6157.000000   7722.000200   7262.999900  14921.999800\n",
      "max    37678.000300  50375.052100  54477.052200  90264.999900  98775.000000\n",
      "summary stats for capped sample on consumption:\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &      FD &      SND &       ND &      DUR &      TOT \\\\\n",
      "\\midrule\n",
      "count & 7387.00 &  7387.00 &  7387.00 &  7387.00 &  7387.00 \\\\\n",
      "mean  & 2076.57 &  4790.18 &  5969.59 &  5369.43 & 11339.02 \\\\\n",
      "std   &  995.51 &  1962.09 &  2502.13 &  3425.05 &  5002.07 \\\\\n",
      "min   &   52.00 &   611.00 &   876.25 &   169.50 &  1818.50 \\\\\n",
      "25\\%   & 1352.00 &  3355.00 &  4132.50 &  3021.00 &  7688.00 \\\\\n",
      "50\\%   & 1940.00 &  4500.00 &  5599.00 &  4549.00 & 10432.00 \\\\\n",
      "75\\%   & 2638.50 &  5921.62 &  7390.00 &  6792.50 & 14121.00 \\\\\n",
      "max   & 6134.00 & 12905.00 & 15994.13 & 23509.00 & 35115.31 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{FD} & \\multicolumn{2}{c}{SND} & \\multicolumn{2}{c}{ND} & \\multicolumn{2}{c}{TOT} \\\\\n",
      "{} &  treat1 &   cont1 &   treat1 &    cont1 &   treat1 &    cont1 &   treat1 &    cont1 \\\\\n",
      "\\midrule\n",
      "count & 1,941.0 & 5,446.0 &  1,941.0 &  5,446.0 &  1,941.0 &  5,446.0 &  1,941.0 &  5,446.0 \\\\\n",
      "mean  & 2,152.1 & 2,049.7 &  5,058.8 &  4,694.4 &  6,228.4 &  5,877.4 & 11,719.7 & 11,203.3 \\\\\n",
      "std   & 1,042.8 &   976.8 &  2,074.6 &  1,911.5 &  2,644.1 &  2,443.2 &  5,151.0 &  4,941.3 \\\\\n",
      "min   &    78.0 &    52.0 &    611.0 &    676.0 &    876.3 &    940.0 &  2,415.0 &  1,818.5 \\\\\n",
      "25\\%   & 1,400.0 & 1,331.3 &  3,569.0 &  3,296.0 &  4,259.5 &  4,092.0 &  7,845.0 &  7,635.3 \\\\\n",
      "50\\%   & 1,985.0 & 1,919.0 &  4,815.0 &  4,411.0 &  5,877.0 &  5,527.5 & 10,973.0 & 10,244.6 \\\\\n",
      "75\\%   & 2,740.0 & 2,600.0 &  6,267.0 &  5,820.8 &  7,784.5 &  7,259.9 & 14,561.5 & 13,905.4 \\\\\n",
      "max   & 5,986.0 & 6,134.0 & 12,855.0 & 12,905.0 & 15,849.0 & 15,994.1 & 34,784.6 & 35,115.3 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>count</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">cont1</th>\n",
       "      <th>FD</th>\n",
       "      <td>1331.250150</td>\n",
       "      <td>1919.00015</td>\n",
       "      <td>2600.00010</td>\n",
       "      <td>5446.0</td>\n",
       "      <td>6133.9998</td>\n",
       "      <td>2049.654607</td>\n",
       "      <td>51.9999</td>\n",
       "      <td>976.803289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ND</th>\n",
       "      <td>4092.000050</td>\n",
       "      <td>5527.49980</td>\n",
       "      <td>7259.87500</td>\n",
       "      <td>5446.0</td>\n",
       "      <td>15994.1336</td>\n",
       "      <td>5877.364805</td>\n",
       "      <td>940.0001</td>\n",
       "      <td>2443.192098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SND</th>\n",
       "      <td>3296.000150</td>\n",
       "      <td>4411.00010</td>\n",
       "      <td>5820.79380</td>\n",
       "      <td>5446.0</td>\n",
       "      <td>12905.0000</td>\n",
       "      <td>4694.441266</td>\n",
       "      <td>675.9999</td>\n",
       "      <td>1911.486019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOT</th>\n",
       "      <td>7635.250075</td>\n",
       "      <td>10244.62525</td>\n",
       "      <td>13905.44955</td>\n",
       "      <td>5446.0</td>\n",
       "      <td>35115.3056</td>\n",
       "      <td>11203.336221</td>\n",
       "      <td>1818.5001</td>\n",
       "      <td>4941.313979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">treat1</th>\n",
       "      <th>FD</th>\n",
       "      <td>1399.999900</td>\n",
       "      <td>1985.00010</td>\n",
       "      <td>2740.00020</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>5986.0000</td>\n",
       "      <td>2152.072131</td>\n",
       "      <td>78.0000</td>\n",
       "      <td>1042.759107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ND</th>\n",
       "      <td>4259.500100</td>\n",
       "      <td>5877.00000</td>\n",
       "      <td>7784.49970</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>15849.0002</td>\n",
       "      <td>6228.362786</td>\n",
       "      <td>876.2501</td>\n",
       "      <td>2644.063350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SND</th>\n",
       "      <td>3569.000100</td>\n",
       "      <td>4815.00000</td>\n",
       "      <td>6267.00010</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>12854.9998</td>\n",
       "      <td>5058.814060</td>\n",
       "      <td>611.0001</td>\n",
       "      <td>2074.626364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOT</th>\n",
       "      <td>7845.000100</td>\n",
       "      <td>10973.00020</td>\n",
       "      <td>14561.50000</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>34784.6310</td>\n",
       "      <td>11719.734302</td>\n",
       "      <td>2415.0002</td>\n",
       "      <td>5150.980866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    25%          50%          75%   count         max  \\\n",
       "cont1  FD   1331.250150   1919.00015   2600.00010  5446.0   6133.9998   \n",
       "       ND   4092.000050   5527.49980   7259.87500  5446.0  15994.1336   \n",
       "       SND  3296.000150   4411.00010   5820.79380  5446.0  12905.0000   \n",
       "       TOT  7635.250075  10244.62525  13905.44955  5446.0  35115.3056   \n",
       "treat1 FD   1399.999900   1985.00010   2740.00020  1941.0   5986.0000   \n",
       "       ND   4259.500100   5877.00000   7784.49970  1941.0  15849.0002   \n",
       "       SND  3569.000100   4815.00000   6267.00010  1941.0  12854.9998   \n",
       "       TOT  7845.000100  10973.00020  14561.50000  1941.0  34784.6310   \n",
       "\n",
       "                    mean        min          std  \n",
       "cont1  FD    2049.654607    51.9999   976.803289  \n",
       "       ND    5877.364805   940.0001  2443.192098  \n",
       "       SND   4694.441266   675.9999  1911.486019  \n",
       "       TOT  11203.336221  1818.5001  4941.313979  \n",
       "treat1 FD    2152.072131    78.0000  1042.759107  \n",
       "       ND    6228.362786   876.2501  2644.063350  \n",
       "       SND   5058.814060   611.0001  2074.626364  \n",
       "       TOT  11719.734302  2415.0002  5150.980866  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('summary stats for full sample on consumption:')\n",
    "print(fs08[CONS].describe())\n",
    "#print(fs08[CONS].describe().to_latex())\n",
    "print('summary stats for capped sample on consumption:')\n",
    "print(fs08_cap[CONS].describe().to_latex(float_format=\"%.2f\" ))\n",
    "\n",
    "treat = ['treat1','treat2','treat3']\n",
    "cont = ['cont1','cont2','cont3','cont4']\n",
    "#baseline:\n",
    "CONS=['FD','SND','ND','TOT']\n",
    "treat = ['treat1']\n",
    "cont = ['cont1']\n",
    "\n",
    "for i in CONS:\n",
    "    if i=='FD':\n",
    "        des = fs08_cap.loc[fs08_cap['treat1']==1,i].describe()\n",
    "        des = pd.concat([des,fs08_cap.loc[fs08_cap['cont1']==1,i].describe()],join='inner',axis=1)\n",
    "    else:\n",
    "        for g in ['treat1','cont1']:\n",
    "            des = pd.concat([des, fs08_cap.loc[fs08_cap[g]==1,i].describe()],join='inner', axis=1)\n",
    "\n",
    "index1 = ['FD']*2 + ['SND']*2 + ['ND']*2 + ['TOT']*2\n",
    "index2 = ['treat1','cont1']*8\n",
    "tuples = list(zip(index1,index2))\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples)\n",
    "print(des.to_latex(float_format=\"{:,.1f}\".format ,multicolumn_format='c'))\n",
    "des.stack().unstack(level=0).stack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x172.8 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#compare histograms of treatment group with different control groups\n",
    "\n",
    "for i in range(len(CONS)):\n",
    "    for t in treat[0:1]:\n",
    "        for c in cont[0:1]:\n",
    "            plt.figure(figsize=(3.2,2.4))\n",
    "            plt.title(f'{CONS[i]}')\n",
    "            plt.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            #plt.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, density=True, bins=30, color='red')\n",
    "            plt.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            #plt.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= t, density=True, bins=30, color='green')\n",
    "            #plt.set_title(f'{CONS[i]}:')\n",
    "            #plt.tight_layout()\n",
    "            plt.xticks(fontsize = 6)\n",
    "            plt.yticks(fontsize=6)\n",
    "            plt.legend(loc='upper right', frameon=True, fontsize=6)\n",
    "            plt.savefig(os.getcwd() + f'\\\\descriptives\\\\{CONS[i]}_pattern.pdf')\n",
    "            plt.close()\n",
    "CONS=['SND','TOT']\n",
    "fig=plt.figure(figsize=(6.4,2.4))\n",
    "for i in range(len(CONS)):\n",
    "    #plt.figure(figsize=(6.4,2.4))\n",
    "    plot = plt.subplot(1,2,i+1)\n",
    "    for t in treat[0:1]:\n",
    "        for c in cont[0:1]:     \n",
    "            #plot = plt.figure(figsize=(10,3))\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            plot.set_title(f'{CONS[i]}:')\n",
    "            plt.tight_layout()\n",
    "            plt.xticks(fontsize = 8)\n",
    "            plt.yticks(fontsize=8)\n",
    "            plt.legend(loc='upper right', frameon=False, fontsize=8)\n",
    "            #plot_count = plot_count+1\n",
    "            plt.savefig(os.getcwd() + f'\\\\descriptives\\\\SND_TOT_pattern_group1_treat1.pdf')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "CONS=['FD','SND','ND','TOT']        \n",
    "plot_count = 1\n",
    "for i in range(len(CONS)):\n",
    "    #plt.figure(figsize=(6.4,2.4))\n",
    "    for t in treat[0:1]:\n",
    "        fig=plt.figure(figsize=(6.4,2.4))\n",
    "        for c in cont[0:3]:     \n",
    "            #plot = plt.figure(figsize=(10,3))\n",
    "            plot = plt.subplot(1,3,cont.index(c)+1)\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            plot.set_title(f'{CONS[i]}:')\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc='upper right', frameon=False)\n",
    "            plot_count = plot_count+1\n",
    "            #plt.savefig(os.getcwd() + f'\\\\descriptives\\\\{CONS[i]}_pattern_groupcomp.pdf')\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "descriptives for rebate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: Machine learning approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Define sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #'educ_bachelor','educ_master','educ_doctorate'\n",
    "DEMO = ['age', 'adults', 'PERSLT18', 'FINCBTAX', 'CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5', 'MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'] \n",
    "DEMO2 = ['FSALARYM', 'FINCBTXM' ] \n",
    "CONS = ['FD', 'SND', 'ND', 'TOT']\n",
    "CONT = ['cont1', 'cont2', 'cont3']\n",
    "TREAT = 'treat1'\n",
    "treatgroup = TREAT\n",
    "trees = 1000\n",
    "\n",
    "#Random Forest for short term consumption: treatment group 1 with imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC + ['finassets_it']  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_finit_baseline.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_nofin_baseline.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group with just the observations where financial assets are included\n",
    "TREAT = 'treat1'\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + ['finassets'] + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1) & (fs08_cap['valid_finassets']==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[(fs08_cap[con]==1) & (fs08_cap['valid_finassets']==1) , [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_fin_baseline.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Run random forest algorithm seperately for treatment and control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "#pkl_file = open('myfile.pkl', 'rb')\n",
    "#rf2 = pickle.load(pkl_file)\n",
    "#pkl_file.close()\n",
    "#\n",
    "#print(rf)\n",
    "#print(rf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Predict Outcomes for overall consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_predicitons_rbt(rf_treat, rf_cont, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if (type(X_cont) is not np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_treat_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('X_treat_rbamt needs to have an array like structure')\n",
    "            else:\n",
    "                X_temp = X_treat.copy()\n",
    "                rbtamt_temp = X_treat_rbtamt.copy()\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_cont_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('if X_cont is specified, X_cont_rbamt needs to have an array like structure')\n",
    "            if sorted(feature_ids_treat)!=sorted(feature_ids_cont):\n",
    "                raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont')\n",
    "            elif (len(feature_ids_treat)==0) | (len(feature_ids_cont)==0):\n",
    "                raise ValueError(f'if X_treat and X_cont are specified, feature_ids must not be empty')\n",
    "            else:\n",
    "                X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                rbtamt_temp = pd.concat([pd.DataFrame(X_treat_rbtamt), pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                X_labels = list(X.columns)\n",
    "                X_temp = np.array(X)\n",
    "                rbtamt_temp = np.array(rbtamt_temp)\n",
    "        else: \n",
    "            raise ValueError('X_treat does not have an array like structure')\n",
    "        y = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "        mpc = y/rbtamt_temp[:,0]\n",
    "        return y,mpc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUR_fin:\n",
      "cont1_treat1_y_pred (1072,)\n",
      "cont1_treat1_mpc_pred (1072,)\n",
      "cont2_treat1_y_pred (740,)\n",
      "cont2_treat1_mpc_pred (740,)\n",
      "cont3_treat1_y_pred (393,)\n",
      "cont3_treat1_mpc_pred (393,)\n",
      "DUR_finit:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "DUR_finit_baseline:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "DUR_fin_baseline:\n",
      "cont1_treat1_y_pred (1072,)\n",
      "cont1_treat1_mpc_pred (1072,)\n",
      "cont2_treat1_y_pred (740,)\n",
      "cont2_treat1_mpc_pred (740,)\n",
      "cont3_treat1_y_pred (393,)\n",
      "cont3_treat1_mpc_pred (393,)\n",
      "DUR_nofin:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "DUR_nofin_baseline:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "FD_fin:\n",
      "cont1_treat1_y_pred (1072,)\n",
      "cont1_treat1_mpc_pred (1072,)\n",
      "cont2_treat1_y_pred (740,)\n",
      "cont2_treat1_mpc_pred (740,)\n",
      "cont3_treat1_y_pred (393,)\n",
      "cont3_treat1_mpc_pred (393,)\n",
      "FD_finit:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "FD_finit_baseline:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "FD_fin_baseline:\n",
      "cont1_treat1_y_pred (1072,)\n",
      "cont1_treat1_mpc_pred (1072,)\n",
      "cont2_treat1_y_pred (740,)\n",
      "cont2_treat1_mpc_pred (740,)\n",
      "cont3_treat1_y_pred (393,)\n",
      "cont3_treat1_mpc_pred (393,)\n",
      "FD_nofin:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "FD_nofin_baseline:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "ND_fin:\n",
      "cont1_treat1_y_pred (1072,)\n",
      "cont1_treat1_mpc_pred (1072,)\n",
      "cont2_treat1_y_pred (740,)\n",
      "cont2_treat1_mpc_pred (740,)\n",
      "cont3_treat1_y_pred (393,)\n",
      "cont3_treat1_mpc_pred (393,)\n",
      "ND_finit:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "ND_finit_baseline:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "ND_fin_baseline:\n",
      "cont1_treat1_y_pred (1072,)\n",
      "cont1_treat1_mpc_pred (1072,)\n",
      "cont2_treat1_y_pred (740,)\n",
      "cont2_treat1_mpc_pred (740,)\n",
      "cont3_treat1_y_pred (393,)\n",
      "cont3_treat1_mpc_pred (393,)\n",
      "ND_nofin:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "ND_nofin_baseline:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "SND_fin:\n",
      "cont1_treat1_y_pred (1072,)\n",
      "cont1_treat1_mpc_pred (1072,)\n",
      "cont2_treat1_y_pred (740,)\n",
      "cont2_treat1_mpc_pred (740,)\n",
      "cont3_treat1_y_pred (393,)\n",
      "cont3_treat1_mpc_pred (393,)\n",
      "SND_finit:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "SND_finit_baseline:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "SND_fin_baseline:\n",
      "cont1_treat1_y_pred (1072,)\n",
      "cont1_treat1_mpc_pred (1072,)\n",
      "cont2_treat1_y_pred (740,)\n",
      "cont2_treat1_mpc_pred (740,)\n",
      "cont3_treat1_y_pred (393,)\n",
      "cont3_treat1_mpc_pred (393,)\n",
      "SND_nofin:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n",
      "SND_nofin_baseline:\n",
      "cont1_treat1_y_pred (7387,)\n",
      "cont1_treat1_mpc_pred (7387,)\n",
      "cont2_treat1_y_pred (6053,)\n",
      "cont2_treat1_mpc_pred (6053,)\n",
      "cont3_treat1_y_pred (5256,)\n",
      "cont3_treat1_mpc_pred (5256,)\n"
     ]
    }
   ],
   "source": [
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\condistr\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for t in treat:\n",
    "        for c in cont:\n",
    "            y,mpc = uplift_predicitons_rbt(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                          rfdicts[k][c+'_X'],rfdicts[k][c+'_rbtamt'],rfdicts[k][t+'_X_labels'],rfdicts[k][c+'_X_labels'])\n",
    "            rfdicts[k][f'{c}_{t}_y_pred'] = y\n",
    "            print(f'{c}_{t}_y_pred', y.shape)\n",
    "            rfdicts[k][f'{c}_{t}_mpc_pred'] = mpc\n",
    "            print(f'{c}_{t}_mpc_pred', mpc.shape)\n",
    "            plt.hist(y, bins=40,  edgecolor='black')\n",
    "            lower = round((min(y)/100),1)*100\n",
    "            upper = round((max(y)/100),1)*100+1\n",
    "            plt.xticks(np.arange(lower, upper, 1000))\n",
    "            plt.title(f'Pred cons resp distr., {vartype},{c},{t}')\n",
    "            plt.xlabel(f'consumption in {cons}, number of observations {y.shape[0]}')\n",
    "            plt.ylabel(f'number of individuals in bin')\n",
    "            plt.savefig(newpath + f'\\\\{vartype}_{c}_{t}_y_pred.pdf')\n",
    "            plt.close()         \n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** Plot distribution of consumption response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5** Variable importance plot for treatment and control group separately and as a weighted sum for the whole sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DUR_fin', 'DUR_finit', 'DUR_finit_baseline', 'DUR_fin_baseline', 'DUR_nofin', 'DUR_nofin_baseline', 'FD_fin', 'FD_finit', 'FD_finit_baseline', 'FD_fin_baseline', 'FD_nofin', 'FD_nofin_baseline', 'ND_fin', 'ND_finit', 'ND_finit_baseline', 'ND_fin_baseline', 'ND_nofin', 'ND_nofin_baseline', 'SND_fin', 'SND_finit', 'SND_finit_baseline', 'SND_fin_baseline', 'SND_nofin', 'SND_nofin_baseline']\n",
      "['treat1', 'cont1', 'cont2', 'cont3', 'treat1_X', 'treat1_X_labels', 'treat1_rbtamt', 'treat1_rf', 'cont1_X', 'cont1_X_labels', 'cont1_rbtamt', 'cont1_rf', 'cont2_X', 'cont2_X_labels', 'cont2_rbtamt', 'cont2_rf', 'cont3_X', 'cont3_X_labels', 'cont3_rbtamt', 'cont3_rf', 'cont1_treat1_y_pred', 'cont1_treat1_mpc_pred', 'cont3_treat1_y_pred', 'cont3_treat1_mpc_pred', 'cont2_treat1_y_pred', 'cont2_treat1_mpc_pred']\n"
     ]
    }
   ],
   "source": [
    "rfdicts_keys = list(rfdicts)\n",
    "print(rfdicts_keys)\n",
    "print(list(rfdicts[rfdicts_keys[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "DUR_fin\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "DUR_finit\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "DUR_finit_baseline\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "DUR_fin_baseline\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "DUR_nofin\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "DUR_nofin_baseline\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "FD_fin\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "FD_finit\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "FD_finit_baseline\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "FD_fin_baseline\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "FD_nofin\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "FD_nofin_baseline\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "ND_fin\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "ND_finit\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "ND_finit_baseline\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "ND_fin_baseline\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "ND_nofin\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "ND_nofin_baseline\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "SND_fin\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "SND_finit\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "SND_finit_baseline\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "SND_fin_baseline\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "SND_nofin\n",
      "['treat1', 'cont1', 'cont2', 'cont3']\n",
      "SND_nofin_baseline\n"
     ]
    }
   ],
   "source": [
    "#def vimp_plot_uplift()\n",
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "for k in rfdicts_keys:\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\varimp\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    print(treat+cont)\n",
    "    uplift_imp = dict()\n",
    "    for i in treat+cont:\n",
    "        importances = (rfdicts[k][i+'_rf'].feature_importances_)\n",
    "        X_importances = [(label, importance) for label, importance in zip(rfdicts[k][i+'_X_labels'],importances)]\n",
    "        #X_importances = [(round(importance,2), label) for importance, label in zip(importances, rf[i+'_X_labels'])]\n",
    "        X_importances = sorted(X_importances, key = lambda x:x[1], reverse = False)\n",
    "        uplift_imp[i+'_varimp_values'] = [x[1] for x in X_importances]\n",
    "        uplift_imp[i+'_varimp_labels'] = [x[0] for x in X_importances]\n",
    "        uplift_imp[i+'_varimp_tuples'] = X_importances\n",
    "        \n",
    "    \n",
    "    for i in treat + cont:\n",
    "        X_importances = sorted(X_importances, key = lambda x:x[0].upper(), reverse = False) #sort in alphabetical order\n",
    "        uplift_imp[i+'_values'] = [x[1] for x in X_importances] #importances \n",
    "        uplift_imp[i+'_labels'] = [x[0] for x in X_importances] \n",
    "        shape = rfdicts[k][i+'_X'].shape\n",
    "        uplift_imp[i+'_sample'] = shape[0] \n",
    "    \n",
    "    for t in treat:\n",
    "        plotgroups = [t]\n",
    "        for c in cont:\n",
    "            plotgroups = plotgroups + [c] + [c+'_'+t]\n",
    "            uplift_imp[f'{c}_{t}_sample'] = uplift_imp[f'{t}_sample'] + uplift_imp[f'{c}_sample'] \n",
    "            uplift_imp[f'{c}_{t}_varimp_values'] = [uplift_imp[f'{t}_sample']/(uplift_imp[f'{c}_{t}_sample'])*uplift_imp[f'{t}_values'][i] + \n",
    "            uplift_imp[f'{c}_sample']/(uplift_imp[f'{c}_{t}_sample'])*uplift_imp[f'{c}_values'][i] for i in range(len(uplift_imp[f'{t}_values']))]\n",
    "\n",
    "            up_importances = [(label, importance) for label, importance in zip(uplift_imp[f'{t}_labels'],uplift_imp[f'{c}_{t}_varimp_values'])]\n",
    "            up_importances = sorted(up_importances, key = lambda x:x[1], reverse = False)\n",
    "            uplift_imp[f'{c}_{t}_varimp_tuples'] = up_importances            \n",
    "            uplift_imp[f'{c}_{t}_varimp_values'] = [up[1] for up in up_importances]\n",
    "            uplift_imp[f'{c}_{t}_varimp_labels'] = [up[0] for up in up_importances]\n",
    "        print(k)   \n",
    "    for g in plotgroups:\n",
    "        freq_series = pd.Series(uplift_imp[g+'_varimp_values'])\n",
    "        y_labels = uplift_imp[g+'_varimp_labels']\n",
    "\n",
    "        # Plot the figure.\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        ax = freq_series.plot(kind='barh')\n",
    "        ssize=uplift_imp[g+'_sample']\n",
    "        ax.set_title(f'Variable Importance Plot for {k},{g.upper()} Sample')\n",
    "        ax.set_xlabel(f'Frequency, sample size = {str(ssize)}')\n",
    "        ax.set_ylabel(f'Variable')\n",
    "        ax.set_yticklabels(y_labels)\n",
    "        #ax.set_xlim(-40, 300) # expand xlim to make labels easier to read\n",
    "\n",
    "        rects = ax.patches\n",
    "\n",
    "        # For each bar: Place a label\n",
    "        for rect in rects:\n",
    "            # Get X and Y placement of label from rect.\n",
    "            x_value = rect.get_width()\n",
    "            y_value = rect.get_y() + rect.get_height() / 2\n",
    "\n",
    "            # Number of points between bar and label. Change to your liking.\n",
    "            space = 3\n",
    "            # Vertical alignment for positive values\n",
    "            ha = 'left'\n",
    "\n",
    "            # Use X value as label and format number with one decimal place\n",
    "            label = \"{:.3f}\".format(x_value)\n",
    "\n",
    "            # Create annotation\n",
    "            plt.annotate(\n",
    "                label,                      # Use `label` as label\n",
    "                (x_value, y_value),         # Place label at end of the bar\n",
    "                xytext=(space, 0),          # Horizontally shift label by `space`\n",
    "                textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "                va='center',                # Vertically center label\n",
    "                ha=ha)                      # Horizontally align label differently for\n",
    "                                            # positive and negative values.\n",
    "        plt.savefig(f'{newpath}\\\\{vartype}_{g}.pdf')\n",
    "        plt.close()\n",
    "        #plt.savefig(\"image.png\")\n",
    "        #plt.savefig(newpath + '\\\\'+ pathend +f'_{i}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Partial dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6.1** Function for simple partial dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_dependency(rf, X, f_id, feature_ids = []): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the dependency (or partial dependency) of a response variable on a predictor (or multiple predictors)\n",
    "    1. Sample a grid of values of a predictor.\n",
    "    2. For each value, replace every row of that predictor with this value, calculate the average prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    X_temp = X.copy()\n",
    "    \n",
    "    if type(f_id) is int:\n",
    "        if f_id > (X_temp.shape[1]-1):\n",
    "            raise ValueError(f'positional number of {f_id} exceeds array shape')\n",
    "        else:\n",
    "            column = f_id+1\n",
    "    elif type(f_id) is str:\n",
    "        if f_id not in feature_ids:\n",
    "            raise ValueError(f\"explanatory variable {f_id} is not in data frame or feature_ids is not passed to the function\")\n",
    "        else:\n",
    "            f_id = feature_ids.index(f_id)\n",
    "    else:\n",
    "        raise ValueError('f_id needs to be either an integer or a string')\n",
    "        #return\n",
    "    \n",
    "    grid = np.linspace(np.percentile(X_temp[:, f_id], 0.1),\n",
    "                       np.percentile(X_temp[:, f_id], 99.5),\n",
    "                       100)\n",
    "    y_pred = np.zeros(len(grid))\n",
    "    \n",
    "    for i, val in enumerate(grid): # i returns the counter, val returns the value at position of counter on grid\n",
    "        X_temp[:, f_id] = val\n",
    "        #data = xgb.DMatrix( X_temp[:, feature_ids].reshape( (len(X_temp), len(feature_ids)) ) )\n",
    "        y_pred[i] = np.average(rf.predict(X_temp)) #any function other than mean is also possible\n",
    "    return grid, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6.2** Function for uplift 2model partial dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_2m_partial_dependency(rf_treat, rf_cont, f_id, X_treat, X_cont=[], feature_ids_treat=[], feature_ids_cont=[], types=['mean'], percentile='none', grid_lower=5, grid_upper=95 ): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor (or multiple predictors) in a random forest uplift 2 model approach.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    y_pred: corresponding predicted values of dependent variable (type: ndarray). \n",
    "            If input types is a list the columns in the array correspond to the chosen types in the same order\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is int):\n",
    "                if f_id > (X_temp.shape[1]-1):\n",
    "                    raise ValueError(f'positional number of {f_id} exceeds array shape')\n",
    "                else:\n",
    "                    X_temp = X_treat.copy()\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str):\n",
    "                if f_id not in feature_ids:\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids is not passed to the function')\n",
    "                else:\n",
    "                    f_id = feature_ids.index(f_id)\n",
    "                    f_id_label = f_id\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('f_id needs to be either an integer or a string')\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(f_id) is int):\n",
    "                raise ValueError(f'if X_cont is specified, then f_id needs to be a string variable')\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str) & ((f_id not in feature_ids_treat) |  (f_id not in feature_ids_cont)):\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids_treat or feature_ids_cont is not passed to the function')\n",
    "            else:\n",
    "                if sorted(feature_ids_treat)!=sorted(feature_ids_cont):\n",
    "                    raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont')\n",
    "                else:\n",
    "                    X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                    X_labels = list(X.columns)\n",
    "                    X_temp = np.array(X)\n",
    "                    f_id_label = f_id\n",
    "                    f_id = X_labels.index(f_id)             \n",
    "        else: \n",
    "            raise ValueError('Either X_cont or X_treat does not have an array like structure')\n",
    "\n",
    "        grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower),\n",
    "                           np.percentile(X_temp[:, f_id], grid_upper),\n",
    "                           100)\n",
    "        \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types\n",
    "        \n",
    "        if set(types) <= set(function_labels):\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i]\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif not 0<=percentile<=100:\n",
    "                    raise ValueError('percentile out of range')\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "\n",
    "    \n",
    "        y_pred = np.zeros((len(grid), len(types)))\n",
    "\n",
    "        for i, val in enumerate(grid): # i returns the counter, val returns the value at position of counter on grid\n",
    "            X_temp[:, f_id] = val\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[i,j] = nptypes[j](y_temp,percentile)\n",
    "                else:\n",
    "                    y_pred[i,j] = nptypes[j](y_temp)\n",
    "\n",
    "        for j in range(len(column_labels)):\n",
    "            if column_labels[j] == 'percentile':\n",
    "                column_labels[j] = 'percentile_' + str(percentile)\n",
    "            else:\n",
    "                pass       \n",
    "        column_labels = ['grid']+column_labels\n",
    "        column_labels = [str(f_id_label)+ '_' + lab for lab in column_labels]\n",
    "\n",
    "        pd = pd.DataFrame(np.c_[grid, y_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return pd #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uplift_num_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[], types=['mean'], percentile='none', grid_lower=5, grid_upper=95 ):\n",
    "#pd = uplift_num_2m_partial_dependency_mpc    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_num_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[], types=['mean'], percentile='none', grid_lower=5, grid_upper=95 ): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor (or multiple predictors) in a random forest uplift 2 model approach.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    y_pred: corresponding predicted values of dependent variable (type: ndarray). \n",
    "            If input types is a list the columns in the array correspond to the chosen types in the same order\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is int):\n",
    "                if f_id > (X_temp.shape[1]-1):\n",
    "                    raise ValueError(f'positional number of {f_id} exceeds array shape')\n",
    "                else:\n",
    "                    X_temp = X_treat.copy()\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str):\n",
    "                if f_id not in feature_ids:\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids is not passed to the function')\n",
    "                else:\n",
    "                    f_id = feature_ids.index(f_id)\n",
    "                    f_id_label = f_id\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('f_id needs to be either an integer or a string')\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(f_id) is int):\n",
    "                raise ValueError(f'if X_cont is specified, then f_id needs to be a string variable')\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str) & ((f_id not in feature_ids_treat) |  (f_id not in feature_ids_cont)):\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids_treat or feature_ids_cont is not passed to the function')\n",
    "            else:\n",
    "                if (sorted(feature_ids_treat)!=sorted(feature_ids_cont)) & (type(X_cont_rbtamt) is not np.ndarray) :\n",
    "                    raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                else:\n",
    "                    X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = np.array(mean_rbt)\n",
    "                    X_labels = list(X.columns)\n",
    "                    X_temp = np.array(X)\n",
    "                    f_id_label = f_id\n",
    "                    f_id = X_labels.index(f_id)             \n",
    "        else: \n",
    "            raise ValueError('Either X_cont or X_treat does not have an array like structure')\n",
    "\n",
    "                       #['age', 'adults', 'PERSLT18'\n",
    "        X_unique = np.array(list(set(X_temp[:, f_id])))\n",
    "        if len(X_unique)*3 > 100:\n",
    "            grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower), np.percentile(X_temp[:, f_id], grid_upper), 100)\n",
    "        else:\n",
    "            grid = np.linspace(np.percentile(X_temp[:, f_id],grid_lower),np.percentile(X_temp[:, f_id],grid_upper), len(X_unique)*3)\n",
    "        #grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower),\n",
    "        #                   np.percentile(X_temp[:, f_id], grid_upper),\n",
    "        #                   100)\n",
    "        \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types.copy()\n",
    "        #column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "\n",
    "        if set(types) <= set(function_labels):\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i]\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif type(percentile) is not list: # 0<=percentile<=100:\n",
    "                    raise ValueError('percentile must be list')\n",
    "                else:\n",
    "                    column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "                    if len(percentile)>1:\n",
    "                        for p in percentile[1:]:\n",
    "                            types = types + ['percentile'] #pass\n",
    "                            nptypes = nptypes + [np.percentile]\n",
    "                            column_labels = column_labels + ['percentile_' +str(p)]\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "        print(types)\n",
    "        y_pred = np.zeros((len(grid),len(types)))\n",
    "        mpc_pred = np.zeros((len(grid), len(types)))\n",
    "        p_pos = 0\n",
    "        for i, val in enumerate(grid): # i returns the counter, val returns the value at position of counter on grid \n",
    "            X_temp[:, f_id] = val\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            p_pos = 0\n",
    "            for j in range(len(types)):\n",
    "                print(types[j])\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[i,j] = nptypes[j](y_temp,percentile[p_pos])\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp,percentile[p_pos])\n",
    "                    p_pos = p_pos + 1\n",
    "                else:\n",
    "                    y_pred[i,j] = nptypes[j](y_temp)\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp)\n",
    "        for j in range(len(column_labels)):\n",
    "            if column_labels[j] == 'percentile':\n",
    "                column_labels[j] = 'percentile_' + str(percentile)\n",
    "            else:\n",
    "                pass \n",
    "        column_labels_cr = ['cr_'+ lab for lab in column_labels]\n",
    "        column_labels_mpc = ['mpc_' + lab for lab in column_labels]\n",
    "        column_labels = ['grid']+column_labels_cr+column_labels_mpc\n",
    "        column_labels = [str(f_id_label)+ '_' + lab for lab in column_labels]\n",
    "\n",
    "        pd = pd.DataFrame(np.c_[grid, y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return pd #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "\n",
    "rfdicts_keys = list(rfdicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean', 'percentile', 'percentile']\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n",
      "mean\n",
      "percentile\n",
      "percentile\n"
     ]
    }
   ],
   "source": [
    "#list(rfdicts[rfdicts_keys[2]])\n",
    "pd = uplift_num_2m_partial_dependency_mpc(rfdicts[rfdicts_keys[2]]['treat1_rf'], rfdicts[rfdicts_keys[2]]['cont1_rf'], 'PERSLT18', rfdicts[rfdicts_keys[2]]['treat1_X'], rfdicts[rfdicts_keys[2]]['treat1_rbtamt'], X_cont=rfdicts[rfdicts_keys[2]]['treat1_X'], X_cont_rbtamt=rfdicts[rfdicts_keys[2]]['cont1_rbtamt'], feature_ids_treat=rfdicts[rfdicts_keys[2]]['treat1_X_labels'], feature_ids_cont=rfdicts[rfdicts_keys[2]]['cont1_X_labels'], types=['mean','percentile'], percentile=[25,75], grid_lower=5, grid_upper=95 ) #: #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERSLT18_grid</th>\n",
       "      <th>PERSLT18_cr_mean</th>\n",
       "      <th>PERSLT18_cr_percentile_25</th>\n",
       "      <th>PERSLT18_cr_percentile_75</th>\n",
       "      <th>PERSLT18_mpc_mean</th>\n",
       "      <th>PERSLT18_mpc_percentile_25</th>\n",
       "      <th>PERSLT18_mpc_percentile_75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>144.622835</td>\n",
       "      <td>-317.946892</td>\n",
       "      <td>549.999567</td>\n",
       "      <td>0.208771</td>\n",
       "      <td>-0.306852</td>\n",
       "      <td>0.546283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.15</td>\n",
       "      <td>144.622835</td>\n",
       "      <td>-317.946892</td>\n",
       "      <td>549.999567</td>\n",
       "      <td>0.208771</td>\n",
       "      <td>-0.306852</td>\n",
       "      <td>0.546283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.30</td>\n",
       "      <td>144.622835</td>\n",
       "      <td>-317.946892</td>\n",
       "      <td>549.999567</td>\n",
       "      <td>0.208771</td>\n",
       "      <td>-0.306852</td>\n",
       "      <td>0.546283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.45</td>\n",
       "      <td>144.622835</td>\n",
       "      <td>-317.946892</td>\n",
       "      <td>549.999567</td>\n",
       "      <td>0.208771</td>\n",
       "      <td>-0.306852</td>\n",
       "      <td>0.546283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.60</td>\n",
       "      <td>154.545227</td>\n",
       "      <td>-315.836949</td>\n",
       "      <td>560.296304</td>\n",
       "      <td>0.223094</td>\n",
       "      <td>-0.299977</td>\n",
       "      <td>0.559251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.75</td>\n",
       "      <td>154.545227</td>\n",
       "      <td>-315.836949</td>\n",
       "      <td>560.296304</td>\n",
       "      <td>0.223094</td>\n",
       "      <td>-0.299977</td>\n",
       "      <td>0.559251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.90</td>\n",
       "      <td>154.545227</td>\n",
       "      <td>-315.836949</td>\n",
       "      <td>560.296304</td>\n",
       "      <td>0.223094</td>\n",
       "      <td>-0.299977</td>\n",
       "      <td>0.559251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.05</td>\n",
       "      <td>154.768080</td>\n",
       "      <td>-315.426262</td>\n",
       "      <td>560.296304</td>\n",
       "      <td>0.223416</td>\n",
       "      <td>-0.299826</td>\n",
       "      <td>0.559496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.20</td>\n",
       "      <td>154.768080</td>\n",
       "      <td>-315.426262</td>\n",
       "      <td>560.296304</td>\n",
       "      <td>0.223416</td>\n",
       "      <td>-0.299826</td>\n",
       "      <td>0.559496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.35</td>\n",
       "      <td>154.768080</td>\n",
       "      <td>-315.426262</td>\n",
       "      <td>560.296304</td>\n",
       "      <td>0.223416</td>\n",
       "      <td>-0.299826</td>\n",
       "      <td>0.559496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.50</td>\n",
       "      <td>154.768080</td>\n",
       "      <td>-315.426262</td>\n",
       "      <td>560.296304</td>\n",
       "      <td>0.223416</td>\n",
       "      <td>-0.299826</td>\n",
       "      <td>0.559496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.65</td>\n",
       "      <td>133.361537</td>\n",
       "      <td>-329.598644</td>\n",
       "      <td>535.170206</td>\n",
       "      <td>0.192515</td>\n",
       "      <td>-0.313065</td>\n",
       "      <td>0.532220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.80</td>\n",
       "      <td>133.361537</td>\n",
       "      <td>-329.598644</td>\n",
       "      <td>535.170206</td>\n",
       "      <td>0.192515</td>\n",
       "      <td>-0.313065</td>\n",
       "      <td>0.532220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.95</td>\n",
       "      <td>133.361537</td>\n",
       "      <td>-329.598644</td>\n",
       "      <td>535.170206</td>\n",
       "      <td>0.192515</td>\n",
       "      <td>-0.313065</td>\n",
       "      <td>0.532220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.10</td>\n",
       "      <td>133.384663</td>\n",
       "      <td>-329.598644</td>\n",
       "      <td>535.170206</td>\n",
       "      <td>0.192548</td>\n",
       "      <td>-0.313360</td>\n",
       "      <td>0.532220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.25</td>\n",
       "      <td>133.384663</td>\n",
       "      <td>-329.598644</td>\n",
       "      <td>535.170206</td>\n",
       "      <td>0.192548</td>\n",
       "      <td>-0.313360</td>\n",
       "      <td>0.532220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.40</td>\n",
       "      <td>133.384663</td>\n",
       "      <td>-329.598644</td>\n",
       "      <td>535.170206</td>\n",
       "      <td>0.192548</td>\n",
       "      <td>-0.313360</td>\n",
       "      <td>0.532220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.55</td>\n",
       "      <td>104.915629</td>\n",
       "      <td>-355.825358</td>\n",
       "      <td>503.481133</td>\n",
       "      <td>0.151451</td>\n",
       "      <td>-0.337412</td>\n",
       "      <td>0.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.70</td>\n",
       "      <td>104.915629</td>\n",
       "      <td>-355.825358</td>\n",
       "      <td>503.481133</td>\n",
       "      <td>0.151451</td>\n",
       "      <td>-0.337412</td>\n",
       "      <td>0.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.85</td>\n",
       "      <td>104.915629</td>\n",
       "      <td>-355.825358</td>\n",
       "      <td>503.481133</td>\n",
       "      <td>0.151451</td>\n",
       "      <td>-0.337412</td>\n",
       "      <td>0.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.00</td>\n",
       "      <td>104.915629</td>\n",
       "      <td>-355.825358</td>\n",
       "      <td>503.481133</td>\n",
       "      <td>0.151451</td>\n",
       "      <td>-0.337412</td>\n",
       "      <td>0.495542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PERSLT18_grid  PERSLT18_cr_mean  PERSLT18_cr_percentile_25  \\\n",
       "0            0.00        144.622835                -317.946892   \n",
       "1            0.15        144.622835                -317.946892   \n",
       "2            0.30        144.622835                -317.946892   \n",
       "3            0.45        144.622835                -317.946892   \n",
       "4            0.60        154.545227                -315.836949   \n",
       "5            0.75        154.545227                -315.836949   \n",
       "6            0.90        154.545227                -315.836949   \n",
       "7            1.05        154.768080                -315.426262   \n",
       "8            1.20        154.768080                -315.426262   \n",
       "9            1.35        154.768080                -315.426262   \n",
       "10           1.50        154.768080                -315.426262   \n",
       "11           1.65        133.361537                -329.598644   \n",
       "12           1.80        133.361537                -329.598644   \n",
       "13           1.95        133.361537                -329.598644   \n",
       "14           2.10        133.384663                -329.598644   \n",
       "15           2.25        133.384663                -329.598644   \n",
       "16           2.40        133.384663                -329.598644   \n",
       "17           2.55        104.915629                -355.825358   \n",
       "18           2.70        104.915629                -355.825358   \n",
       "19           2.85        104.915629                -355.825358   \n",
       "20           3.00        104.915629                -355.825358   \n",
       "\n",
       "    PERSLT18_cr_percentile_75  PERSLT18_mpc_mean  PERSLT18_mpc_percentile_25  \\\n",
       "0                  549.999567           0.208771                   -0.306852   \n",
       "1                  549.999567           0.208771                   -0.306852   \n",
       "2                  549.999567           0.208771                   -0.306852   \n",
       "3                  549.999567           0.208771                   -0.306852   \n",
       "4                  560.296304           0.223094                   -0.299977   \n",
       "5                  560.296304           0.223094                   -0.299977   \n",
       "6                  560.296304           0.223094                   -0.299977   \n",
       "7                  560.296304           0.223416                   -0.299826   \n",
       "8                  560.296304           0.223416                   -0.299826   \n",
       "9                  560.296304           0.223416                   -0.299826   \n",
       "10                 560.296304           0.223416                   -0.299826   \n",
       "11                 535.170206           0.192515                   -0.313065   \n",
       "12                 535.170206           0.192515                   -0.313065   \n",
       "13                 535.170206           0.192515                   -0.313065   \n",
       "14                 535.170206           0.192548                   -0.313360   \n",
       "15                 535.170206           0.192548                   -0.313360   \n",
       "16                 535.170206           0.192548                   -0.313360   \n",
       "17                 503.481133           0.151451                   -0.337412   \n",
       "18                 503.481133           0.151451                   -0.337412   \n",
       "19                 503.481133           0.151451                   -0.337412   \n",
       "20                 503.481133           0.151451                   -0.337412   \n",
       "\n",
       "    PERSLT18_mpc_percentile_75  \n",
       "0                     0.546283  \n",
       "1                     0.546283  \n",
       "2                     0.546283  \n",
       "3                     0.546283  \n",
       "4                     0.559251  \n",
       "5                     0.559251  \n",
       "6                     0.559251  \n",
       "7                     0.559496  \n",
       "8                     0.559496  \n",
       "9                     0.559496  \n",
       "10                    0.559496  \n",
       "11                    0.532220  \n",
       "12                    0.532220  \n",
       "13                    0.532220  \n",
       "14                    0.532220  \n",
       "15                    0.532220  \n",
       "16                    0.532220  \n",
       "17                    0.495542  \n",
       "18                    0.495542  \n",
       "19                    0.495542  \n",
       "20                    0.495542  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentile = [25,75]\n",
    "p_pos = 0 \n",
    "percentile[p_pos]\n",
    "p_pos = p_pos + 1\n",
    "percentile[p_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_cat_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, feature_ids_treat, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[],  feature_ids_cont=[], types=['mean'], percentile='none'): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor (or multiple predictors) in a random forest uplift 2 model approach.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    y_pred: corresponding predicted values of dependent variable (type: ndarray). \n",
    "            If input types is a list the columns in the array correspond to the chosen types in the same order\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is list):\n",
    "                if (len(f_id)<2):\n",
    "                    raise ValueError(f'{f_id} must be a list of hot-encoding features a hence neither empty nor have a length of 1')\n",
    "                else:\n",
    "                    cat = dict()\n",
    "                    positions = []\n",
    "                    for f in f_id:\n",
    "                        if type(f) is not str:\n",
    "                            raise ValueError('features in f_id list must be variable names of string type')\n",
    "                        else:\n",
    "                            if f not in feature_ids_treat:\n",
    "                                raise ValueError(f'categorical variable {f_id} is not a varibale of data frame')                                     \n",
    "                            else:\n",
    "                                cat[f+'_id'] = feature_ids_treat.index(f)\n",
    "                                cat[f+'_id_label'] = f\n",
    "                                positions = positions + [cat[f+'_id']]\n",
    "                    f_tuple = list(zip(f_id,positions))\n",
    "                    f_tuple = sorted(f_tuple, key = lambda x:x[1], reverse = False)\n",
    "                    f_id = [i[0] for i in f_tuple]\n",
    "                    positions = [i[1] for i in f_tuple]\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('Either X_treat or f_id is not correctly specified')\n",
    "        elif (type(X_treat) is np.ndarray) & (type(X_treat) is np.ndarray) & (type(f_id) is list):\n",
    "            if (len(f_id)<2):\n",
    "                raise ValueError(f'{f_id} must be a list of hot-encoding features a hence neither empty nor have a length of 1')                            \n",
    "            else:\n",
    "                cat = dict()\n",
    "                positions = []\n",
    "                for f in f_id:\n",
    "                    if type(f) is not str:\n",
    "                        raise ValueError('features in f_id list must be variable names of string type')\n",
    "                    else:\n",
    "                        if (f not in feature_ids_treat) | (f not in feature_ids_cont):\n",
    "                            raise ValueError(f'categorical variable {f_id} is not a varibale of cont or treat data frame')                                     \n",
    "                        elif (sorted(feature_ids_treat)!=sorted(feature_ids_cont)) & (type(X_cont_rbtamt) is not np.ndarray):\n",
    "                            raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                        else:\n",
    "                            cat[f+'_id'] = feature_ids_treat.index(f)\n",
    "                            cat[f+'_id_label'] = f\n",
    "                            #cat[f+'_tuple'] = listzip\n",
    "                            positions = positions + [cat[f+'_id']]\n",
    "                f_tuple = list(zip(f_id,positions))\n",
    "                f_tuple = sorted(f_tuple, key = lambda x:x[1], reverse = False)\n",
    "                f_id = [i[0] for i in f_tuple]\n",
    "                positions = [i[1] for i in f_tuple]\n",
    "                X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                mean_rbt = np.array(mean_rbt)\n",
    "                X_labels = list(X.columns)\n",
    "                X_temp = np.array(X)\n",
    "        else:\n",
    "            raise ValueError('X_treat and X_cont (if specified) need to be array types, f_id has to be a list of hot-encoded categorical variables')\n",
    "        \n",
    "        for f in f_id:\n",
    "            if (np.max(X_temp[:,cat[f+'_id']])!=1) | (np.min(X_temp[:,cat[f+'_id']])!=0):\n",
    "                raise ValueError('hot encoded variable is not of binary classification')\n",
    "            else:\n",
    "                pass\n",
    "        #grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower),\n",
    "        #np.percentile(X_temp[:, f_id], grid_upper),\n",
    "        \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types.copy()\n",
    "        #column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "\n",
    "        if set(types) <= set(function_labels):\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i]\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif type(percentile) is not list: # 0<=percentile<=100:\n",
    "                    raise ValueError('percentile must be list')\n",
    "                else:\n",
    "                    column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "                    if len(percentile)>1:\n",
    "                        for p in percentile[1:]:\n",
    "                            types = types + ['percentile'] #pass\n",
    "                            nptypes = nptypes + [np.percentile]\n",
    "                            column_labels = column_labels + ['percentile_' +str(p)]\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "        print(types)\n",
    "        #y_pred = np.zeros((len(grid),len(types)))\n",
    "        #mpc_pred = np.zeros((len(grid), len(types)))        \n",
    "        y_pred = np.zeros((1, len(types)*len(f_id)))\n",
    "        mpc_pred = np.zeros((1, len(types)*len(f_id)))\n",
    "        grid_row = np.identity(len(f_id))\n",
    "        k=0\n",
    "        \n",
    "        column_labels_cr=[]\n",
    "        column_labels_mpc=[]\n",
    "        for f in range(len(f_id)): # i returns the counter, val returns the value at position of counter on grid\n",
    "            p_pos = 0\n",
    "            A = np.array([list(grid_row[f]) for _ in range(len(X_temp))])\n",
    "            X_temp[:, positions] = A\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[0,k] = nptypes[j](y_temp,percentile[p_pos])\n",
    "                    mpc_pred[0,k] = nptypes[j](mean_rbt_temp,percentile[p_pos])\n",
    "                    column_labels_cr = column_labels_cr + ['cr_'+ f_id[f] +'_'+ types[j] + str(percentile[p_pos])]\n",
    "                    column_labels_mpc= column_labels_mpc + ['mpc_'+ f_id[f] +'_'+ types[j] + str(percentile[p_pos])]\n",
    "                    p_pos = p_pos+1\n",
    "                else:\n",
    "                    y_pred[0,k] = nptypes[j](y_temp)\n",
    "                    mpc_pred[0,k] = nptypes[j](mean_rbt_temp)\n",
    "                    column_labels_cr = column_labels_cr + ['cr_'+ f_id[f] +'_'+ types[j]]\n",
    "                    column_labels_mpc= column_labels_mpc + ['mpc_'+ f_id[f] +'_'+ types[j]]\n",
    "                k = k+1\n",
    "        column_labels = column_labels_cr + column_labels_mpc\n",
    "        pd = pd.DataFrame(np.c_[y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return pd #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean', 'percentile', 'std', 'percentile']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cr_educ_nodegree_mean</th>\n",
       "      <th>cr_educ_nodegree_percentile25</th>\n",
       "      <th>cr_educ_nodegree_std</th>\n",
       "      <th>cr_educ_nodegree_percentile75</th>\n",
       "      <th>cr_educ_highschool_mean</th>\n",
       "      <th>cr_educ_highschool_percentile25</th>\n",
       "      <th>cr_educ_highschool_std</th>\n",
       "      <th>cr_educ_highschool_percentile75</th>\n",
       "      <th>cr_educ_higher_mean</th>\n",
       "      <th>cr_educ_higher_percentile25</th>\n",
       "      <th>...</th>\n",
       "      <th>mpc_educ_nodegree_std</th>\n",
       "      <th>mpc_educ_nodegree_percentile75</th>\n",
       "      <th>mpc_educ_highschool_mean</th>\n",
       "      <th>mpc_educ_highschool_percentile25</th>\n",
       "      <th>mpc_educ_highschool_std</th>\n",
       "      <th>mpc_educ_highschool_percentile75</th>\n",
       "      <th>mpc_educ_higher_mean</th>\n",
       "      <th>mpc_educ_higher_percentile25</th>\n",
       "      <th>mpc_educ_higher_std</th>\n",
       "      <th>mpc_educ_higher_percentile75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.195354</td>\n",
       "      <td>-372.653092</td>\n",
       "      <td>853.042301</td>\n",
       "      <td>524.784721</td>\n",
       "      <td>46.549566</td>\n",
       "      <td>-372.7504</td>\n",
       "      <td>876.412161</td>\n",
       "      <td>567.952731</td>\n",
       "      <td>69.43166</td>\n",
       "      <td>-332.53894</td>\n",
       "      <td>...</td>\n",
       "      <td>4.726758</td>\n",
       "      <td>0.525297</td>\n",
       "      <td>0.068543</td>\n",
       "      <td>-0.356623</td>\n",
       "      <td>4.860943</td>\n",
       "      <td>0.568041</td>\n",
       "      <td>0.102237</td>\n",
       "      <td>-0.323154</td>\n",
       "      <td>4.81918</td>\n",
       "      <td>0.573794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cr_educ_nodegree_mean  cr_educ_nodegree_percentile25  cr_educ_nodegree_std  \\\n",
       "0              23.195354                    -372.653092            853.042301   \n",
       "\n",
       "   cr_educ_nodegree_percentile75  cr_educ_highschool_mean  \\\n",
       "0                     524.784721                46.549566   \n",
       "\n",
       "   cr_educ_highschool_percentile25  cr_educ_highschool_std  \\\n",
       "0                        -372.7504              876.412161   \n",
       "\n",
       "   cr_educ_highschool_percentile75  cr_educ_higher_mean  \\\n",
       "0                       567.952731             69.43166   \n",
       "\n",
       "   cr_educ_higher_percentile25  ...  mpc_educ_nodegree_std  \\\n",
       "0                   -332.53894  ...               4.726758   \n",
       "\n",
       "   mpc_educ_nodegree_percentile75  mpc_educ_highschool_mean  \\\n",
       "0                        0.525297                  0.068543   \n",
       "\n",
       "   mpc_educ_highschool_percentile25  mpc_educ_highschool_std  \\\n",
       "0                         -0.356623                 4.860943   \n",
       "\n",
       "   mpc_educ_highschool_percentile75  mpc_educ_higher_mean  \\\n",
       "0                          0.568041              0.102237   \n",
       "\n",
       "   mpc_educ_higher_percentile25  mpc_educ_higher_std  \\\n",
       "0                     -0.323154              4.81918   \n",
       "\n",
       "   mpc_educ_higher_percentile75  \n",
       "0                      0.573794  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean', 'std']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cr_educ_nodegree_mean</th>\n",
       "      <th>cr_educ_nodegree_std</th>\n",
       "      <th>cr_educ_highschool_mean</th>\n",
       "      <th>cr_educ_highschool_std</th>\n",
       "      <th>cr_educ_higher_mean</th>\n",
       "      <th>cr_educ_higher_std</th>\n",
       "      <th>mpc_educ_nodegree_mean</th>\n",
       "      <th>mpc_educ_nodegree_std</th>\n",
       "      <th>mpc_educ_highschool_mean</th>\n",
       "      <th>mpc_educ_highschool_std</th>\n",
       "      <th>mpc_educ_higher_mean</th>\n",
       "      <th>mpc_educ_higher_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.195354</td>\n",
       "      <td>853.042301</td>\n",
       "      <td>46.549566</td>\n",
       "      <td>876.412161</td>\n",
       "      <td>69.43166</td>\n",
       "      <td>867.44203</td>\n",
       "      <td>0.034155</td>\n",
       "      <td>4.726758</td>\n",
       "      <td>0.068543</td>\n",
       "      <td>4.860943</td>\n",
       "      <td>0.102237</td>\n",
       "      <td>4.81918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cr_educ_nodegree_mean  cr_educ_nodegree_std  cr_educ_highschool_mean  \\\n",
       "0              23.195354            853.042301                46.549566   \n",
       "\n",
       "   cr_educ_highschool_std  cr_educ_higher_mean  cr_educ_higher_std  \\\n",
       "0              876.412161             69.43166           867.44203   \n",
       "\n",
       "   mpc_educ_nodegree_mean  mpc_educ_nodegree_std  mpc_educ_highschool_mean  \\\n",
       "0                0.034155               4.726758                  0.068543   \n",
       "\n",
       "   mpc_educ_highschool_std  mpc_educ_higher_mean  mpc_educ_higher_std  \n",
       "0                 4.860943              0.102237              4.81918  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_id = ['educ_nodegree', 'educ_highschool','educ_higher']\n",
    "display(uplift_cat_2m_partial_dependency_mpc(rfdicts['DUR_fin_baseline']['treat1_rf'], rfdicts['DUR_fin_baseline']['cont1_rf'], f_id, rfdicts['DUR_fin_baseline']['cont1_X_labels'], rfdicts['DUR_fin_baseline']['treat1_X'], rfdicts['DUR_fin_baseline']['treat1_rbtamt'], X_cont=rfdicts['DUR_fin_baseline']['cont1_X'], X_cont_rbtamt=rfdicts['DUR_fin_baseline']['cont1_rbtamt'],  feature_ids_cont=rfdicts['DUR_fin_baseline']['treat1_X_labels'], types=['mean','percentile','std'],percentile=[25,75]))\n",
    "f_id = ['educ_highschool','educ_higher','educ_nodegree']\n",
    "display(uplift_cat_2m_partial_dependency_mpc(rfdicts['DUR_fin_baseline']['treat1_rf'], rfdicts['DUR_fin_baseline']['cont1_rf'], f_id, rfdicts['DUR_fin_baseline']['cont1_X_labels'], rfdicts['DUR_fin_baseline']['treat1_X'], rfdicts['DUR_fin_baseline']['treat1_rbtamt'], X_cont=rfdicts['DUR_fin_baseline']['cont1_X'], X_cont_rbtamt=rfdicts['DUR_fin_baseline']['cont1_rbtamt'],  feature_ids_cont=rfdicts['DUR_fin_baseline']['treat1_X_labels'], types=['mean','std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run partial dependence function for given sample and explanatory variables. this may take a while. Hence, save as later as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "\n",
    "rfdicts_keys = list(rfdicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['treat1', 'cont1', 'cont2', 'cont3', 'treat1_X', 'treat1_X_labels', 'treat1_rbtamt', 'treat1_rf', 'cont1_X', 'cont1_X_labels', 'cont1_rbtamt', 'cont1_rf', 'cont2_X', 'cont2_X_labels', 'cont2_rbtamt', 'cont2_rf', 'cont3_X', 'cont3_X_labels', 'cont3_rbtamt', 'cont3_rf', 'cont3_treat1_y_pred', 'cont3_treat1_mpc_pred', 'cont2_treat1_y_pred', 'cont2_treat1_mpc_pred', 'cont1_treat1_y_pred', 'cont1_treat1_mpc_pred', 'cont3_treat1_pdp', 'cont2_treat1_pdp', 'cont1_treat1_pdp']\n",
      "['DUR_fin', 'DUR_finit', 'DUR_nofin', 'FD_fin', 'FD_finit', 'FD_nofin', 'ND_fin', 'ND_finit', 'ND_nofin', 'SND_fin', 'SND_finit', 'SND_nofin']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'adults',\n",
       " 'PERSLT18',\n",
       " 'MARITAL1',\n",
       " 'FINCBTAX',\n",
       " 'FSALARYM',\n",
       " 'FINCBTXM',\n",
       " 'morgpayment',\n",
       " 'qblncm1x_sum',\n",
       " 'orgmrtx_sum',\n",
       " 'qescrowx_sum',\n",
       " 'timeleft',\n",
       " 'finassets',\n",
       " 'educ_nodegree',\n",
       " 'educ_highschool',\n",
       " 'educ_higher']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(rfdicts[rfdicts_keys[0]]))\n",
    "print(list(rfdicts))\n",
    "rfdicts['DUR_fin']['cont1_X_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DUR_fin', 'DUR_finit', 'DUR_nofin', 'FD_fin', 'FD_finit', 'FD_nofin', 'ND_fin', 'ND_finit', 'ND_nofin', 'SND_fin', 'SND_finit', 'SND_nofin']\n",
      "DUR_fin:\n",
      "DUR_finit:\n",
      "DUR_nofin:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d1f78148522e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 pdp = pdp.join(uplift_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n\u001b[0;32m     24\u001b[0m                 \u001b[0mX_cont\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrfdicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_cont_rbtamt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrfdicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_rbtamt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_ids_treat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrfdicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_X_labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=30))\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mrfdicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'{c}_{t}_pdp'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-70b97b4ead3b>\u001b[0m in \u001b[0;36muplift_2m_partial_dependency_mpc\u001b[1;34m(rf_treat, rf_cont, f_id, X_treat, X_treat_rbtamt, X_cont, X_cont_rbtamt, feature_ids_treat, feature_ids_cont, types, percentile, grid_lower, grid_upper)\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                     \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnptypes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[0mmpc_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnptypes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_rbt_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcolumn_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'percentile'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[1;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[0;32m   3495\u001b[0m     \"\"\"\n\u001b[0;32m   3496\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[1;32m-> 3497\u001b[1;33m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[0;32m   3498\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3499\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[1;34m(a, func, **kwargs)\u001b[0m\n\u001b[0;32m   3404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3405\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3406\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#def vimp_plot_uplift()\n",
    "rfdicts_keys = list(rfdicts)\n",
    "#rfdicts_keys = rfdicts_keys[:1]\n",
    "print(rfdicts_keys)\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\pdp\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "    for t in treat:\n",
    "        for c in cont:\n",
    "            expvars = rfdicts[k][c+'_X_labels']           \n",
    "            pdp = uplift_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],expvars[0],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "            X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'], \n",
    "            feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=30)\n",
    "            for v in expvars[1:]:\n",
    "                pdp = pdp.join(uplift_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'], \n",
    "                feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=30))\n",
    "                \n",
    "            rfdicts[k][f'{c}_{t}_pdp'] = pdp\n",
    "            pdp.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}.csv')\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vars = expvars\n",
    "#\n",
    "#pdp = uplift_2m_partial_dependency(rf['treat_rf'], rf['cont_rf'], vars[0], rf['treat_X'], rf['cont_X'], \n",
    "#                                            feature_ids_treat=rf['treat_X_labels'], feature_ids_cont = rf['cont_X_labels'], \n",
    "#                                             types = ['mean','percentile','std','median'], percentile=30)\n",
    "#\n",
    "#for var in vars[1:]:\n",
    "#    pdp = pdp.join(uplift_2m_partial_dependency(rf['treat_rf'], rf['cont_rf'], var, rf['treat_X'], rf['cont_X'], \n",
    "#                                                feature_ids_treat=rf['treat_X_labels'], feature_ids_cont = rf['cont_X_labels'],                                                 \n",
    "#                                                types = ['mean','percentile','std','median'], percentile=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vars = ['age','FINCBTAX']\n",
    "#\n",
    "#pdp = uplift_2m_partial_dependency_mpc(rf['treat_rf'], rf['cont_rf'], vars[0], rf['treat_X'], rf['treat_rbtamt'], X_cont = rf['cont_X'], X_cont_rbtamt=rf['cont_rbtamt'], \n",
    "#                                            feature_ids_treat=rf['treat_X_labels'], feature_ids_cont = rf['cont_X_labels'], \n",
    "#                                             types = ['mean','percentile','std','median'], percentile=30)\n",
    "#\n",
    "#for var in vars[1:]:\n",
    "#    pdp = pdp.join(uplift_2m_partial_dependency(rf['treat_rf'], rf['cont_rf'], var, rf['treat_X'], rf['cont_X'], \n",
    "#                                                feature_ids_treat=rf['treat_X_labels'], feature_ids_cont = rf['cont_X_labels'],                                                 \n",
    "#                                                types = ['mean','percentile','std','median'], percentile=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data frame as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_grid</th>\n",
       "      <th>age_cr_mean</th>\n",
       "      <th>age_cr_percentile_30</th>\n",
       "      <th>age_cr_std</th>\n",
       "      <th>age_cr_median</th>\n",
       "      <th>age_mpc_mean</th>\n",
       "      <th>age_mpc_percentile_30</th>\n",
       "      <th>age_mpc_std</th>\n",
       "      <th>age_mpc_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>370.843239</td>\n",
       "      <td>132.853910</td>\n",
       "      <td>506.938559</td>\n",
       "      <td>365.924840</td>\n",
       "      <td>0.958758</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>269.890186</td>\n",
       "      <td>1.628889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.404040</td>\n",
       "      <td>380.987243</td>\n",
       "      <td>141.193544</td>\n",
       "      <td>508.634730</td>\n",
       "      <td>375.626818</td>\n",
       "      <td>1.743750</td>\n",
       "      <td>0.771117</td>\n",
       "      <td>91.994473</td>\n",
       "      <td>1.625173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.808081</td>\n",
       "      <td>382.083828</td>\n",
       "      <td>143.253810</td>\n",
       "      <td>509.247192</td>\n",
       "      <td>377.409834</td>\n",
       "      <td>0.995660</td>\n",
       "      <td>0.771237</td>\n",
       "      <td>95.569600</td>\n",
       "      <td>1.623875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.212121</td>\n",
       "      <td>383.284882</td>\n",
       "      <td>144.689416</td>\n",
       "      <td>509.304835</td>\n",
       "      <td>378.245810</td>\n",
       "      <td>-93.454401</td>\n",
       "      <td>0.776429</td>\n",
       "      <td>8749.393515</td>\n",
       "      <td>1.626481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.616162</td>\n",
       "      <td>386.513091</td>\n",
       "      <td>148.974087</td>\n",
       "      <td>511.650902</td>\n",
       "      <td>379.880213</td>\n",
       "      <td>-9.506080</td>\n",
       "      <td>0.764236</td>\n",
       "      <td>1043.286742</td>\n",
       "      <td>1.607405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30.020202</td>\n",
       "      <td>384.373291</td>\n",
       "      <td>146.935043</td>\n",
       "      <td>511.830827</td>\n",
       "      <td>379.834350</td>\n",
       "      <td>2.286673</td>\n",
       "      <td>0.760356</td>\n",
       "      <td>74.746137</td>\n",
       "      <td>1.604022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30.424242</td>\n",
       "      <td>386.338028</td>\n",
       "      <td>149.271362</td>\n",
       "      <td>512.081182</td>\n",
       "      <td>381.012704</td>\n",
       "      <td>2.702126</td>\n",
       "      <td>0.759806</td>\n",
       "      <td>301.364391</td>\n",
       "      <td>1.599085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30.828283</td>\n",
       "      <td>389.423591</td>\n",
       "      <td>151.822197</td>\n",
       "      <td>515.317748</td>\n",
       "      <td>383.138657</td>\n",
       "      <td>-106.072405</td>\n",
       "      <td>0.755318</td>\n",
       "      <td>9288.889446</td>\n",
       "      <td>1.588675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31.232323</td>\n",
       "      <td>392.145991</td>\n",
       "      <td>153.803107</td>\n",
       "      <td>516.066230</td>\n",
       "      <td>387.548576</td>\n",
       "      <td>0.193688</td>\n",
       "      <td>0.758671</td>\n",
       "      <td>164.479699</td>\n",
       "      <td>1.588461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31.636364</td>\n",
       "      <td>398.500335</td>\n",
       "      <td>155.294120</td>\n",
       "      <td>522.816542</td>\n",
       "      <td>390.806746</td>\n",
       "      <td>2.394913</td>\n",
       "      <td>0.753275</td>\n",
       "      <td>66.856040</td>\n",
       "      <td>1.570227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32.040404</td>\n",
       "      <td>397.619932</td>\n",
       "      <td>152.845431</td>\n",
       "      <td>523.400088</td>\n",
       "      <td>391.825781</td>\n",
       "      <td>1.544964</td>\n",
       "      <td>0.741349</td>\n",
       "      <td>157.924522</td>\n",
       "      <td>1.559890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32.444444</td>\n",
       "      <td>395.966401</td>\n",
       "      <td>151.338705</td>\n",
       "      <td>523.603578</td>\n",
       "      <td>389.608423</td>\n",
       "      <td>3.575192</td>\n",
       "      <td>0.740804</td>\n",
       "      <td>178.946930</td>\n",
       "      <td>1.563272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32.848485</td>\n",
       "      <td>395.251531</td>\n",
       "      <td>147.378484</td>\n",
       "      <td>524.890021</td>\n",
       "      <td>386.814417</td>\n",
       "      <td>6.553764</td>\n",
       "      <td>0.733087</td>\n",
       "      <td>467.264972</td>\n",
       "      <td>1.555269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33.252525</td>\n",
       "      <td>394.692859</td>\n",
       "      <td>145.944484</td>\n",
       "      <td>526.012359</td>\n",
       "      <td>386.322392</td>\n",
       "      <td>-9.127129</td>\n",
       "      <td>0.730543</td>\n",
       "      <td>771.555013</td>\n",
       "      <td>1.553527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>33.656566</td>\n",
       "      <td>394.771779</td>\n",
       "      <td>147.277546</td>\n",
       "      <td>526.523284</td>\n",
       "      <td>385.521189</td>\n",
       "      <td>-1.175959</td>\n",
       "      <td>0.730930</td>\n",
       "      <td>364.325320</td>\n",
       "      <td>1.555026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>34.060606</td>\n",
       "      <td>391.388363</td>\n",
       "      <td>142.354616</td>\n",
       "      <td>526.169424</td>\n",
       "      <td>382.181560</td>\n",
       "      <td>0.214875</td>\n",
       "      <td>0.722078</td>\n",
       "      <td>113.060818</td>\n",
       "      <td>1.549723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>34.464646</td>\n",
       "      <td>388.660079</td>\n",
       "      <td>140.291226</td>\n",
       "      <td>526.395167</td>\n",
       "      <td>381.436169</td>\n",
       "      <td>-1.517814</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>487.595912</td>\n",
       "      <td>1.551908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>34.868687</td>\n",
       "      <td>387.994306</td>\n",
       "      <td>138.180324</td>\n",
       "      <td>527.155288</td>\n",
       "      <td>380.920113</td>\n",
       "      <td>0.663406</td>\n",
       "      <td>0.711543</td>\n",
       "      <td>66.983365</td>\n",
       "      <td>1.545646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>35.272727</td>\n",
       "      <td>382.528014</td>\n",
       "      <td>131.121276</td>\n",
       "      <td>528.778613</td>\n",
       "      <td>375.827540</td>\n",
       "      <td>-7.026872</td>\n",
       "      <td>0.701778</td>\n",
       "      <td>612.905942</td>\n",
       "      <td>1.543342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>35.676768</td>\n",
       "      <td>380.079103</td>\n",
       "      <td>126.676270</td>\n",
       "      <td>529.324866</td>\n",
       "      <td>373.453375</td>\n",
       "      <td>-1.014282</td>\n",
       "      <td>0.695111</td>\n",
       "      <td>130.497185</td>\n",
       "      <td>1.540265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>36.080808</td>\n",
       "      <td>377.254983</td>\n",
       "      <td>124.461958</td>\n",
       "      <td>529.336400</td>\n",
       "      <td>370.395840</td>\n",
       "      <td>-2.547803</td>\n",
       "      <td>0.686733</td>\n",
       "      <td>245.368158</td>\n",
       "      <td>1.537032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>36.484848</td>\n",
       "      <td>364.446898</td>\n",
       "      <td>112.628164</td>\n",
       "      <td>528.816883</td>\n",
       "      <td>358.671400</td>\n",
       "      <td>2.421474</td>\n",
       "      <td>0.673641</td>\n",
       "      <td>369.907057</td>\n",
       "      <td>1.543784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>36.888889</td>\n",
       "      <td>364.183590</td>\n",
       "      <td>111.372869</td>\n",
       "      <td>530.096930</td>\n",
       "      <td>359.300803</td>\n",
       "      <td>7.399130</td>\n",
       "      <td>0.670583</td>\n",
       "      <td>253.511363</td>\n",
       "      <td>1.544912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>37.292929</td>\n",
       "      <td>362.855421</td>\n",
       "      <td>108.521272</td>\n",
       "      <td>531.118170</td>\n",
       "      <td>357.274985</td>\n",
       "      <td>28.821298</td>\n",
       "      <td>0.664715</td>\n",
       "      <td>2381.017430</td>\n",
       "      <td>1.540570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>37.696970</td>\n",
       "      <td>360.664645</td>\n",
       "      <td>107.666422</td>\n",
       "      <td>530.551893</td>\n",
       "      <td>355.454845</td>\n",
       "      <td>12.044123</td>\n",
       "      <td>0.661755</td>\n",
       "      <td>1073.462463</td>\n",
       "      <td>1.543093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>38.101010</td>\n",
       "      <td>361.080984</td>\n",
       "      <td>108.898005</td>\n",
       "      <td>530.283664</td>\n",
       "      <td>356.581862</td>\n",
       "      <td>1.287046</td>\n",
       "      <td>0.661964</td>\n",
       "      <td>93.771369</td>\n",
       "      <td>1.540632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38.505051</td>\n",
       "      <td>363.479839</td>\n",
       "      <td>110.740168</td>\n",
       "      <td>531.225245</td>\n",
       "      <td>357.775833</td>\n",
       "      <td>6.257838</td>\n",
       "      <td>0.661196</td>\n",
       "      <td>287.724582</td>\n",
       "      <td>1.531524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>38.909091</td>\n",
       "      <td>368.057137</td>\n",
       "      <td>115.651710</td>\n",
       "      <td>532.687640</td>\n",
       "      <td>362.696229</td>\n",
       "      <td>0.893151</td>\n",
       "      <td>0.659390</td>\n",
       "      <td>137.442297</td>\n",
       "      <td>1.522954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>39.313131</td>\n",
       "      <td>368.906510</td>\n",
       "      <td>116.417693</td>\n",
       "      <td>533.062133</td>\n",
       "      <td>363.679682</td>\n",
       "      <td>5.177576</td>\n",
       "      <td>0.658980</td>\n",
       "      <td>450.378009</td>\n",
       "      <td>1.523138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>39.717172</td>\n",
       "      <td>367.704575</td>\n",
       "      <td>113.628971</td>\n",
       "      <td>533.793928</td>\n",
       "      <td>362.239059</td>\n",
       "      <td>30.494894</td>\n",
       "      <td>0.659107</td>\n",
       "      <td>2464.461257</td>\n",
       "      <td>1.525532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>56.282828</td>\n",
       "      <td>374.992295</td>\n",
       "      <td>142.359414</td>\n",
       "      <td>520.654393</td>\n",
       "      <td>371.957118</td>\n",
       "      <td>191.697454</td>\n",
       "      <td>0.780223</td>\n",
       "      <td>16546.457265</td>\n",
       "      <td>1.635368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>56.686869</td>\n",
       "      <td>378.449606</td>\n",
       "      <td>148.932263</td>\n",
       "      <td>518.580886</td>\n",
       "      <td>374.643152</td>\n",
       "      <td>-18.297035</td>\n",
       "      <td>0.795690</td>\n",
       "      <td>1785.379022</td>\n",
       "      <td>1.645308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>57.090909</td>\n",
       "      <td>380.994813</td>\n",
       "      <td>151.021029</td>\n",
       "      <td>517.856985</td>\n",
       "      <td>378.981860</td>\n",
       "      <td>14.893062</td>\n",
       "      <td>0.805382</td>\n",
       "      <td>859.232628</td>\n",
       "      <td>1.653755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>57.494949</td>\n",
       "      <td>376.077438</td>\n",
       "      <td>144.265823</td>\n",
       "      <td>517.959845</td>\n",
       "      <td>374.400999</td>\n",
       "      <td>13.189363</td>\n",
       "      <td>0.803199</td>\n",
       "      <td>1011.224175</td>\n",
       "      <td>1.658700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>57.898990</td>\n",
       "      <td>363.719091</td>\n",
       "      <td>135.892979</td>\n",
       "      <td>516.109379</td>\n",
       "      <td>363.317697</td>\n",
       "      <td>2.812707</td>\n",
       "      <td>0.785967</td>\n",
       "      <td>99.012475</td>\n",
       "      <td>1.653340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>58.303030</td>\n",
       "      <td>358.498838</td>\n",
       "      <td>129.940847</td>\n",
       "      <td>516.021521</td>\n",
       "      <td>361.821000</td>\n",
       "      <td>4.007176</td>\n",
       "      <td>0.766138</td>\n",
       "      <td>222.502283</td>\n",
       "      <td>1.641098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>58.707071</td>\n",
       "      <td>358.694403</td>\n",
       "      <td>133.514600</td>\n",
       "      <td>515.561054</td>\n",
       "      <td>362.795396</td>\n",
       "      <td>-1.324747</td>\n",
       "      <td>0.767643</td>\n",
       "      <td>163.664933</td>\n",
       "      <td>1.642547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>59.111111</td>\n",
       "      <td>351.333181</td>\n",
       "      <td>126.325748</td>\n",
       "      <td>513.398504</td>\n",
       "      <td>359.680578</td>\n",
       "      <td>280.575716</td>\n",
       "      <td>0.761092</td>\n",
       "      <td>24429.776400</td>\n",
       "      <td>1.646673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>59.515152</td>\n",
       "      <td>355.829983</td>\n",
       "      <td>132.179613</td>\n",
       "      <td>513.071975</td>\n",
       "      <td>363.230945</td>\n",
       "      <td>9.733424</td>\n",
       "      <td>0.770910</td>\n",
       "      <td>761.541311</td>\n",
       "      <td>1.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>59.919192</td>\n",
       "      <td>346.929156</td>\n",
       "      <td>123.025722</td>\n",
       "      <td>513.571913</td>\n",
       "      <td>356.639297</td>\n",
       "      <td>1.833439</td>\n",
       "      <td>0.749572</td>\n",
       "      <td>77.192531</td>\n",
       "      <td>1.641052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>60.323232</td>\n",
       "      <td>347.575887</td>\n",
       "      <td>121.739735</td>\n",
       "      <td>512.484905</td>\n",
       "      <td>355.595821</td>\n",
       "      <td>0.881245</td>\n",
       "      <td>0.748608</td>\n",
       "      <td>57.374690</td>\n",
       "      <td>1.640532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>60.727273</td>\n",
       "      <td>344.155821</td>\n",
       "      <td>118.464128</td>\n",
       "      <td>512.359692</td>\n",
       "      <td>350.880792</td>\n",
       "      <td>0.041314</td>\n",
       "      <td>0.739995</td>\n",
       "      <td>84.815778</td>\n",
       "      <td>1.637448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>61.131313</td>\n",
       "      <td>333.429207</td>\n",
       "      <td>110.186664</td>\n",
       "      <td>513.404042</td>\n",
       "      <td>340.980088</td>\n",
       "      <td>1.234134</td>\n",
       "      <td>0.724022</td>\n",
       "      <td>129.780184</td>\n",
       "      <td>1.636767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>61.535354</td>\n",
       "      <td>325.605748</td>\n",
       "      <td>102.724200</td>\n",
       "      <td>509.554965</td>\n",
       "      <td>332.738936</td>\n",
       "      <td>4.893000</td>\n",
       "      <td>0.716283</td>\n",
       "      <td>729.572057</td>\n",
       "      <td>1.643339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>61.939394</td>\n",
       "      <td>319.134293</td>\n",
       "      <td>97.318066</td>\n",
       "      <td>509.603537</td>\n",
       "      <td>325.893763</td>\n",
       "      <td>4.513358</td>\n",
       "      <td>0.706727</td>\n",
       "      <td>291.618988</td>\n",
       "      <td>1.644712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>62.343434</td>\n",
       "      <td>312.900925</td>\n",
       "      <td>92.262503</td>\n",
       "      <td>508.889092</td>\n",
       "      <td>320.385073</td>\n",
       "      <td>2.188431</td>\n",
       "      <td>0.698066</td>\n",
       "      <td>106.779095</td>\n",
       "      <td>1.650858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>62.747475</td>\n",
       "      <td>310.847374</td>\n",
       "      <td>90.345212</td>\n",
       "      <td>508.067666</td>\n",
       "      <td>317.252419</td>\n",
       "      <td>18.920028</td>\n",
       "      <td>0.695289</td>\n",
       "      <td>1618.348184</td>\n",
       "      <td>1.655978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>63.151515</td>\n",
       "      <td>314.705315</td>\n",
       "      <td>94.448238</td>\n",
       "      <td>506.379438</td>\n",
       "      <td>322.606984</td>\n",
       "      <td>7.136936</td>\n",
       "      <td>0.717376</td>\n",
       "      <td>262.813738</td>\n",
       "      <td>1.668704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>63.555556</td>\n",
       "      <td>310.681635</td>\n",
       "      <td>91.150672</td>\n",
       "      <td>505.879887</td>\n",
       "      <td>321.078854</td>\n",
       "      <td>3.356241</td>\n",
       "      <td>0.704921</td>\n",
       "      <td>300.078208</td>\n",
       "      <td>1.666226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>63.959596</td>\n",
       "      <td>301.968339</td>\n",
       "      <td>80.889603</td>\n",
       "      <td>506.047890</td>\n",
       "      <td>312.123554</td>\n",
       "      <td>-0.133859</td>\n",
       "      <td>0.678081</td>\n",
       "      <td>139.577481</td>\n",
       "      <td>1.656981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>64.363636</td>\n",
       "      <td>301.367046</td>\n",
       "      <td>80.620831</td>\n",
       "      <td>505.933727</td>\n",
       "      <td>311.714472</td>\n",
       "      <td>6.224494</td>\n",
       "      <td>0.679974</td>\n",
       "      <td>344.302009</td>\n",
       "      <td>1.661042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>64.767677</td>\n",
       "      <td>297.596520</td>\n",
       "      <td>78.173976</td>\n",
       "      <td>506.634021</td>\n",
       "      <td>307.496430</td>\n",
       "      <td>2.292101</td>\n",
       "      <td>0.672011</td>\n",
       "      <td>140.904807</td>\n",
       "      <td>1.660106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>65.171717</td>\n",
       "      <td>297.806507</td>\n",
       "      <td>78.682072</td>\n",
       "      <td>506.717713</td>\n",
       "      <td>307.557143</td>\n",
       "      <td>-1.081125</td>\n",
       "      <td>0.671885</td>\n",
       "      <td>262.962889</td>\n",
       "      <td>1.658886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>65.575758</td>\n",
       "      <td>295.241250</td>\n",
       "      <td>76.505955</td>\n",
       "      <td>506.700852</td>\n",
       "      <td>302.721790</td>\n",
       "      <td>4.391816</td>\n",
       "      <td>0.652783</td>\n",
       "      <td>216.250036</td>\n",
       "      <td>1.647989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>65.979798</td>\n",
       "      <td>291.285319</td>\n",
       "      <td>73.089960</td>\n",
       "      <td>506.933809</td>\n",
       "      <td>298.577505</td>\n",
       "      <td>3.943861</td>\n",
       "      <td>0.635328</td>\n",
       "      <td>215.223080</td>\n",
       "      <td>1.640948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>66.383838</td>\n",
       "      <td>288.307002</td>\n",
       "      <td>70.516088</td>\n",
       "      <td>506.718372</td>\n",
       "      <td>295.118839</td>\n",
       "      <td>0.831224</td>\n",
       "      <td>0.630194</td>\n",
       "      <td>50.165295</td>\n",
       "      <td>1.644417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>66.787879</td>\n",
       "      <td>287.749749</td>\n",
       "      <td>71.265814</td>\n",
       "      <td>505.086465</td>\n",
       "      <td>293.168701</td>\n",
       "      <td>4.105404</td>\n",
       "      <td>0.642503</td>\n",
       "      <td>209.909644</td>\n",
       "      <td>1.660653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>67.191919</td>\n",
       "      <td>287.598966</td>\n",
       "      <td>70.712084</td>\n",
       "      <td>505.145229</td>\n",
       "      <td>293.178884</td>\n",
       "      <td>-5.100644</td>\n",
       "      <td>0.642069</td>\n",
       "      <td>602.317451</td>\n",
       "      <td>1.661314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>67.595960</td>\n",
       "      <td>284.107138</td>\n",
       "      <td>63.092589</td>\n",
       "      <td>505.686333</td>\n",
       "      <td>290.924256</td>\n",
       "      <td>-2.360790</td>\n",
       "      <td>0.622516</td>\n",
       "      <td>484.180990</td>\n",
       "      <td>1.655262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>68.000000</td>\n",
       "      <td>285.253027</td>\n",
       "      <td>63.613822</td>\n",
       "      <td>505.745380</td>\n",
       "      <td>292.558761</td>\n",
       "      <td>0.077427</td>\n",
       "      <td>0.624622</td>\n",
       "      <td>159.389980</td>\n",
       "      <td>1.652096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age_grid  age_cr_mean  age_cr_percentile_30  age_cr_std  age_cr_median  \\\n",
       "0   28.000000   370.843239            132.853910  506.938559     365.924840   \n",
       "1   28.404040   380.987243            141.193544  508.634730     375.626818   \n",
       "2   28.808081   382.083828            143.253810  509.247192     377.409834   \n",
       "3   29.212121   383.284882            144.689416  509.304835     378.245810   \n",
       "4   29.616162   386.513091            148.974087  511.650902     379.880213   \n",
       "5   30.020202   384.373291            146.935043  511.830827     379.834350   \n",
       "6   30.424242   386.338028            149.271362  512.081182     381.012704   \n",
       "7   30.828283   389.423591            151.822197  515.317748     383.138657   \n",
       "8   31.232323   392.145991            153.803107  516.066230     387.548576   \n",
       "9   31.636364   398.500335            155.294120  522.816542     390.806746   \n",
       "10  32.040404   397.619932            152.845431  523.400088     391.825781   \n",
       "11  32.444444   395.966401            151.338705  523.603578     389.608423   \n",
       "12  32.848485   395.251531            147.378484  524.890021     386.814417   \n",
       "13  33.252525   394.692859            145.944484  526.012359     386.322392   \n",
       "14  33.656566   394.771779            147.277546  526.523284     385.521189   \n",
       "15  34.060606   391.388363            142.354616  526.169424     382.181560   \n",
       "16  34.464646   388.660079            140.291226  526.395167     381.436169   \n",
       "17  34.868687   387.994306            138.180324  527.155288     380.920113   \n",
       "18  35.272727   382.528014            131.121276  528.778613     375.827540   \n",
       "19  35.676768   380.079103            126.676270  529.324866     373.453375   \n",
       "20  36.080808   377.254983            124.461958  529.336400     370.395840   \n",
       "21  36.484848   364.446898            112.628164  528.816883     358.671400   \n",
       "22  36.888889   364.183590            111.372869  530.096930     359.300803   \n",
       "23  37.292929   362.855421            108.521272  531.118170     357.274985   \n",
       "24  37.696970   360.664645            107.666422  530.551893     355.454845   \n",
       "25  38.101010   361.080984            108.898005  530.283664     356.581862   \n",
       "26  38.505051   363.479839            110.740168  531.225245     357.775833   \n",
       "27  38.909091   368.057137            115.651710  532.687640     362.696229   \n",
       "28  39.313131   368.906510            116.417693  533.062133     363.679682   \n",
       "29  39.717172   367.704575            113.628971  533.793928     362.239059   \n",
       "..        ...          ...                   ...         ...            ...   \n",
       "70  56.282828   374.992295            142.359414  520.654393     371.957118   \n",
       "71  56.686869   378.449606            148.932263  518.580886     374.643152   \n",
       "72  57.090909   380.994813            151.021029  517.856985     378.981860   \n",
       "73  57.494949   376.077438            144.265823  517.959845     374.400999   \n",
       "74  57.898990   363.719091            135.892979  516.109379     363.317697   \n",
       "75  58.303030   358.498838            129.940847  516.021521     361.821000   \n",
       "76  58.707071   358.694403            133.514600  515.561054     362.795396   \n",
       "77  59.111111   351.333181            126.325748  513.398504     359.680578   \n",
       "78  59.515152   355.829983            132.179613  513.071975     363.230945   \n",
       "79  59.919192   346.929156            123.025722  513.571913     356.639297   \n",
       "80  60.323232   347.575887            121.739735  512.484905     355.595821   \n",
       "81  60.727273   344.155821            118.464128  512.359692     350.880792   \n",
       "82  61.131313   333.429207            110.186664  513.404042     340.980088   \n",
       "83  61.535354   325.605748            102.724200  509.554965     332.738936   \n",
       "84  61.939394   319.134293             97.318066  509.603537     325.893763   \n",
       "85  62.343434   312.900925             92.262503  508.889092     320.385073   \n",
       "86  62.747475   310.847374             90.345212  508.067666     317.252419   \n",
       "87  63.151515   314.705315             94.448238  506.379438     322.606984   \n",
       "88  63.555556   310.681635             91.150672  505.879887     321.078854   \n",
       "89  63.959596   301.968339             80.889603  506.047890     312.123554   \n",
       "90  64.363636   301.367046             80.620831  505.933727     311.714472   \n",
       "91  64.767677   297.596520             78.173976  506.634021     307.496430   \n",
       "92  65.171717   297.806507             78.682072  506.717713     307.557143   \n",
       "93  65.575758   295.241250             76.505955  506.700852     302.721790   \n",
       "94  65.979798   291.285319             73.089960  506.933809     298.577505   \n",
       "95  66.383838   288.307002             70.516088  506.718372     295.118839   \n",
       "96  66.787879   287.749749             71.265814  505.086465     293.168701   \n",
       "97  67.191919   287.598966             70.712084  505.145229     293.178884   \n",
       "98  67.595960   284.107138             63.092589  505.686333     290.924256   \n",
       "99  68.000000   285.253027             63.613822  505.745380     292.558761   \n",
       "\n",
       "    age_mpc_mean  age_mpc_percentile_30   age_mpc_std  age_mpc_median  \n",
       "0       0.958758               0.758333    269.890186        1.628889  \n",
       "1       1.743750               0.771117     91.994473        1.625173  \n",
       "2       0.995660               0.771237     95.569600        1.623875  \n",
       "3     -93.454401               0.776429   8749.393515        1.626481  \n",
       "4      -9.506080               0.764236   1043.286742        1.607405  \n",
       "5       2.286673               0.760356     74.746137        1.604022  \n",
       "6       2.702126               0.759806    301.364391        1.599085  \n",
       "7    -106.072405               0.755318   9288.889446        1.588675  \n",
       "8       0.193688               0.758671    164.479699        1.588461  \n",
       "9       2.394913               0.753275     66.856040        1.570227  \n",
       "10      1.544964               0.741349    157.924522        1.559890  \n",
       "11      3.575192               0.740804    178.946930        1.563272  \n",
       "12      6.553764               0.733087    467.264972        1.555269  \n",
       "13     -9.127129               0.730543    771.555013        1.553527  \n",
       "14     -1.175959               0.730930    364.325320        1.555026  \n",
       "15      0.214875               0.722078    113.060818        1.549723  \n",
       "16     -1.517814               0.717293    487.595912        1.551908  \n",
       "17      0.663406               0.711543     66.983365        1.545646  \n",
       "18     -7.026872               0.701778    612.905942        1.543342  \n",
       "19     -1.014282               0.695111    130.497185        1.540265  \n",
       "20     -2.547803               0.686733    245.368158        1.537032  \n",
       "21      2.421474               0.673641    369.907057        1.543784  \n",
       "22      7.399130               0.670583    253.511363        1.544912  \n",
       "23     28.821298               0.664715   2381.017430        1.540570  \n",
       "24     12.044123               0.661755   1073.462463        1.543093  \n",
       "25      1.287046               0.661964     93.771369        1.540632  \n",
       "26      6.257838               0.661196    287.724582        1.531524  \n",
       "27      0.893151               0.659390    137.442297        1.522954  \n",
       "28      5.177576               0.658980    450.378009        1.523138  \n",
       "29     30.494894               0.659107   2464.461257        1.525532  \n",
       "..           ...                    ...           ...             ...  \n",
       "70    191.697454               0.780223  16546.457265        1.635368  \n",
       "71    -18.297035               0.795690   1785.379022        1.645308  \n",
       "72     14.893062               0.805382    859.232628        1.653755  \n",
       "73     13.189363               0.803199   1011.224175        1.658700  \n",
       "74      2.812707               0.785967     99.012475        1.653340  \n",
       "75      4.007176               0.766138    222.502283        1.641098  \n",
       "76     -1.324747               0.767643    163.664933        1.642547  \n",
       "77    280.575716               0.761092  24429.776400        1.646673  \n",
       "78      9.733424               0.770910    761.541311        1.646700  \n",
       "79      1.833439               0.749572     77.192531        1.641052  \n",
       "80      0.881245               0.748608     57.374690        1.640532  \n",
       "81      0.041314               0.739995     84.815778        1.637448  \n",
       "82      1.234134               0.724022    129.780184        1.636767  \n",
       "83      4.893000               0.716283    729.572057        1.643339  \n",
       "84      4.513358               0.706727    291.618988        1.644712  \n",
       "85      2.188431               0.698066    106.779095        1.650858  \n",
       "86     18.920028               0.695289   1618.348184        1.655978  \n",
       "87      7.136936               0.717376    262.813738        1.668704  \n",
       "88      3.356241               0.704921    300.078208        1.666226  \n",
       "89     -0.133859               0.678081    139.577481        1.656981  \n",
       "90      6.224494               0.679974    344.302009        1.661042  \n",
       "91      2.292101               0.672011    140.904807        1.660106  \n",
       "92     -1.081125               0.671885    262.962889        1.658886  \n",
       "93      4.391816               0.652783    216.250036        1.647989  \n",
       "94      3.943861               0.635328    215.223080        1.640948  \n",
       "95      0.831224               0.630194     50.165295        1.644417  \n",
       "96      4.105404               0.642503    209.909644        1.660653  \n",
       "97     -5.100644               0.642069    602.317451        1.661314  \n",
       "98     -2.360790               0.622516    484.180990        1.655262  \n",
       "99      0.077427               0.624622    159.389980        1.652096  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pdp\n",
    "#%whos DataFrame\n",
    "#pdp.to_csv(os.getcwd() + '\\\\pdp\\\\pdp_' + pathend + '.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot partial dependeny as comparison between the different specifications for a given control group and type of consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import PercentFormatter #plot as percentage\n",
    "import seaborn #plot density and histogram at the same time\n",
    "# Set directory where files are downloaded to. Chdir has to be changed in order to run on another computer\n",
    "os.chdir('C:\\\\Users\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE')\n",
    "os.getcwd()\n",
    "\n",
    "pds_dir = os.listdir(os.getcwd()+'\\\\pdp')\n",
    "pds_dir = [s for s in pds_dir if s[-4:]=='.csv']\n",
    "pds = dict()\n",
    "pds_dir\n",
    "\n",
    "\n",
    "for i in pds_dir:\n",
    "    pds[i[:-4]] = pd.read_csv(os.getcwd()+'\\\\pdp\\\\'+ i)\n",
    "pds\n",
    "pds_keys = list(pds.keys())\n",
    "\n",
    "pathlist=[p[:13] for p in pds_keys]\n",
    "pathlist = list(set(pathlist))\n",
    "for path in pathlist:\n",
    "    newpath = os.getcwd() + '\\\\pdp\\\\' + path\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "#pd.read_csv(os.getcwd()+'\\\\pdp\\\\'+ )\n",
    "\n",
    "var_plot = ['age']\n",
    "type_plot = ['_mean']\n",
    "var_plot = ['age','FINCBTXM','FSALARYM','FINCBTAX','orgmrtx_sum','qblncm1x_sum','adults','PERSLT18','morgpayment']\n",
    "vars_label = ['age', 'tot amount of family income bef taxes', 'income from wages', 'income in last 12 months', \n",
    "              'sum of mortgage amounts', 'sum of principal balances outstanding at the beginning of month', 'number of adults', \n",
    "              'people below 18 in hh', 'mortgage payment per month']\n",
    "\n",
    "cons = ['FD_','SND']\n",
    "#check = [key for key in pds_keys if key[0:7] == 'pdp_' + cons[0]]\n",
    "\n",
    "for c in cons:\n",
    "    pds_cons = [key for key in pds_keys if key[0:7] == 'pdp_' + c]\n",
    "    for var in var_plot:\n",
    "        for i in pds_cons:\n",
    "            for j in type_plot:\n",
    "                plt.plot(pds[i][var+'_grid'], pds[i][var+j], label=i[4:])\n",
    "        plt.legend()\n",
    "        plt.xlabel(vars_label[var_plot.index(var)])\n",
    "        plt.ylabel(j[1:] + ' estimated consumption response')\n",
    "        plt.title('Partial Dependence Plot,'+ pds_cons[0][4:7])\n",
    "        plt.savefig(os.getcwd() + '\\\\pdp\\\\' + f'{i[:13]}' + f'\\\\pdp_{var}.pdf')\n",
    "        plt.show()\n",
    "   \n",
    "    \n",
    "fin_keys = [key for key in pds_keys if key[-5:]!='nofin']\n",
    "\n",
    "for c in cons:\n",
    "    fin_cons = [key for key in fin_keys if key[0:7] == 'pdp_' + c]\n",
    "    for j in type_plot:\n",
    "        for i in fin_cons:\n",
    "            if 'finassets_it' + j  in list(pds[i]):\n",
    "                plt.plot( pds[i]['finassets_it_grid'],pds[i]['finassets_it'+j],label=i[4:])\n",
    "            else:\n",
    "                plt.plot(pds[i]['finassets_grid'], pds[i]['finassets'+j],label=i[4:])\n",
    "        plt.xlabel('sum of checkings and savings account')\n",
    "        plt.ylabel(j[1:] + ' estimated consumption response')\n",
    "        plt.title('Partial Dependence Plot,'+ fin_cons[0][4:7])\n",
    "        plt.legend()\n",
    "        plt.savefig(os.getcwd() + '\\\\pdp\\\\' + f'{i[:13]}' + f'\\\\pdp_finassets.pdf')\n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7** Visualize tree (not done yet) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cons = ['FD_','SND']\n",
    "check = [key for key in pds_keys if key[0:7] == 'pdp_' + cons[0]]\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pull out one tree from forest\n",
    "tree = rf.estimators_[5]\n",
    "\n",
    "#export image to a dot file\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = x_list, rounded = True, precision = 1 )\n",
    "(graph, )= pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree_check.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
