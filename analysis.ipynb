{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import PercentFormatter #plot as percentage\n",
    "import seaborn #plot density and histogram at the same time\n",
    "# Set directory where files are downloaded to. Chdir has to be changed in order to run on another computer\n",
    "os.chdir('C:\\\\Users\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE') #change this to the folder where the data set is stored, all the results will be saved in the same folder\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Data** (necessary for part1 and part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08 = pd.read_csv(os.getcwd()+'\\\\fs08.csv').set_index('CustID')\n",
    "#identifier\n",
    "TIME = ['QINTRVMO', 'QINTRVYR', 'rbtmo_1', 'rbtmo_2', 'diff_1', 'diff_2']\n",
    "ID = ['NEWID']\n",
    "\n",
    "#dependent variables\n",
    "CONS = ['FD','SND','ND','DUR','TOT']\n",
    "FUTCONS = ['fut_' + c for c in CONS]\n",
    "LRUNCONS = ['lrun_' + c for c in CONS]\n",
    "\n",
    "for i in range(len(LRUNCONS)):\n",
    "    fs08[LRUNCONS[i]] = fs08[[CONS[i],FUTCONS[i]]].sum(axis=1)\n",
    "\n",
    "#explanatory variables\n",
    "DEMO = ['age', 'adults', 'PERSLT18', 'MARITAL1', 'CUTENURE'] #exclude , 'FINCBTAX'\n",
    "    #age; number of adults; people below 18; marital status; housing tenure; income in the last 12 months\n",
    "DEMO2 = ['FSALARYM', 'FINCBTXM'] \n",
    "    #FSALARYM: income from salary and wages, CKBKACTX: balance/market value in balance accounts/brookerage accounts;    \n",
    "    #FINCBTXM: Total amount of family income before taxes (Imputed or collected data); (relevant demographics available for the second stimulus only)\n",
    "ASSETS = ['valid_finassets','finassets']\n",
    "    # finassets: sum of 1) SAVACCTX (Total balance/market value (including interest earned) CU had in savings accounts in banks, savings and loans,\n",
    "                         #credit unions, etc., as of the last day of previous month;)\n",
    "                # and    2)CKBKACTX (Total balance or market value (including interest earned) CU had in checking accounts, brokerage accounts, \n",
    "                            #and other similar accounts as of the last day of the previous month\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'qescrowx_sum', 'timeleft'] #exclude , 'orgmrtx_sum',\n",
    "    #morgpayment: morgage payment per month; qblncm1x_sum: sum of principal balances outstanding at the beginning of month M1; orgmrtx_sum: sum of mortgage amounts;\n",
    "    #qescrowx_sum: sum of last regular escrow payments; timeleft: maximum time left on mortgage payment\n",
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #\n",
    "#sample split\n",
    "RBT = ['rbtamt', 'rbtamt_chk', 'rbtamt_e']\n",
    "LAGRBT = ['last_' + var for var in RBT] #lagged variables\n",
    "FUTRBT = ['fut_' + var for var in RBT] #future variables\n",
    "\n",
    "for m in MORTGAGE:\n",
    "    fs08.loc[fs08[m].isna(),m]=0\n",
    "        \n",
    "\n",
    "\n",
    "fs08 = fs08[TIME + ID + CONS + DEMO + DEMO2 + ASSETS + MORTGAGE + RBT + ['rbtamt_1','rbtamt_2'] + LAGRBT + FUTRBT + FUTCONS + LRUNCONS + EDUC] #+ CHGCONS + LAGCONS \n",
    "#fs08 = fs08.loc[fs08['timeleft']>0,:]\n",
    "fs08 = pd.get_dummies(fs08, columns=['CUTENURE','MARITAL1']) #change categorical variables to dummy variables\n",
    "\n",
    "DEMO = [s for s in DEMO if s!='CUTENURE' if s!='MARITAL1'] + ['CUTENURE' + f'_{j}' for j in list(range(1,6)) if j!=3] +['MARITAL1' + f'_{j}' for j in list(range(1,5))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the average rebate amount per individual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08['rbtamt_idmean'] = 0\n",
    "fs08['rbtamt_idmean'] = fs08.groupby('CustID')['rbtamt'].transform('mean')\n",
    "fs08['rbt_count'] = 0\n",
    "fs08['rbt_count'] = fs08.groupby('CustID')['rbtamt'].transform('count')\n",
    "\n",
    "#\n",
    "#sometimes individuals give information of rebate receipt preceding (following) three months of the first (last) interview.\n",
    "#Wherever this is the case, the average rebate should be the weighted mean of rebates received before (after) the relevant time and the actual rebate \n",
    "\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['last_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) #weighted mean,  & (fs08['rbtamt_idmean']>0)\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist() # & (fs08['rbtamt_idmean']>0)\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first') #change for all entries for a given individual\n",
    "\n",
    "\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['fut_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) #weighted mean  & (fs08['rbtamt_idmean']>0)\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0)  & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() #& (fs08['rbtamt_idmean']>0)\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last')  #change for all entries for a given individual\n",
    "#display(fs08.loc[index, ['rbtamt_idmean', 'rbtamt', 'fut_rbtamt', 'last_rbtamt']])\n",
    "\n",
    "#wherever there is no entry for rebates received in the relevant time period but when there were rebates receivde in the past (future) change mean to the value\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() #                                                                               \n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 fs08['fut_rbtamt'], fs08['rbtamt_idmean'])\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last')\n",
    "\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist()\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())), \n",
    "                                fs08['last_rbtamt'], fs08['rbtamt_idmean'])\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first')\n",
    "\n",
    "\n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[(fs08['rbtamt']>0) | (fs08['fut_rbtamt']>0) | (fs08['last_rbtamt']>0) ,'rbt_flag'] = 1\n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('sum')\n",
    "fs08.loc[fs08['rbt_flag']>0, 'rbt_flag'] = 1\n",
    "fs08 = fs08.loc[fs08['rbt_flag']==1]\n",
    "\n",
    "\n",
    "index = fs08.index[fs08['rbtamt_idmean'].isna()].tolist()\n",
    "index = list(set(index))\n",
    "fs08.loc[index, 'rbtamt_idmean'] = fs08.loc[index,'fut_rbtamt']\n",
    "fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'fut_rbtamt' ]\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'last_rbtamt' ]\n",
    "\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations, impute values for financial liquidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(RBT)):\n",
    "    fs08.loc[fs08[RBT[i]]==0, RBT[i]] = np.nan\n",
    "    fs08.loc[fs08[LAGRBT[i]]==0, LAGRBT[i]] = np.nan\n",
    "    fs08.loc[fs08[FUTRBT[i]]==0, FUTRBT[i]] = np.nan\n",
    "    #fs08.loc[(fs08[LAGRBT[i]]==0) | (fs08[LAGRBT[i]].isna()), LAGRBT[i]] =  fs08.loc[(fs08[LAGRBT[i]]==0) | (fs08[LAGRBT[i]].isna())].groupby('CustID')[RBT[i]].shift(-1) \n",
    "    fs08.loc[(fs08[FUTRBT[i]]==0) | (fs08[FUTRBT[i]].isna()), FUTRBT[i]] =  fs08.loc[(fs08[FUTRBT[i]]==0) | (fs08[FUTRBT[i]].isna())].groupby('CustID')[RBT[i]].shift(-1) \n",
    "\n",
    "\n",
    "\n",
    "fs08 = fs08.reset_index()\n",
    "\n",
    "#Iterative imputation for financial liquidity\n",
    "#explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "fs08 = fs08.dropna(subset=CONS+DEMO+DEMO2+MORTGAGE) #Keep only observations that have all info on explanatory variables, dropping missing values on mortgage lowers the sample to half \n",
    "\n",
    "\n",
    "fs08_finit = fs08.copy()\n",
    "fs08_finit = fs08_finit[ ID + CONS + DEMO + DEMO2 + ASSETS + MORTGAGE +  LRUNCONS + EDUC]\n",
    "#fs08_finit = fs08_finit.loc[:,CONS+DEMO+DEMO2+MORTGAGE+['CustID','NEWID','finassets']]\n",
    "labels = list(fs08_finit.columns)\n",
    "imp_mean = IterativeImputer(random_state=0) #use python package iterative imputer\n",
    "imp_mean.fit(fs08_finit[2:])\n",
    "fs08_finit = pd.DataFrame(imp_mean.transform(fs08_finit),columns=labels)\n",
    "fs08_finit = fs08_finit.loc[:,['finassets','NEWID']]\n",
    "fs08_finit = fs08_finit.rename(columns={'finassets':'finassets_it'})\n",
    "\n",
    "fs08 = pd.merge(fs08.sort_values(by = ['NEWID']).reset_index(), fs08_finit.sort_values(by = ['NEWID']).reset_index(), how = 'left', on = 'NEWID', validate = '1:1')\n",
    "fs08 = fs08.drop(columns=['index_x','index_y']) \n",
    "ASSETS = ['valid_finassets','finassets', 'finassets_it' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate treatment and control groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate treatment group:\n",
    "fs08['treat1'] = 0 \n",
    "fs08.loc[fs08['rbtamt'].notna(),'treat1'] = 1 #all entries with actual info on rebate are in the treatment group\n",
    "\n",
    "#three different control groups:\n",
    "#control group 1: those who didn't receive the rebate in the given month\n",
    "fs08['cont1'] = 0\n",
    "fs08.loc[fs08['rbtamt'].isna(), 'cont1'] = 1\n",
    "\n",
    "#control group 2: drop one time period after receipt of rebate, as part of rebate might've been consumed one time period after\n",
    "fs08['cont2'] = 0 \n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[fs08['rbtamt']>0,'rbt_flag'] = 1 #identifier for rebate\n",
    "fs08['rbt_flag_lag'] = fs08.groupby('CustID')['rbt_flag'].shift(1) #identifier for rebate a period before (lag)\n",
    "fs08.loc[fs08['rbt_flag_lag']==1,'rbt_flag']=1 #change rebate identifier so it capture now if a rebate was received this period or the period before\n",
    "fs08.loc[(fs08['rbtamt'].isna()) & (fs08['rbt_flag']==0), 'cont2'] = 1 #those who didn't receive a rebate now or a period before are in the control group\n",
    "\n",
    "#treatment 2: all individuals who received a rebate last time period. This group should be compared to control group 2 only\n",
    "fs08['treat2'] = 0\n",
    "fs08.loc[(fs08['cont2']==0) & (fs08['treat1']==0),'treat2'] = 1\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt'].isna()), 'treat2'] = 1\n",
    "\n",
    "#treatment 3: long run consumption response: all individuals who received a rebate in this period and where the rebate interview is not the last\n",
    "fs08['treat3'] = 0\n",
    "fs08.loc[(fs08['rbtamt']>0) & (fs08[FUTCONS[0]].notna()), 'treat3'] = 1\n",
    "\n",
    "#control 3: long-run consumption: those who haven't received a rebate two periods from current period and have information on consumption for next period as well\n",
    "fs08['cont4'] = 0\n",
    "fs08.loc[(fs08['cont2']==1) & (fs08[FUTCONS[0]].notna()),'cont4'] = 1\n",
    "\n",
    "#control 4: those who haven't received the rebate yet\n",
    "fs08['cont3'] = 0 \n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('cumsum') #starts counting from the point on which the first rebate was received\n",
    "fs08.loc[(fs08['rbtamt'].isna()) & (fs08['rbt_flag']==0),'cont3'] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08_cap = fs08[(np.abs(stats.zscore(fs08.loc[:,CONS+LRUNCONS])) < 3).all(axis=1)] #drop outliers\n",
    "fs08_cap = fs08_cap.loc[fs08_cap['FD']>0] #there are still two observations where food consumption is zero; drop bc of common sense\n",
    "fs08_cap = fs08_cap.loc[fs08['ND']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Descriptive statistics** (part 1 and part 2 cn be run seperately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Take a look at the explanatory variables used for random forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4028.000000\n",
       "mean       49.705313\n",
       "std        15.572740\n",
       "min        21.000000\n",
       "25%        37.500000\n",
       "50%        49.000000\n",
       "75%        61.500000\n",
       "max        84.500000\n",
       "Name: age, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{age} & \\multicolumn{2}{c}{FINCBTXM} & \\multicolumn{2}{c}{finassets} & \\multicolumn{2}{c}{finassets\\_it} \\\\\n",
      "{} &  treat1 &    cont1 &   treat1 &    cont1 &    treat1 &     cont1 &       treat1 &    cont1 \\\\\n",
      "\\midrule\n",
      "count & 4,028.0 & 11,195.0 &  4,028.0 & 11,195.0 &     723.0 &   1,458.0 &      4,028.0 & 11,195.0 \\\\\n",
      "mean  &    49.7 &     50.2 & 61,456.0 & 62,444.7 &  36,182.4 &  44,898.0 &     48,045.8 & 50,124.9 \\\\\n",
      "std   &    15.6 &     15.3 & 46,710.9 & 47,227.1 & 144,797.3 & 214,301.3 &     78,665.0 & 94,903.9 \\\\\n",
      "25\\%   &    37.5 &     38.0 & 28,149.5 & 29,000.0 &     225.0 &     200.0 &      9,616.5 & 11,342.8 \\\\\n",
      "50\\%   &    49.0 &     49.0 & 50,780.5 & 52,056.0 &   2,200.0 &   2,600.0 &     30,743.0 & 31,556.1 \\\\\n",
      "75\\%   &    61.5 &     61.5 & 81,927.0 & 82,553.5 &  14,000.0 &  16,000.0 &     61,857.0 & 62,810.2 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{morgpayment} & \\multicolumn{2}{c}{qblncm1x\\_sum} & \\multicolumn{2}{c}{qescrowx\\_sum} & \\multicolumn{2}{c}{timeleft} \\\\\n",
      "{} &      treat1 &   cont1 &       treat1 &     cont1 &       treat1 &   cont1 &   treat1 &   cont1 \\\\\n",
      "\\midrule\n",
      "count &     1,854.0 & 5,258.0 &      1,861.0 &   5,266.0 &      1,562.0 & 4,326.0 &  1,870.0 & 5,299.0 \\\\\n",
      "mean  &        39.8 &    39.5 &    125,795.3 & 123,429.3 &        359.7 &   364.1 &     19.5 &    19.4 \\\\\n",
      "std   &        54.4 &    57.7 &    107,836.2 & 108,120.2 &        315.3 &   338.7 &      8.7 &     8.8 \\\\\n",
      "25\\%   &        20.0 &    19.6 &     54,423.0 &  53,027.5 &        154.0 &   155.2 &     12.5 &    12.2 \\\\\n",
      "50\\%   &        29.5 &    28.9 &     99,183.0 &  96,212.0 &        275.0 &   273.0 &     21.8 &    21.9 \\\\\n",
      "75\\%   &        45.5 &    44.5 &    165,662.0 & 159,485.8 &        478.0 &   464.0 &     26.9 &    26.9 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'qescrowx_sum', 'timeleft'] #'orgmrtx_sum',\n",
    "expvars = DEMO+DEMO2+ASSETS+MORTGAGE+EDUC+['rbtamt','rbtamt_idmean']\n",
    "\n",
    "#income table\n",
    "for exp in expvars:\n",
    "    if exp == DEMO[0]:\n",
    "        des = fs08_cap.loc[fs08_cap['treat1']==1,exp].describe()\n",
    "        display(des)\n",
    "        des = pd.concat([des, fs08_cap.loc[fs08_cap['cont1']==1,exp].describe()], axis=1, join='inner')\n",
    "    else:\n",
    "        for g in ['treat1','cont1']:\n",
    "            if exp in MORTGAGE:\n",
    "                des = pd.concat([des, fs08_cap.loc[(fs08[g]==1)&(fs08[exp]>0),exp].describe()], axis=1, join='inner')\n",
    "            else:\n",
    "                des = pd.concat([des, fs08_cap.loc[fs08[g]==1,exp].describe()], axis=1, join='inner')\n",
    "\n",
    "index1 = [ i for i in expvars for reps in range(2) ]\n",
    "index2 = ['treat1','cont1']*int((len(index1)/2))\n",
    "tuples = list(zip(index1,index2))\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples) \n",
    "des_cols = list(des)\n",
    "des_cols=[i for i in des_cols if i[0] in ['FINCBTXM','finassets','finassets_it','age']]\n",
    "print(des.iloc[[0,1,2,4,5,6]].to_latex(float_format=\"{:,.1f}\".format, columns=des_cols,multicolumn_format='c')) #table1\n",
    "des_cols = list(des)\n",
    "des_cols = [i for i in des_cols if i[0] in ['morgpayment','qblncm1x_sum','qescrowx_sum','timeleft']]\n",
    "print(des.iloc[[0,1,2,4,5,6]].to_latex(float_format=\"{:,.1f}\".format, columns=des_cols,multicolumn_format='c')) #table2\n",
    "des_cols = list(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average amount of rebate received per household\n",
      "\\begin{tabular}{llrrrrrrr}\n",
      "\\toprule\n",
      "       &             &  count &  mean &   max &  min &  25\\% &   75\\% &  std \\\\\n",
      "\\midrule\n",
      "cont1 & rbtamt\\_mean & 11,195 &   959 & 3,660 &    6 &  600 & 1,200 &  507 \\\\\n",
      "treat1 & rbtamt &  4,028 &   944 & 3,660 &    1 &  600 & 1,200 &  508 \\\\\n",
      "       & rbtamt\\_mean &  4,028 &   945 & 3,660 &    6 &  600 & 1,200 &  502 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Average amount of rebate received per household')\n",
    "des = fs08_cap.loc[fs08_cap['cont1']==1,'rbtamt_idmean'].describe()\n",
    "des = pd.concat([des, fs08_cap.loc[fs08_cap['treat1']==1,'rbtamt_idmean'].describe()], axis=1, join='inner',names=['cont1','treat1'])\n",
    "des = pd.concat([des, fs08_cap.loc[fs08['rbtamt']>0,'rbtamt'].describe()], axis=1, join='inner',names=['cont1','treat1','treat1'])\n",
    "tuples = [('rbtamt_mean', 'cont1'), ('rbtamt_mean', 'treat1'), ('rbtamt', 'treat1')]\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples)\n",
    "print(des.stack().unstack(level=0).stack(level=0).to_latex(float_format=\"{:,.0f}\".format,  columns=['count','mean','max','min','25%','75%','std'])) #table 3\n",
    "#print(des1.loc[['count', 'mean', 'std', 'min','50%','max'],:].to_latex(float_format=\"{:,.1f}\".format))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Descriptives for dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{FD} & \\multicolumn{2}{c}{SND} & \\multicolumn{2}{c}{ND} & \\multicolumn{2}{c}{TOT} \\\\\n",
      "{} &  treat1 &    cont1 &   treat1 &    cont1 &   treat1 &    cont1 &   treat1 &    cont1 \\\\\n",
      "\\midrule\n",
      "count & 4,028.0 & 11,195.0 &  4,028.0 & 11,195.0 &  4,028.0 & 11,195.0 &  4,028.0 & 11,195.0 \\\\\n",
      "mean  & 1,867.7 &  1,795.1 &  4,264.4 &  4,010.1 &  5,318.6 &  5,082.2 &  9,432.4 &  9,119.5 \\\\\n",
      "std   &   981.4 &    943.9 &  1,999.9 &  1,871.9 &  2,532.5 &  2,392.4 &  4,766.9 &  4,645.6 \\\\\n",
      "min   &    65.0 &     26.0 &    250.0 &    133.0 &    372.0 &    444.0 &    372.0 &    747.0 \\\\\n",
      "25\\%   & 1,154.0 &  1,105.0 &  2,768.8 &  2,620.0 &  3,386.8 &  3,290.7 &  5,807.2 &  5,696.3 \\\\\n",
      "50\\%   & 1,695.0 &  1,635.0 &  4,002.5 &  3,736.0 &  4,960.0 &  4,712.0 &  8,568.3 &  8,248.4 \\\\\n",
      "75\\%   & 2,425.0 &  2,320.0 &  5,485.0 &  5,119.0 &  6,844.9 &  6,480.8 & 12,243.1 & 11,653.0 \\\\\n",
      "max   & 5,580.0 &  5,617.0 & 11,881.5 & 11,803.3 & 14,759.7 & 14,799.7 & 31,505.9 & 31,664.5 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "treat = ['treat1','treat2','treat3']\n",
    "cont = ['cont1','cont2','cont3','cont4']\n",
    "#baseline:\n",
    "CONS=['FD','SND','ND','TOT']\n",
    "treat = ['treat1']\n",
    "cont = ['cont1']\n",
    "\n",
    "for i in CONS:\n",
    "    if i=='FD':\n",
    "        des = fs08_cap.loc[fs08_cap['treat1']==1,i].describe()\n",
    "        des = pd.concat([des,fs08_cap.loc[fs08_cap['cont1']==1,i].describe()],join='inner',axis=1)\n",
    "    else:\n",
    "        for g in ['treat1','cont1']:\n",
    "            des = pd.concat([des, fs08_cap.loc[fs08_cap[g]==1,i].describe()],join='inner', axis=1)\n",
    "\n",
    "index1 = ['FD']*2 + ['SND']*2 + ['ND']*2 + ['TOT']*2\n",
    "index2 = ['treat1','cont1']*8\n",
    "tuples = list(zip(index1,index2))\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples)\n",
    "print(des.to_latex(float_format=\"{:,.1f}\".format ,multicolumn_format='c')) #table 4\n",
    "#des.stack().unstack(level=0).stack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare histograms of treatment group with different control groups\n",
    "\n",
    "for i in range(len(CONS)):\n",
    "    for t in treat[0:1]:\n",
    "        for c in cont[0:1]:\n",
    "            plt.figure(figsize=(3.2,2.4))\n",
    "            plt.title(f'{CONS[i]}')\n",
    "            plt.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            #plt.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, density=True, bins=30, color='red')\n",
    "            plt.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            #plt.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= t, density=True, bins=30, color='green')\n",
    "            #plt.set_title(f'{CONS[i]}:')\n",
    "            #plt.tight_layout()\n",
    "            plt.xticks(fontsize = 6)\n",
    "            plt.yticks(fontsize=6)\n",
    "            plt.legend(loc='upper right', frameon=True, fontsize=6)\n",
    "            plt.savefig(os.getcwd() + f'\\\\descriptives\\\\{CONS[i]}_pattern.pdf')\n",
    "            plt.close()\n",
    "CONS=['SND','TOT']\n",
    "fig=plt.figure(figsize=(6.4,2.4))\n",
    "for i in range(len(CONS)):\n",
    "    #plt.figure(figsize=(6.4,2.4))\n",
    "    plot = plt.subplot(1,2,i+1)\n",
    "    for t in treat[0:1]:\n",
    "        for c in cont[0:1]:     \n",
    "            #plot = plt.figure(figsize=(10,3))\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            plot.set_title(f'{CONS[i]}:')\n",
    "            plt.tight_layout()\n",
    "            plt.xticks(fontsize = 8)\n",
    "            plt.yticks(fontsize=8)\n",
    "            plt.legend(loc='upper right', frameon=False, fontsize=8)\n",
    "            #plot_count = plot_count+1\n",
    "            plt.savefig(os.getcwd() + f'\\\\descriptives\\\\SND_TOT_pattern_group1_treat1.pdf') #figure 1\n",
    "plt.close()\n",
    "\n",
    "\n",
    "CONS=['FD','SND','ND','TOT']        \n",
    "plot_count = 1\n",
    "for i in range(len(CONS)):\n",
    "    #plt.figure(figsize=(6.4,2.4))\n",
    "    for t in treat[0:1]:\n",
    "        fig=plt.figure(figsize=(6.4,2.4))\n",
    "        for c in cont[0:3]:     \n",
    "            #plot = plt.figure(figsize=(10,3))\n",
    "            plot = plt.subplot(1,3,cont.index(c)+1)\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            plot.set_title(f'{CONS[i]}:')\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc='upper right', frameon=False)\n",
    "            plot_count = plot_count+1\n",
    "            #plt.savefig(os.getcwd() + f'\\\\descriptives\\\\{CONS[i]}_pattern_groupcomp.pdf')\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finassets_it</th>\n",
       "      <th>finassets</th>\n",
       "      <th>qblncm1x_sum</th>\n",
       "      <th>FINCBTXM</th>\n",
       "      <th>age</th>\n",
       "      <th>CUTENURE_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finassets_it</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039973</td>\n",
       "      <td>0.297345</td>\n",
       "      <td>0.220370</td>\n",
       "      <td>0.009109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finassets</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032776</td>\n",
       "      <td>0.140357</td>\n",
       "      <td>0.156547</td>\n",
       "      <td>-0.023834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qblncm1x_sum</th>\n",
       "      <td>0.039973</td>\n",
       "      <td>0.032776</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.366562</td>\n",
       "      <td>-0.214654</td>\n",
       "      <td>0.611398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FINCBTXM</th>\n",
       "      <td>0.297345</td>\n",
       "      <td>0.140357</td>\n",
       "      <td>0.366562</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.151055</td>\n",
       "      <td>0.347557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.220370</td>\n",
       "      <td>0.156547</td>\n",
       "      <td>-0.214654</td>\n",
       "      <td>-0.151055</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.196864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUTENURE_1</th>\n",
       "      <td>0.009109</td>\n",
       "      <td>-0.023834</td>\n",
       "      <td>0.611398</td>\n",
       "      <td>0.347557</td>\n",
       "      <td>-0.196864</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              finassets_it  finassets  qblncm1x_sum  FINCBTXM       age  \\\n",
       "finassets_it      1.000000   1.000000      0.039973  0.297345  0.220370   \n",
       "finassets         1.000000   1.000000      0.032776  0.140357  0.156547   \n",
       "qblncm1x_sum      0.039973   0.032776      1.000000  0.366562 -0.214654   \n",
       "FINCBTXM          0.297345   0.140357      0.366562  1.000000 -0.151055   \n",
       "age               0.220370   0.156547     -0.214654 -0.151055  1.000000   \n",
       "CUTENURE_1        0.009109  -0.023834      0.611398  0.347557 -0.196864   \n",
       "\n",
       "              CUTENURE_1  \n",
       "finassets_it    0.009109  \n",
       "finassets      -0.023834  \n",
       "qblncm1x_sum    0.611398  \n",
       "FINCBTXM        0.347557  \n",
       "age            -0.196864  \n",
       "CUTENURE_1      1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finassets_it</th>\n",
       "      <th>finassets</th>\n",
       "      <th>qblncm1x_sum</th>\n",
       "      <th>FINCBTXM</th>\n",
       "      <th>age</th>\n",
       "      <th>CUTENURE_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finassets_it</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.103821</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.067753</td>\n",
       "      <td>0.070930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finassets</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.177962</td>\n",
       "      <td>0.269407</td>\n",
       "      <td>-0.005001</td>\n",
       "      <td>0.192747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qblncm1x_sum</th>\n",
       "      <td>0.103821</td>\n",
       "      <td>0.177962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.189569</td>\n",
       "      <td>0.171876</td>\n",
       "      <td>0.676245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FINCBTXM</th>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.269407</td>\n",
       "      <td>0.189569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078989</td>\n",
       "      <td>0.194902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.067753</td>\n",
       "      <td>-0.005001</td>\n",
       "      <td>0.171876</td>\n",
       "      <td>0.078989</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.208625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUTENURE_1</th>\n",
       "      <td>0.070930</td>\n",
       "      <td>0.192747</td>\n",
       "      <td>0.676245</td>\n",
       "      <td>0.194902</td>\n",
       "      <td>0.208625</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              finassets_it  finassets  qblncm1x_sum  FINCBTXM       age  \\\n",
       "finassets_it      1.000000   1.000000      0.103821  0.403993  0.067753   \n",
       "finassets         1.000000   1.000000      0.177962  0.269407 -0.005001   \n",
       "qblncm1x_sum      0.103821   0.177962      1.000000  0.189569  0.171876   \n",
       "FINCBTXM          0.403993   0.269407      0.189569  1.000000  0.078989   \n",
       "age               0.067753  -0.005001      0.171876  0.078989  1.000000   \n",
       "CUTENURE_1        0.070930   0.192747      0.676245  0.194902  0.208625   \n",
       "\n",
       "              CUTENURE_1  \n",
       "finassets_it    0.070930  \n",
       "finassets       0.192747  \n",
       "qblncm1x_sum    0.676245  \n",
       "FINCBTXM        0.194902  \n",
       "age             0.208625  \n",
       "CUTENURE_1      1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finassets_it</th>\n",
       "      <th>finassets</th>\n",
       "      <th>qblncm1x_sum</th>\n",
       "      <th>FINCBTXM</th>\n",
       "      <th>age</th>\n",
       "      <th>CUTENURE_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finassets_it</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024288</td>\n",
       "      <td>0.159287</td>\n",
       "      <td>0.138848</td>\n",
       "      <td>0.078261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finassets</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017513</td>\n",
       "      <td>-0.067504</td>\n",
       "      <td>0.198891</td>\n",
       "      <td>0.007436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qblncm1x_sum</th>\n",
       "      <td>0.024288</td>\n",
       "      <td>-0.017513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.153316</td>\n",
       "      <td>-0.081274</td>\n",
       "      <td>0.613950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FINCBTXM</th>\n",
       "      <td>0.159287</td>\n",
       "      <td>-0.067504</td>\n",
       "      <td>0.153316</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.098808</td>\n",
       "      <td>0.215017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.138848</td>\n",
       "      <td>0.198891</td>\n",
       "      <td>-0.081274</td>\n",
       "      <td>-0.098808</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.052226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUTENURE_1</th>\n",
       "      <td>0.078261</td>\n",
       "      <td>0.007436</td>\n",
       "      <td>0.613950</td>\n",
       "      <td>0.215017</td>\n",
       "      <td>-0.052226</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              finassets_it  finassets  qblncm1x_sum  FINCBTXM       age  \\\n",
       "finassets_it      1.000000   1.000000      0.024288  0.159287  0.138848   \n",
       "finassets         1.000000   1.000000     -0.017513 -0.067504  0.198891   \n",
       "qblncm1x_sum      0.024288  -0.017513      1.000000  0.153316 -0.081274   \n",
       "FINCBTXM          0.159287  -0.067504      0.153316  1.000000 -0.098808   \n",
       "age               0.138848   0.198891     -0.081274 -0.098808  1.000000   \n",
       "CUTENURE_1        0.078261   0.007436      0.613950  0.215017 -0.052226   \n",
       "\n",
       "              CUTENURE_1  \n",
       "finassets_it    0.078261  \n",
       "finassets       0.007436  \n",
       "qblncm1x_sum    0.613950  \n",
       "FINCBTXM        0.215017  \n",
       "age            -0.052226  \n",
       "CUTENURE_1      1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.scatter(fs08_cap['FINCBTXM'], fs08_cap['qblncm1x_sum'], s=10)\n",
    "#plt.scatter(fs08_cap['FINCBTXM'], fs08_cap['finassets_it'])\n",
    "#plt.scatter(fs08_cap['age'],fs08_cap['FINCBTXM'],  s=10)\n",
    "display(fs08_cap.loc[:,['finassets_it','finassets','qblncm1x_sum','FINCBTXM','age','CUTENURE_1']].corr())\n",
    "display(fs08_cap.loc[(fs08_cap['age']<=35) & (fs08_cap['FINCBTXM']<50000),['finassets_it','finassets','qblncm1x_sum','FINCBTXM','age','CUTENURE_1']].corr())\n",
    "display(fs08_cap.loc[(fs08_cap['age'].between(35,60)) & (fs08_cap['FINCBTXM']<50000),['finassets_it','finassets','qblncm1x_sum','FINCBTXM','age','CUTENURE_1']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "descriptives for rebate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: Machine learning approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Define sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Run random forest algorithm seperately for treatment and control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #'educ_bachelor','educ_master','educ_doctorate'\n",
    "DEMO = ['age', 'adults', 'PERSLT18','CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5', 'MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'] # exclude 'FINCBTAX',\n",
    "DEMO2 = [ 'FINCBTXM'] # exclude 'FSALARYM',\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum','qescrowx_sum', 'timeleft'] # 'orgmrtx_sum', \n",
    "CONS = ['FD', 'SND', 'ND', 'TOT']\n",
    "CONT = ['cont1', 'cont2', 'cont3']\n",
    "TREAT = 'treat1'\n",
    "treatgroup = TREAT\n",
    "trees = 1000\n",
    "\n",
    "#Random Forest for short term consumption: treatment group 1 with imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC + ['finassets_it']  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_finit.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_nofin.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group with just the observations where financial assets are included\n",
    "TREAT = 'treat1'\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + ['finassets'] + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1) & (fs08_cap['valid_finassets']==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[(fs08_cap[con]==1) & (fs08_cap['valid_finassets']==1) , [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_fin.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment with treatment group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(rfdicts)\n",
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #'educ_bachelor','educ_master','educ_doctorate'\n",
    "DEMO = ['age', 'adults', 'PERSLT18','CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5', 'MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'] # exclude 'FINCBTAX',\n",
    "DEMO2 = [ 'FINCBTXM'] # exclude 'FSALARYM',\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum','qescrowx_sum', 'timeleft'] # 'orgmrtx_sum', \n",
    "CONS = ['FD', 'SND', 'ND', 'TOT']\n",
    "TREAT = 'treat2'\n",
    "treatgroup = TREAT\n",
    "trees = 1000\n",
    "\n",
    "#include finassets only\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + ['finassets'] + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "\n",
    "for c in CONS:\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1) & (fs08_cap['valid_finassets']==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rfdicts[f'{c}_fin'][TREAT] = treat\n",
    "    y = np.array(rfdicts[f'{c}_fin'][TREAT][depvar]) #array for dependent variable\n",
    "    X = np.array(rfdicts[f'{c}_fin'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "    X_labels = list(rfdicts[f'{c}_fin'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_X_labels'] = X_labels #save in dict\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_rbtamt'] = np.array(rfdicts[f'{c}_fin'][TREAT]['rbtamt_idmean'])\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_fin.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[f'{c}_fin'], output)\n",
    "    output.close()\n",
    "    \n",
    "#Random Forest for short term consumption: treatment group without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "for c in CONS:\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rfdicts[f'{c}_nofin'][TREAT] = treat\n",
    "    y = np.array(rfdicts[f'{c}_nofin'][TREAT][depvar]) #array for dependent variable\n",
    "    X = np.array(rfdicts[f'{c}_nofin'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "    X_labels = list(rfdicts[f'{c}_nofin'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_X_labels'] = X_labels #save in dict\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_rbtamt'] = np.array(rfdicts[f'{c}_nofin'][TREAT]['rbtamt_idmean'])\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_nofin.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[f'{c}_nofin'], output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC + ['finassets_it']  #define explanatory variables + ['finassets_it'] \n",
    "for c in CONS:\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rfdicts[f'{c}_finit'][TREAT] = treat\n",
    "    y = np.array(rfdicts[f'{c}_finit'][TREAT][depvar]) #array for dependent variable\n",
    "    X = np.array(rfdicts[f'{c}_finit'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "    X_labels = list(rfdicts[f'{c}_finit'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_X_labels'] = X_labels #save in dict\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_rbtamt'] = np.array(rfdicts[f'{c}_finit'][TREAT]['rbtamt_idmean'])\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_finit.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[f'{c}_finit'], output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Predict Outcomes for overall consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_predicitons_rbt(rf_treat, rf_cont, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if (type(X_cont) is not np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_treat_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('X_treat_rbamt needs to have an array like structure')\n",
    "            else:\n",
    "                X_temp = X_treat.copy()\n",
    "                rbtamt_temp = X_treat_rbtamt.copy()\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_cont_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('if X_cont is specified, X_cont_rbamt needs to have an array like structure')\n",
    "            if sorted(feature_ids_treat)!=sorted(feature_ids_cont):\n",
    "                raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont')\n",
    "            elif (len(feature_ids_treat)==0) | (len(feature_ids_cont)==0):\n",
    "                raise ValueError(f'if X_treat and X_cont are specified, feature_ids must not be empty')\n",
    "            else:\n",
    "                #stack treatment and control groups on top of each other\n",
    "                X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                #stack rebate for each household on top of each other\n",
    "                rbtamt_temp = pd.concat([pd.DataFrame(X_treat_rbtamt), pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                X_labels = list(X.columns)\n",
    "                X_temp = np.array(X)\n",
    "                rbtamt_temp = np.array(rbtamt_temp)\n",
    "        else: \n",
    "            raise ValueError('X_treat does not have an array like structure')\n",
    "        y = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp)) #for each household predict consumption response\n",
    "        mpc = y/rbtamt_temp[:,0] #calculate fraction of rebate that was consumed\n",
    "        return y,mpc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate consumption response for each household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD_fin:\n",
      "cont2_treat1_y_pred (1479,)\n",
      "cont2_treat1_mpc_pred (1479,)\n",
      "cont1_treat1_y_pred (2181,)\n",
      "cont1_treat1_mpc_pred (2181,)\n",
      "cont3_treat1_y_pred (798,)\n",
      "cont3_treat1_mpc_pred (798,)\n",
      "cont2_treat2_y_pred (1471,)\n",
      "cont2_treat2_mpc_pred (1471,)\n",
      "cont3_treat2_y_pred (790,)\n",
      "cont3_treat2_mpc_pred (790,)\n",
      "FD_finit:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "FD_nofin:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "ND_fin:\n",
      "cont2_treat1_y_pred (1479,)\n",
      "cont2_treat1_mpc_pred (1479,)\n",
      "cont1_treat1_y_pred (2181,)\n",
      "cont1_treat1_mpc_pred (2181,)\n",
      "cont3_treat1_y_pred (798,)\n",
      "cont3_treat1_mpc_pred (798,)\n",
      "cont2_treat2_y_pred (1471,)\n",
      "cont2_treat2_mpc_pred (1471,)\n",
      "cont3_treat2_y_pred (790,)\n",
      "cont3_treat2_mpc_pred (790,)\n",
      "ND_finit:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "ND_nofin:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "SND_fin:\n",
      "cont2_treat1_y_pred (1479,)\n",
      "cont2_treat1_mpc_pred (1479,)\n",
      "cont1_treat1_y_pred (2181,)\n",
      "cont1_treat1_mpc_pred (2181,)\n",
      "cont3_treat1_y_pred (798,)\n",
      "cont3_treat1_mpc_pred (798,)\n",
      "cont2_treat2_y_pred (1471,)\n",
      "cont2_treat2_mpc_pred (1471,)\n",
      "cont3_treat2_y_pred (790,)\n",
      "cont3_treat2_mpc_pred (790,)\n",
      "SND_finit:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "SND_nofin:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "TOT_fin:\n",
      "cont2_treat1_y_pred (1479,)\n",
      "cont2_treat1_mpc_pred (1479,)\n",
      "cont1_treat1_y_pred (2181,)\n",
      "cont1_treat1_mpc_pred (2181,)\n",
      "cont3_treat1_y_pred (798,)\n",
      "cont3_treat1_mpc_pred (798,)\n",
      "cont2_treat2_y_pred (1471,)\n",
      "cont2_treat2_mpc_pred (1471,)\n",
      "cont3_treat2_y_pred (790,)\n",
      "cont3_treat2_mpc_pred (790,)\n",
      "TOT_finit:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "TOT_nofin:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n"
     ]
    }
   ],
   "source": [
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\condistr\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for t in treat:\n",
    "        for c in cont:\n",
    "            if (t=='treat2') & (c=='cont1'):\n",
    "                pass\n",
    "            else:\n",
    "                y,mpc = uplift_predicitons_rbt(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                              rfdicts[k][c+'_X'],rfdicts[k][c+'_rbtamt'],rfdicts[k][t+'_X_labels'],rfdicts[k][c+'_X_labels'])\n",
    "                rfdicts[k][f'{c}_{t}_y_pred'] = y\n",
    "                print(f'{c}_{t}_y_pred', y.shape)\n",
    "                rfdicts[k][f'{c}_{t}_mpc_pred'] = mpc\n",
    "                print(f'{c}_{t}_mpc_pred', mpc.shape)\n",
    "\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate basic histograms for the consumption response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD_fin\n",
      "FD_finit\n",
      "FD_nofin\n",
      "ND_fin\n",
      "ND_finit\n",
      "ND_nofin\n",
      "SND_fin\n",
      "SND_finit\n",
      "SND_nofin\n",
      "TOT_fin\n",
      "TOT_finit\n",
      "TOT_nofin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\condistr\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for t in treat:\n",
    "        for c in cont:\n",
    "            if (t=='treat2') & (c=='cont1'):\n",
    "                pass            \n",
    "            else:\n",
    "                y = rfdicts[k][f'{c}_{t}_y_pred']\n",
    "                mpc = rfdicts[k][f'{c}_{t}_mpc_pred']\n",
    "                plt.hist(y, bins=40,  edgecolor='black')\n",
    "                lower = round((min(y)/100),1)*100\n",
    "                upper = round((max(y)/100),1)*100+1\n",
    "                ticks = int((upper-lower)/4)\n",
    "                plt.xticks(np.arange(lower, upper, ticks))\n",
    "                plt.title(f'Pred cons resp distr., {vartype},{c},{t}')\n",
    "                plt.xlabel(f'consumption in {cons}, number of observations {y.shape[0]}')\n",
    "                plt.axvline(np.median(y), color='red', linestyle='dashed', linewidth=1)\n",
    "                min_ylim, max_ylim = plt.ylim()\n",
    "                plt.text(np.median(y)*2, max_ylim*0.9, 'Median: {:.2f}'.format(np.median(y)),fontsize=8)\n",
    "                plt.ylabel(f'number of individuals in bin')\n",
    "                plt.savefig(newpath + f'\\\\{vartype}_{c}_{t}_y_pred.pdf')\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot consumption responses in a subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONS = ['FD','SND','ND','TOT']\n",
    "fintype = ['nofin','finit','fin']\n",
    "rfdicts_keys = list(rfdicts)\n",
    "for f in fintype:\n",
    "    rfdicts_subplot = [k for k in rfdicts_keys if k.split('_')[0] in CONS if k.split('_')[1]==f]\n",
    "\n",
    "    fig=plt.figure(figsize=(12,6))\n",
    "\n",
    "    for i,k in enumerate(rfdicts_subplot):\n",
    "        c = 'cont1'\n",
    "        t= 'treat1'\n",
    "        plot = plt.subplot(2,2,i+1)    \n",
    "        y = rfdicts[k][f'{c}_{t}_y_pred']\n",
    "        plot.hist(y, bins=40,  edgecolor='black')\n",
    "        lower = round((min(y)/100),1)*100\n",
    "        upper = round((max(y)/100),1)*100+1\n",
    "        ticks = int((upper-lower)/4)\n",
    "        plt.xticks(np.arange(lower, upper, ticks), fontsize = 8)\n",
    "        plt.axvline(np.median(y), color='red', linestyle='dashed', linewidth=1)\n",
    "        min_ylim, max_ylim = plt.ylim()\n",
    "        plt.text(np.median(y)*2, max_ylim*0.9, 'Median: {:.2f}'.format(np.median(y)),fontsize=8)\n",
    "        plt.title(k.split('_')[0])\n",
    "        plt.tight_layout()\n",
    "        plt.yticks(fontsize=8)\n",
    "        #if i == 2:\n",
    "            #plt.ylabel('households in bin')\n",
    "        #    plt.xlabel('predicted consumption response')\n",
    "        #plt.savefig(os.getcwd() + f'\\\\condistr\\\\{CONS[0]}_{CONS[1]}_{c}_{t}_{f}.pdf') #figure 1\n",
    "        plt.savefig(os.getcwd() + f'\\\\condistr\\\\allcons_{c}_{t}_{f}.pdf') #figure 1\n",
    "    #fig.suptitle(f'Predicted consumption response')\n",
    "    plt.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate tables for consumption responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD\n",
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{FD} & \\multicolumn{2}{c}{SND} & \\multicolumn{2}{c}{ND} & \\multicolumn{2}{c}{TOT} \\\\\n",
      "{} &    finit &    nofin &    finit &    nofin &    finit &    nofin &    finit &    nofin \\\\\n",
      "\\midrule\n",
      "count & 15,223.0 & 15,223.0 & 15,223.0 & 15,223.0 & 15,223.0 & 15,223.0 & 15,223.0 & 15,223.0 \\\\\n",
      "mean  &     72.4 &     77.3 &    255.0 &    272.3 &    226.9 &    267.9 &    272.1 &    335.8 \\\\\n",
      "std   &    241.3 &    236.8 &    436.2 &    436.1 &    554.9 &    556.5 &    972.4 &  1,000.9 \\\\\n",
      "min   & -1,026.0 &   -948.7 & -1,896.1 & -2,308.9 & -2,594.0 & -2,289.4 & -4,635.1 & -4,571.5 \\\\\n",
      "25\\%   &    -69.3 &    -60.5 &     -3.4 &     14.4 &    -94.6 &    -61.8 &   -255.1 &   -221.0 \\\\\n",
      "50\\%   &     74.2 &     79.8 &    249.6 &    266.0 &    223.2 &    268.3 &    292.6 &    359.5 \\\\\n",
      "75\\%   &    219.3 &    216.2 &    517.9 &    534.8 &    556.8 &    597.3 &    821.4 &    920.1 \\\\\n",
      "max   &  1,320.2 &  1,250.9 &  2,063.2 &  2,130.1 &  2,560.0 &  2,506.2 &  5,148.6 &  5,650.8 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{FD} & \\multicolumn{2}{c}{ND} & \\multicolumn{2}{c}{SND} & \\multicolumn{2}{c}{TOT} \\\\\n",
      "{} &   finit &   nofin &   finit &   nofin &   finit &   nofin &   finit &    nofin \\\\\n",
      "\\midrule\n",
      "mean &   0.119 &   0.129 &   0.404 &   0.485 &   0.426 &   0.463 &   0.520 &    0.645 \\\\\n",
      "std  &   1.288 &   1.465 &   3.892 &   5.098 &   3.711 &   4.396 &   5.343 &    6.430 \\\\\n",
      "min  & -26.505 & -24.948 & -46.663 & -56.494 & -41.236 & -45.904 & -97.500 & -102.254 \\\\\n",
      "25\\%  &  -0.076 &  -0.067 &  -0.106 &  -0.069 &  -0.004 &   0.016 &  -0.285 &   -0.237 \\\\\n",
      "50\\%  &   0.085 &   0.093 &   0.257 &   0.313 &   0.286 &   0.310 &   0.338 &    0.422 \\\\\n",
      "75\\%  &   0.275 &   0.273 &   0.687 &   0.743 &   0.642 &   0.660 &   1.038 &    1.152 \\\\\n",
      "max  &  77.694 &  86.404 & 238.843 & 303.299 & 235.839 & 277.846 & 462.083 &  517.390 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('FD', 'finit'),\n",
       " ('FD', 'nofin'),\n",
       " ('ND', 'finit'),\n",
       " ('ND', 'nofin'),\n",
       " ('SND', 'finit'),\n",
       " ('SND', 'nofin'),\n",
       " ('TOT', 'finit'),\n",
       " ('TOT', 'nofin')]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fintype = ['nofin','finit','fin']\n",
    "rfdicts_keys = list(rfdicts)\n",
    "list(rfdicts[rfdicts_keys[0]])\n",
    "cons = list(set([k.split('_')[0] for k in rfdicts_keys]))\n",
    "spec = list(set([k.split('_')[0] for k in rfdicts_keys]))\n",
    "c = 'cont1'\n",
    "t = 'treat1'\n",
    "rfdicts_subkeys = [k for k in rfdicts_keys if k.split('_')[1]!='fin' ]\n",
    "\n",
    "for i,k in enumerate(rfdicts_subkeys):\n",
    "    cr = rfdicts[k][f'{c}_{t}_y_pred']\n",
    "    mpc = rfdicts[k][f'{c}_{t}_mpc_pred']\n",
    "    if i==0:\n",
    "        print(k.split('_')[0])\n",
    "        cr_pred = pd.DataFrame(cr, columns=['cr_'+k.split('_')[0]+'_'+k.split('_')[1]]).describe()\n",
    "        mpc_pred = pd.DataFrame(mpc, columns=['mpc_'+k.split('_')[0]+'_'+k.split('_')[1]]).describe()\n",
    "    else:\n",
    "        cr_pred = cr_pred.join(pd.DataFrame(cr, columns=['cr_'+k.split('_')[0]+'_'+k.split('_')[1]]).describe())\n",
    "        #display(cr_pred)\n",
    "        mpc_pred = mpc_pred.join(pd.DataFrame(mpc, columns=['mpc_'+k.split('_')[0]+'_'+k.split('_')[1]]).describe())\n",
    "        #display(mpc_pred)\n",
    "\n",
    "index1 = [i.split('_')[1] for i in list(mpc_pred)]\n",
    "index2 = [i.split('_')[2] for i in list(mpc_pred)]\n",
    "tuples = list(zip(index1,index2))\n",
    "mpc_pred.columns = pd.MultiIndex.from_tuples(tuples) \n",
    "index1 = [i.split('_')[1] for i in list(cr_pred)]\n",
    "index2 = [i.split('_')[2] for i in list(cr_pred)]\n",
    "tuples = list(zip(index1,index2))\n",
    "cr_pred.columns = pd.MultiIndex.from_tuples(tuples) \n",
    "\n",
    "\n",
    "cr_pred\n",
    "mpc_pred\n",
    "#tuples = sorted(tuples, key = lambda x: x[0], reverse = False)\n",
    "tuples = tuples[:2] + tuples[4:6] + tuples[2:4] + tuples[6:]\n",
    "\n",
    "print(cr_pred.to_latex(float_format=\"{:,.1f}\".format,columns = tuples, multicolumn_format='c'))\n",
    "print(mpc_pred.iloc[1:].to_latex(float_format=\"{:,.3f}\".format,multicolumn_format='c'))#table1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** Variable importance plot for treatment and control group separately and as a weighted sum for the whole sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfdicts_keys = list(rfdicts)\n",
    "#print(rfdicts_keys)\n",
    "#print(list(rfdicts[rfdicts_keys[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "FD_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "FD_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "FD_nofin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "ND_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "ND_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "ND_nofin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "SND_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "SND_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "SND_nofin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "TOT_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "TOT_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "TOT_nofin\n"
     ]
    }
   ],
   "source": [
    "#def vimp_plot_uplift()\n",
    "rfdicts_keys = list(rfdicts)\n",
    "uplift_imp_subplot = dict()\n",
    "for k in rfdicts_keys:\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\varimp\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    print(treat+cont)\n",
    "    uplift_imp = dict()\n",
    "    for i in treat+cont:\n",
    "        importances = (rfdicts[k][i+'_rf'].feature_importances_)\n",
    "        X_importances = [(label, importance) for label, importance in zip(rfdicts[k][i+'_X_labels'],importances)]\n",
    "        #X_importances = [(round(importance,2), label) for importance, label in zip(importances, rf[i+'_X_labels'])]\n",
    "        X_importances = sorted(X_importances, key = lambda x:x[1], reverse = False)\n",
    "        uplift_imp[i+'_varimp_values'] = [x[1] for x in X_importances]\n",
    "        uplift_imp[i+'_varimp_labels'] = [x[0] for x in X_importances]\n",
    "        uplift_imp[i+'_varimp_tuples'] = X_importances\n",
    "        \n",
    "    \n",
    "    for i in treat + cont:\n",
    "        X_importances = sorted(X_importances, key = lambda x:x[0].upper(), reverse = False) #sort in alphabetical order\n",
    "        uplift_imp[i+'_values'] = [x[1] for x in X_importances] #importances \n",
    "        uplift_imp[i+'_labels'] = [x[0] for x in X_importances] \n",
    "        shape = rfdicts[k][i+'_X'].shape\n",
    "        uplift_imp[i+'_sample'] = shape[0] \n",
    "    \n",
    "    for t in treat:\n",
    "        plotgroups = [t]\n",
    "        for c in cont:\n",
    "            plotgroups = plotgroups + [c] + [c+'_'+t]\n",
    "            uplift_imp[f'{c}_{t}_sample'] = uplift_imp[f'{t}_sample'] + uplift_imp[f'{c}_sample'] \n",
    "            uplift_imp[f'{c}_{t}_varimp_values'] = [uplift_imp[f'{t}_sample']/(uplift_imp[f'{c}_{t}_sample'])*uplift_imp[f'{t}_values'][i] + \n",
    "            uplift_imp[f'{c}_sample']/(uplift_imp[f'{c}_{t}_sample'])*uplift_imp[f'{c}_values'][i] for i in range(len(uplift_imp[f'{t}_values']))]\n",
    "\n",
    "            up_importances = [(label, importance) for label, importance in zip(uplift_imp[f'{t}_labels'],uplift_imp[f'{c}_{t}_varimp_values'])]\n",
    "            up_importances = sorted(up_importances, key = lambda x:x[1], reverse = False)\n",
    "            uplift_imp[f'{c}_{t}_varimp_tuples'] = up_importances            \n",
    "            uplift_imp[f'{c}_{t}_varimp_values'] = [up[1] for up in up_importances]\n",
    "            uplift_imp[f'{c}_{t}_varimp_labels'] = [up[0] for up in up_importances]\n",
    "            uplift_imp_subplot[f'{cons}_{c}_{t}_varimp_values_{vartype}'] = uplift_imp[f'{c}_{t}_varimp_values']\n",
    "            uplift_imp_subplot[f'{cons}_{c}_{t}_varimp_labels_{vartype}'] = uplift_imp[f'{c}_{t}_varimp_labels']\n",
    "        print(k)   \n",
    "    for g in plotgroups:\n",
    "        freq_series = pd.Series(uplift_imp[g+'_varimp_values'])\n",
    "        y_labels = uplift_imp[g+'_varimp_labels']\n",
    "\n",
    "        # Plot the figure.\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        ax = freq_series.plot(kind='barh')\n",
    "        ssize=uplift_imp[g+'_sample']\n",
    "        #ax.set_title(f'{k},{g.upper()}')\n",
    "        #ax.set_xlabel(f'sample size = {str(ssize)}')\n",
    "        ax.set_ylabel(f'Variable')\n",
    "        ax.set_yticklabels(y_labels)\n",
    "        #ax.set_xlim(-40, 300) # expand xlim to make labels easier to read\n",
    "\n",
    "        rects = ax.patches\n",
    "\n",
    "        # For each bar: Place a label\n",
    "        for rect in rects:\n",
    "            # Get X and Y placement of label from rect.\n",
    "            x_value = rect.get_width()\n",
    "            y_value = rect.get_y() + rect.get_height() / 2\n",
    "\n",
    "            # Number of points between bar and label. Change to your liking.\n",
    "            space = 3\n",
    "            # Vertical alignment for positive values\n",
    "            ha = 'left'\n",
    "\n",
    "            # Use X value as label and format number with one decimal place\n",
    "            label = \"{:.3f}\".format(x_value)\n",
    "\n",
    "            # Create annotation\n",
    "            plt.annotate(\n",
    "                label,                      # Use `label` as label\n",
    "                (x_value, y_value),         # Place label at end of the bar\n",
    "                xytext=(space, 0),          # Horizontally shift label by `space`\n",
    "                textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "                va='center',                # Vertically center label\n",
    "                ha=ha)                      # Horizontally align label differently for\n",
    "                                            # positive and negative values.\n",
    "        plt.savefig(f'{newpath}\\\\{cons}_{vartype}_{g}.pdf')\n",
    "        plt.close()\n",
    "        #plt.savefig(\"image.png\")\n",
    "        #plt.savefig(newpath + '\\\\'+ pathend +f'_{i}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(uplift_imp_subplot))\n",
    "consgroups = ['FD','SND'],['ND','TOT']\n",
    "j=1\n",
    "for consg in consgroups:\n",
    "    for i,cons in enumerate(consg): #,'ND','TOT'\n",
    "        if i == 0:\n",
    "            plotsubgroups = [cons+'_cont1_treat1']\n",
    "        else:\n",
    "            plotsubgroups = plotsubgroups + [cons+'_cont1_treat1']\n",
    "    plotsubgroups = sorted(plotsubgroups) \n",
    "    plotsubgroups\n",
    "    #plotsubgroups = plotsubgroups[:2] + plotsubgroups[4:6] + plotsubgroups[2:4] + plotsubgroups[6:]    \n",
    "    varimpvalues = [p + '_varimp_values_nofin' for p in plotsubgroups] + [p + '_varimp_values_finit' for p in plotsubgroups]\n",
    "    varimpvalues = sorted(varimpvalues)\n",
    "    varimpvalues = varimpvalues[:2] + varimpvalues[4:6] + varimpvalues[2:4] + varimpvalues[6:] \n",
    "\n",
    "    varimplabels = [p + '_varimp_labels_nofin' for p in plotsubgroups] + [p + '_varimp_labels_finit' for p in plotsubgroups]\n",
    "    varimplabels = sorted(varimplabels)\n",
    "    varimplabels = varimplabels[:2] + varimplabels[4:6] + varimplabels[2:4] + varimplabels[6:] \n",
    "    #print(varimpvalues)\n",
    "    #print(varimplabels)\n",
    "    #\n",
    "    # \n",
    "    plotsubgroups\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    i=0\n",
    "    for v in range(len(varimpvalues)):\n",
    "        plot = plt.subplot(len(plotsubgroups),2,i+1) \n",
    "        freq_series = pd.Series(uplift_imp_subplot[varimpvalues[v]])\n",
    "        y_labels = uplift_imp_subplot[varimplabels[v]]\n",
    "        # Plot the figure.\n",
    "\n",
    "        ax = freq_series.plot(kind='barh')\n",
    "        ax.set_title(varimpvalues[i].split('_')[0], fontsize = 14)\n",
    "        ax.set_yticklabels(y_labels, fontsize = 14)\n",
    "        #ax.set_xlim(-40, 300) # expand xlim to make labels easier to read\n",
    "\n",
    "        rects = ax.patches\n",
    "        i = i+1\n",
    "        #print(i)\n",
    "        # For each bar: Place a label\n",
    "        for rect in rects:\n",
    "            # Get X and Y placement of label from rect.\n",
    "            x_value = rect.get_width()\n",
    "            y_value = rect.get_y() + rect.get_height() / 2\n",
    "\n",
    "            # Number of points between bar and label. Change to your liking.\n",
    "            space = 3\n",
    "            # Vertical alignment for positive values\n",
    "            ha = 'left'\n",
    "\n",
    "            # Use X value as label and format number with one decimal place\n",
    "            label = \"{:.3f}\".format(x_value)\n",
    "\n",
    "            # Create annotation\n",
    "            plt.annotate(\n",
    "                label,                      # Use `label` as label\n",
    "                (x_value, y_value),         # Place label at end of the bar\n",
    "                xytext=(space, 0),          # Horizontally shift label by `space`\n",
    "                textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "                va='center',                # Vertically center label\n",
    "                ha=ha, fontsize =14)                      # Horizontally align label differently for\n",
    "                                            # positive and negative values.\n",
    "        #plt.savefig(f'{newpath}\\\\{vartype}_{g}.pdf')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.getcwd()+f'\\\\varimp\\\\varimp_subplot_{j}.pdf')\n",
    "    plt.close()\n",
    "    j = j+1\n",
    "    #    #plt.savefig(\"image.png\")\n",
    "    #    #plt.savefig(newpath + '\\\\'+ pathend +f'_{i}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Partial dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6.2** Function for uplift 2model partial dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def uplift_num_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[], types=['mean'], percentile='none', grid_lower=5, grid_upper=95 ): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor (or multiple predictors) in a random forest uplift 2 model approach.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    y_pred: corresponding predicted values of dependent variable (type: ndarray). \n",
    "            If input types is a list the columns in the array correspond to the chosen types in the same order\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is int):\n",
    "                if f_id > (X_temp.shape[1]-1):\n",
    "                    raise ValueError(f'positional number of {f_id} exceeds array shape')\n",
    "                else:\n",
    "                    X_temp = X_treat.copy()\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str):\n",
    "                if f_id not in feature_ids:\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids is not passed to the function')\n",
    "                else:\n",
    "                    f_id = feature_ids.index(f_id)\n",
    "                    f_id_label = f_id\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('f_id needs to be either an integer or a string')\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(f_id) is int):\n",
    "                raise ValueError(f'if X_cont is specified, then f_id needs to be a string variable')\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str) & ((f_id not in feature_ids_treat) |  (f_id not in feature_ids_cont)):\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids_treat or feature_ids_cont is not passed to the function')\n",
    "            else:\n",
    "                if (sorted(feature_ids_treat)!=sorted(feature_ids_cont)) & (type(X_cont_rbtamt) is not np.ndarray) :\n",
    "                    raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                else:\n",
    "                    X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = np.array(mean_rbt)\n",
    "                    X_labels = list(X.columns)\n",
    "                    X_temp = np.array(X)\n",
    "                    f_id_label = f_id\n",
    "                    f_id = X_labels.index(f_id)             \n",
    "        else: \n",
    "            raise ValueError('Either X_cont or X_treat does not have an array like structure')\n",
    "\n",
    "                       #['age', 'adults', 'PERSLT18'\n",
    "        X_unique = np.array(list(set(X_temp[:, f_id])))\n",
    "        if len(X_unique)*3 > 100:\n",
    "            grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower), np.percentile(X_temp[:, f_id], grid_upper), 100)\n",
    "        else:\n",
    "            grid = np.linspace(np.percentile(X_temp[:, f_id],grid_lower),np.percentile(X_temp[:, f_id],grid_upper), len(X_unique)*2)\n",
    "       \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types.copy()\n",
    "        #column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "\n",
    "        if set(types) <= set(function_labels):\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i]\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif type(percentile) is not list: # 0<=percentile<=100:\n",
    "                    raise ValueError('percentile must be list')\n",
    "                else:\n",
    "                    column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "                    if len(percentile)>1:\n",
    "                        for p in percentile[1:]:\n",
    "                            types = types + ['percentile'] #pass\n",
    "                            nptypes = nptypes + [np.percentile]\n",
    "                            column_labels = column_labels + ['percentile_' +str(p)]\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "        y_pred = np.zeros((len(grid),len(types)))\n",
    "        mpc_pred = np.zeros((len(grid), len(types)))\n",
    "        p_pos = 0\n",
    "        for i, val in enumerate(grid): # i returns the counter, val returns the value at position of counter on grid \n",
    "            X_temp[:, f_id] = val\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            p_pos = 0\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[i,j] = nptypes[j](y_temp,percentile[p_pos])\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp,percentile[p_pos])\n",
    "                    p_pos = p_pos + 1\n",
    "                else:\n",
    "                    y_pred[i,j] = nptypes[j](y_temp)\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp)\n",
    "        for j in range(len(column_labels)):\n",
    "            if column_labels[j] == 'percentile':\n",
    "                column_labels[j] = 'percentile_' + str(percentile)\n",
    "            else:\n",
    "                pass \n",
    "        column_labels_cr = ['cr_'+ lab for lab in column_labels]\n",
    "        column_labels_mpc = ['mpc_' + lab for lab in column_labels]\n",
    "        column_labels = ['grid']+column_labels_cr+column_labels_mpc\n",
    "        column_labels = [str(f_id_label)+ '_' + lab for lab in column_labels]\n",
    "        df = pd.DataFrame(np.c_[grid, y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return df #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for partial dependency of categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def uplift_cat_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, feature_ids_treat, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[],  feature_ids_cont=[], types=['mean'], percentile='none'): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor (or multiple predictors) in a random forest uplift 2 model approach.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    y_pred: corresponding predicted values of dependent variable (type: ndarray). \n",
    "            If input types is a list the columns in the array correspond to the chosen types in the same order\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is list):\n",
    "                if (len(f_id)<2):\n",
    "                    raise ValueError(f'{f_id} must be a list of hot-encoding features a hence neither empty nor have a length of 1')\n",
    "                else:\n",
    "                    cat = dict()\n",
    "                    positions = []\n",
    "                    for f in f_id:\n",
    "                        if type(f) is not str:\n",
    "                            raise ValueError('features in f_id list must be variable names of string type')\n",
    "                        else:\n",
    "                            if f not in feature_ids_treat:\n",
    "                                raise ValueError(f'categorical variable {f_id} is not a varibale of data frame')                                     \n",
    "                            else:\n",
    "                                cat[f+'_id'] = feature_ids_treat.index(f)\n",
    "                                cat[f+'_id_label'] = f\n",
    "                                positions = positions + [cat[f+'_id']]\n",
    "                    f_tuple = list(zip(f_id,positions))\n",
    "                    f_tuple = sorted(f_tuple, key = lambda x:x[1], reverse = False)\n",
    "                    f_id = [i[0] for i in f_tuple]\n",
    "                    positions = [i[1] for i in f_tuple]\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('Either X_treat or f_id is not correctly specified')\n",
    "        elif (type(X_treat) is np.ndarray) & (type(X_treat) is np.ndarray) & (type(f_id) is list):\n",
    "            if (len(f_id)<2):\n",
    "                raise ValueError(f'{f_id} must be a list of hot-encoding features a hence neither empty nor have a length of 1')                            \n",
    "            else:\n",
    "                cat = dict()\n",
    "                positions = []\n",
    "                for f in f_id:\n",
    "                    if type(f) is not str:\n",
    "                        raise ValueError('features in f_id list must be variable names of string type')\n",
    "                    else:\n",
    "                        if (f not in feature_ids_treat) | (f not in feature_ids_cont):\n",
    "                            raise ValueError(f'categorical variable {f_id} is not a varibale of cont or treat data frame')                                     \n",
    "                        elif (sorted(feature_ids_treat)!=sorted(feature_ids_cont)) & (type(X_cont_rbtamt) is not np.ndarray):\n",
    "                            raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                        else:\n",
    "                            cat[f+'_id'] = feature_ids_treat.index(f)\n",
    "                            cat[f+'_id_label'] = f\n",
    "                            #cat[f+'_tuple'] = listzip\n",
    "                            positions = positions + [cat[f+'_id']]\n",
    "                f_tuple = list(zip(f_id,positions))\n",
    "                f_tuple = sorted(f_tuple, key = lambda x:x[1], reverse = False)\n",
    "                f_id = [i[0] for i in f_tuple]\n",
    "                positions = [i[1] for i in f_tuple]\n",
    "                X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                mean_rbt = np.array(mean_rbt)\n",
    "                X_labels = list(X.columns)\n",
    "                X_temp = np.array(X)\n",
    "        else:\n",
    "            raise ValueError('X_treat and X_cont (if specified) need to be array types, f_id has to be a list of hot-encoded categorical variables')\n",
    "        \n",
    "        for f in f_id:\n",
    "            print(np.max(X_temp[:,cat[f+'_id']]))\n",
    "            print(np.min(X_temp[:,cat[f+'_id']]))\n",
    "            if (np.max(X_temp[:,cat[f+'_id']])!=1) | (np.min(X_temp[:,cat[f+'_id']])!=0):\n",
    "                raise ValueError('hot encoded variable is not of binary classification')\n",
    "            else:\n",
    "                pass\n",
    "        #grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower),\n",
    "        #np.percentile(X_temp[:, f_id], grid_upper),\n",
    "        \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types.copy()\n",
    "        #column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "\n",
    "        if set(types) <= set(function_labels):\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i]\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif type(percentile) is not list: # 0<=percentile<=100:\n",
    "                    raise ValueError('percentile must be list')\n",
    "                else:\n",
    "                    column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "                    if len(percentile)>1:\n",
    "                        for p in percentile[1:]:\n",
    "                            types = types + ['percentile'] #pass\n",
    "                            nptypes = nptypes + [np.percentile]\n",
    "                            column_labels = column_labels + ['percentile_' +str(p)]\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "        #y_pred = np.zeros((len(grid),len(types)))\n",
    "        #mpc_pred = np.zeros((len(grid), len(types)))        \n",
    "        y_pred = np.zeros((1, len(types)*len(f_id)))\n",
    "        mpc_pred = np.zeros((1, len(types)*len(f_id)))\n",
    "        grid_row = np.identity(len(f_id))\n",
    "        k=0\n",
    "        \n",
    "        column_labels_cr=[]\n",
    "        column_labels_mpc=[]\n",
    "        for f in range(len(f_id)): # i returns the counter, val returns the value at position of counter on grid\n",
    "            p_pos = 0\n",
    "            A = np.array([list(grid_row[f]) for _ in range(len(X_temp))])\n",
    "            X_temp[:, positions] = A\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[0,k] = nptypes[j](y_temp,percentile[p_pos])\n",
    "                    mpc_pred[0,k] = nptypes[j](mean_rbt_temp,percentile[p_pos])\n",
    "                    column_labels_cr = column_labels_cr + ['cr_'+ f_id[f] +'_'+ types[j] + str(percentile[p_pos])]\n",
    "                    column_labels_mpc= column_labels_mpc + ['mpc_'+ f_id[f] +'_'+ types[j] + str(percentile[p_pos])]\n",
    "                    p_pos = p_pos+1\n",
    "                else:\n",
    "                    y_pred[0,k] = nptypes[j](y_temp)\n",
    "                    mpc_pred[0,k] = nptypes[j](mean_rbt_temp)\n",
    "                    column_labels_cr = column_labels_cr + ['cr_'+ f_id[f] +'_'+ types[j]]\n",
    "                    column_labels_mpc= column_labels_mpc + ['mpc_'+ f_id[f] +'_'+ types[j]]\n",
    "                k = k+1\n",
    "        column_labels = column_labels_cr + column_labels_mpc\n",
    "        df = pd.DataFrame(np.c_[y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return df #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_id = ['educ_nodegree', 'educ_highschool','educ_higher']\n",
    "#display(uplift_cat_2m_partial_dependency_mpc(rfdicts['DUR_fin_baseline']['treat1_rf'], rfdicts['DUR_fin_baseline']['cont1_rf'], f_id, rfdicts['DUR_fin_baseline']['cont1_X_labels'], rfdicts['DUR_fin_baseline']['treat1_X'], rfdicts['DUR_fin_baseline']['treat1_rbtamt'], X_cont=rfdicts['DUR_fin_baseline']['cont1_X'], X_cont_rbtamt=rfdicts['DUR_fin_baseline']['cont1_rbtamt'],  feature_ids_cont=rfdicts['DUR_fin_baseline']['treat1_X_labels'], types=['mean','percentile','std'],percentile=[25,75]))\n",
    "#f_id = ['educ_highschool','educ_higher','educ_nodegree']\n",
    "#display(uplift_cat_2m_partial_dependency_mpc(rfdicts['TOT_fin']['treat1_rf'], rfdicts['TOT_fin']['cont1_rf'], f_id, rfdicts['TOT_fin']['cont1_X_labels'], rfdicts['TOT_fin']['treat1_X'], rfdicts['TOT_fin']['treat1_rbtamt'], X_cont=rfdicts['TOT_fin']['cont1_X'], X_cont_rbtamt=rfdicts['TOT_fin']['cont1_rbtamt'],  feature_ids_cont=rfdicts['TOT_fin']['treat1_X_labels'], types=['mean','std'])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run partial dependence function for given sample and explanatory variables. this may take a while. Hence, save as later as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['treat1', 'cont1', 'cont2', 'cont3', 'treat1_X', 'treat1_X_labels', 'treat1_rbtamt', 'treat1_rf', 'cont1_X', 'cont1_X_labels', 'cont1_rbtamt', 'cont1_rf', 'cont2_X', 'cont2_X_labels', 'cont2_rbtamt', 'cont2_rf', 'cont3_X', 'cont3_X_labels', 'cont3_rbtamt', 'cont3_rf', 'cont2_treat1_y_pred', 'cont2_treat1_mpc_pred', 'cont1_treat1_y_pred', 'cont1_treat1_mpc_pred', 'cont3_treat1_y_pred', 'cont3_treat1_mpc_pred']\n",
      "['FD_fin', 'FD_finit', 'FD_nofin', 'ND_fin', 'ND_finit', 'ND_nofin', 'SND_fin', 'SND_finit', 'SND_nofin', 'TOT_fin', 'TOT_finit', 'TOT_nofin']\n"
     ]
    }
   ],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "\n",
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "print(list(rfdicts[rfdicts_keys[0]]))\n",
    "print(list(rfdicts))\n",
    "#rfdicts['DUR_fin']['cont1_X_labels']\n",
    "INCOME = ['FINCBTXM'] #'FINCBTAX','FSALARYM',\n",
    "CONTROL = ['adults', 'PERSLT18']\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'qescrowx_sum', 'timeleft'] #'orgmrtx_sum'\n",
    "CAT = [['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5'],['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'],['educ_nodegree','educ_highschool','educ_higher']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SND_fin', 'SND_finit', 'SND_nofin', 'TOT_fin', 'TOT_finit', 'TOT_nofin']\n",
      "SND_fin:\n",
      "SND_fin treat1 cont1 FINCBTXM\n",
      "SND_fin treat1 cont1 finassets\n",
      "SND_fin treat1 cont1 morgpayment\n",
      "SND_fin treat1 cont1 qblncm1x_sum\n",
      "SND_fin treat1 cont1 qescrowx_sum\n",
      "SND_fin treat1 cont1 timeleft\n",
      "SND_fin treat1 cont1 age\n",
      "SND_fin treat1 cont1 adults\n",
      "SND_fin treat1 cont1 PERSLT18\n",
      "SND_fin treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_fin treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_finit:\n",
      "SND_finit treat1 cont1 FINCBTXM\n",
      "SND_finit treat1 cont1 finassets_it\n",
      "SND_finit treat1 cont1 morgpayment\n",
      "SND_finit treat1 cont1 qblncm1x_sum\n",
      "SND_finit treat1 cont1 qescrowx_sum\n",
      "SND_finit treat1 cont1 timeleft\n",
      "SND_finit treat1 cont1 age\n",
      "SND_finit treat1 cont1 adults\n",
      "SND_finit treat1 cont1 PERSLT18\n",
      "SND_finit treat1 cont1 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_finit treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_finit treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_nofin:\n",
      "SND_nofin treat1 cont1 FINCBTXM\n",
      "SND_nofin treat1 cont1 morgpayment\n",
      "SND_nofin treat1 cont1 qblncm1x_sum\n",
      "SND_nofin treat1 cont1 qescrowx_sum\n",
      "SND_nofin treat1 cont1 timeleft\n",
      "SND_nofin treat1 cont1 age\n",
      "SND_nofin treat1 cont1 adults\n",
      "SND_nofin treat1 cont1 PERSLT18\n",
      "SND_nofin treat1 cont1 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_nofin treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_nofin treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_fin:\n",
      "TOT_fin treat1 cont1 FINCBTXM\n",
      "TOT_fin treat1 cont1 finassets\n",
      "TOT_fin treat1 cont1 morgpayment\n",
      "TOT_fin treat1 cont1 qblncm1x_sum\n",
      "TOT_fin treat1 cont1 qescrowx_sum\n",
      "TOT_fin treat1 cont1 timeleft\n",
      "TOT_fin treat1 cont1 age\n",
      "TOT_fin treat1 cont1 adults\n",
      "TOT_fin treat1 cont1 PERSLT18\n",
      "TOT_fin treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_fin treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_finit:\n",
      "TOT_finit treat1 cont1 FINCBTXM\n",
      "TOT_finit treat1 cont1 finassets_it\n",
      "TOT_finit treat1 cont1 morgpayment\n",
      "TOT_finit treat1 cont1 qblncm1x_sum\n",
      "TOT_finit treat1 cont1 qescrowx_sum\n",
      "TOT_finit treat1 cont1 timeleft\n",
      "TOT_finit treat1 cont1 age\n",
      "TOT_finit treat1 cont1 adults\n",
      "TOT_finit treat1 cont1 PERSLT18\n",
      "TOT_finit treat1 cont1 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_finit treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_finit treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_nofin:\n",
      "TOT_nofin treat1 cont1 FINCBTXM\n",
      "TOT_nofin treat1 cont1 morgpayment\n",
      "TOT_nofin treat1 cont1 qblncm1x_sum\n",
      "TOT_nofin treat1 cont1 qescrowx_sum\n",
      "TOT_nofin treat1 cont1 timeleft\n",
      "TOT_nofin treat1 cont1 age\n",
      "TOT_nofin treat1 cont1 adults\n",
      "TOT_nofin treat1 cont1 PERSLT18\n",
      "TOT_nofin treat1 cont1 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_nofin treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_nofin treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#def vimp_plot_uplift()\n",
    "rfdicts_keys = list(rfdicts)\n",
    "rfdicts_keys = [key for key in rfdicts_keys if (key.split('_')[0]=='SND') | (key.split('_')[0]=='TOT')]\n",
    "#rfdicts_keys = rfdicts_keys[:1]\n",
    "print(rfdicts_keys)\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\pdp\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "    for t in treat:\n",
    "        for c in ['cont1']:\n",
    "            expvars = rfdicts[k][c+'_X_labels']\n",
    "            INCOME = ['FINCBTXM'] #'FINCBTAX','FSALARYM',\n",
    "            if 'finassets' in expvars:\n",
    "                INCOME = INCOME + ['finassets']\n",
    "            if 'finassets_it' in expvars:\n",
    "                INCOME = INCOME + ['finassets_it']\n",
    "            for v in INCOME:\n",
    "                print(k,t,c,v)\n",
    "                if INCOME.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                               feature_ids_treat=rfdicts[k][t+'_X_labels'], feature_ids_cont=rfdicts[k][t+'_X_labels'], \n",
    "                                                               types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                                        feature_ids_treat=rfdicts[k][t+'_X_labels'],feature_ids_cont=rfdicts[k][t+'_X_labels'], \n",
    "                                                                        types=['mean','percentile','std','median'], percentile=[25,75])) \n",
    "            rfdicts[k][f'{c}_{t}_pdp_INCOME'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_INCOME.csv')\n",
    "            MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'qescrowx_sum', 'timeleft']\n",
    "            for v in MORTGAGE:\n",
    "                print(k,t,c,v)\n",
    "                if MORTGAGE.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'], \n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                                        feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75]))    \n",
    "            rfdicts[k][f'{c}_{t}_pdp_MORTGAGE'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_MORTGAGE.csv')\n",
    "            \n",
    "            for v in ['age']:\n",
    "                print(k,t,c,v)\n",
    "                df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "            rfdicts[k][f'{c}_{t}_pdp_age'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_age.csv')\n",
    "            \n",
    "            for v in CONTROL:\n",
    "                print(k,t,c,v)\n",
    "                if CONTROL.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                                        feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75]))    \n",
    "            rfdicts[k][f'{c}_{t}_pdp_CONTROL'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_CONTROL.csv')\n",
    "            \n",
    "            CAT = [['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5'],['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'],['educ_nodegree','educ_highschool','educ_higher']]\n",
    "            i = 1\n",
    "            if 'finassets' in expvars:\n",
    "                CAT = [['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'],['educ_nodegree','educ_highschool','educ_higher']]\n",
    "            for cat in CAT:\n",
    "                print(k,t,c,cat)\n",
    "                df = uplift_cat_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],cat,rfdicts[k][t+'_X_labels'],rfdicts[k][t+'_X'],\n",
    "                                                           rfdicts[k][t+'_rbtamt'],X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                           feature_ids_cont=rfdicts[k][c+'_X_labels'],types=['mean','percentile','std','median'],\n",
    "                                                           percentile=[25,75])\n",
    "                rfdicts[k][f'{c}_{t}_pdp_CAT_{i}'] = df\n",
    "                df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_CAT_{i}.csv')\n",
    "                i = i+1\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SND_fin', 'SND_finit', 'SND_nofin', 'TOT_fin', 'TOT_finit', 'TOT_nofin']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "rfdicts_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot partial dependeny as comparison between the different specifications for a given control group and type of consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import PercentFormatter #plot as percentage\n",
    "import seaborn #plot density and histogram at the same time\n",
    "# Set directory where files are downloaded to. Chdir has to be changed in order to run on another computer\n",
    "os.chdir('C:\\\\Users\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE') #change this to the folder where the data set is stored, all the results will be saved in the same folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "#five variables to plot: income, age, finanassets, mortgage, education; education is categorical all are stored in different csv files\n",
    "#generate list of tuples for filename and variable\n",
    "pdp_tuples = [('INCOME','FINCBTXM'),('MORTGAGE','qblncm1x_sum'),('MORTGAGE','timeleft'),('age','age')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('INCOME', 'FINCBTXM')\n",
      "('MORTGAGE', 'qblncm1x_sum')\n",
      "('MORTGAGE', 'timeleft')\n",
      "('age', 'age')\n",
      "('INCOME', 'FINCBTXM')\n",
      "('MORTGAGE', 'qblncm1x_sum')\n",
      "('MORTGAGE', 'timeleft')\n",
      "('age', 'age')\n"
     ]
    }
   ],
   "source": [
    "control = 'cont1'\n",
    "treatment = 'treat1'\n",
    "consumption = ['FD','SND','ND','TOT']\n",
    "crtype = ['cr','mpc']\n",
    "for cr in crtype:\n",
    "    for t in pdp_tuples:\n",
    "        print(t)\n",
    "        fig = plt.figure(figsize=(18, 10))\n",
    "        for c,cons in enumerate(consumption):\n",
    "            path = os.getcwd()+f'\\\\pdp\\\\{cons}'\n",
    "            pdp_list = os.listdir(path)\n",
    "            #plot income\n",
    "            pdp_list = [p for p in pdp_list if (p.split('_')[0]=='finit') | (p.split('_')[0]=='nofin') if p.split('_')[3]==f'{t[0]}.csv' if p.split('_')[1]==control if p.split('_')[2]==treatment]\n",
    "            pdp_plot_dict = dict()\n",
    "            for i,p in enumerate(pdp_list):\n",
    "                pdp_plot = pd.read_csv(f'{path}\\\\{p}')\n",
    "                #display(pdp_plot.head())\n",
    "                colnames = list(pdp_plot)\n",
    "                expvar = t[1]\n",
    "                colnames = [c for c in colnames if c.split('_')[0]==expvar.split('_')[0]]\n",
    "                pdp_plot = pdp_plot.loc[:,colnames]\n",
    "                colrenames = [p.split('_')[0]+'_'+n for n in colnames]\n",
    "                rename = dict(zip(colnames,colrenames))\n",
    "                pdp_plot = pdp_plot.rename(columns=rename)\n",
    "                pdp_plot_dict[p] = pdp_plot\n",
    "\n",
    "            for i,p in enumerate(pdp_list):\n",
    "                if i == 0:\n",
    "                    pdp_plot = pdp_plot_dict[p]\n",
    "                else:\n",
    "                    pdp_plot = pdp_plot.join(pdp_plot_dict[p])\n",
    "            #display(pdp_plot.head())\n",
    "            plot = plt.subplot(int(len(consumption)/2),2,c+1)\n",
    "            colors=['red','green']\n",
    "            for j,i in enumerate(['nofin','finit']):\n",
    "                plt.plot(pdp_plot[f'{i}_{expvar}_grid'],pdp_plot[f'{i}_{expvar}_{cr}_mean'],label=f'{i} mean', color=colors[j])\n",
    "                plt.margins(x=0)\n",
    "                plt.plot(pdp_plot[f'{i}_{expvar}_grid'],pdp_plot[f'{i}_{expvar}_{cr}_median'],label=f'{i} median', color=colors[j], linestyle='dashed')\n",
    "                plt.margins(x=0)\n",
    "            if c==0:\n",
    "                plt.legend()\n",
    "            plt.title(cons, fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.getcwd()+f'\\\\pdp\\\\pdp_{cr}_{t[1]}_{control}_{treatment}.pdf')\n",
    "        plt.close()\n",
    "#expvars = list(set([c.split('_')[0] for c in colnames]))[1:]\n",
    "#types = [c.lstrip(c.split('_')[0]) for c in colnames]\n",
    "#plt.plot(pdp_plot[colnames[1]],pdp_plot[colnames[2]])\n",
    "#colnames\n",
    "#pdp_plot[colnames[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nofin_cont1_treat1_CAT_1.csv']\n",
      "['nofin_cont1_treat1_CAT_1.csv']\n",
      "['nofin_cont1_treat1_CAT_1.csv']\n",
      "['nofin_cont1_treat1_CAT_1.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 648x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nofin_cont1_treat1_CAT_3.csv']\n",
      "['nofin_cont1_treat1_CAT_3.csv']\n",
      "['nofin_cont1_treat1_CAT_3.csv']\n",
      "['nofin_cont1_treat1_CAT_3.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 648x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdp_tuples = [('CAT_1','CUTENURE_'),('CAT_3','educ_')]\n",
    "educ = ['nodegree','highschool','higher']\n",
    "\n",
    "#path = os.getcwd()+f'\\\\pdp\\\\{cons}'\n",
    "\n",
    "#pdp_list = os.listdir(path)\n",
    "#print(pdp_list)\n",
    "            #plot income\n",
    "treatment = 'treat1'\n",
    "control = 'cont1'\n",
    "consumption = ['FD','SND','ND','TOT']\n",
    "for t in pdp_tuples:\n",
    "    pdp_car_dict = dict()\n",
    "    for c in consumption:\n",
    "        carvar_means = []\n",
    "        path = os.getcwd()+f'\\\\pdp\\\\{c}'\n",
    "        pdp_list = os.listdir(path)\n",
    "        pdp_list = [p for p in pdp_list if  (p.split('_')[0]=='nofin') if p[-9:-4]==f'{t[0]}' if p.split('_')[1]==control if p.split('_')[2]==treatment] ##(p.split('_')[0]=='finit') |\n",
    "        print(pdp_list)\n",
    "        for p in pdp_list:\n",
    "            pdp_plot = pd.read_csv(f'{path}\\\\{p}')\n",
    "            catvars = [v for v in list(pdp_plot) if v[-6:]=='median' if v[0:3]=='mpc']\n",
    "            labels = [v[4:-7] for v in catvars]\n",
    "            catvars_val = [pdp_plot.loc[0,v] for v in catvars]\n",
    "            catvars_tuples = list(zip(catvars,catvars_val))\n",
    "            #print(catvars_tuples)\n",
    "            pdp_car_dict[f'{c}_tuples'] = catvars_tuples\n",
    "            pdp_car_dict[f'{c}_values'] = catvars_val\n",
    "            #pdp_car_dict[f'{c}_rects'] = ax.bar(x - width/2, catvars_val, width, label=c)\n",
    "\n",
    "                # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 10))\n",
    "    fig, ax = plt.subplots()\n",
    "    width = 0.2\n",
    "    x = np.arange(len(labels))\n",
    "    #print(x-width/2)\n",
    "    #print((x-width/2)/2)\n",
    "    #print((x-width/2)/2+0.0875)\n",
    "    pos = (x-width/2)-width\n",
    "    #print(pos)\n",
    "    for i,c in enumerate(consumption):\n",
    "        #print(x-width/4)\n",
    "        pdp_car_dict[f'{c}_rects'] = ax.bar(pos, pdp_car_dict[f'{c}_values'], width, label=c)\n",
    "        pos = pos +width\n",
    "        #print(pos)\n",
    "        def autolabel(rects):\n",
    "            \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "            for rect in rects:\n",
    "                height = rect.get_height()\n",
    "                ax.annotate('{:,.2f}'.format(height),\n",
    "                            xy=(rect.get_x() + rect.get_width() / 2, height-0.005),\n",
    "                            xytext=(0, 3),  # 3 points vertical offset\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom')\n",
    "\n",
    "\n",
    "        autolabel(pdp_car_dict[f'{c}_rects'])\n",
    "    #ax.set_ylabel('Scores')\n",
    "    #ax.set_title('Scores by group and gender')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "labels = ['G1', 'G2', 'G3', 'G4', 'G5']\n",
    "men_means = [20, 34, 30, 35, 27]\n",
    "women_means = [25, 32, 34, 20, 25]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, men_means, width, label='Men')\n",
    "rects2 = ax.bar(x + width/2, women_means, width, label='Women')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Scores by group and gender')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7** Visualize tree (not done yet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = ['FD_','SND']\n",
    "check = [key for key in pds_keys if key[0:7] == 'pdp_' + cons[0]]\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull out one tree from forest\n",
    "tree = rf.estimators_[5]\n",
    "\n",
    "#export image to a dot file\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = x_list, rounded = True, precision = 1 )\n",
    "(graph, )= pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree_check.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
