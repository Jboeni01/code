{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE'"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import PercentFormatter #plot as percentage\n",
    "import seaborn #plot density and histogram at the same time\n",
    "# Set directory where files are downloaded to. Chdir has to be changed in order to run on another computer\n",
    "os.chdir('C:\\\\Users\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE') #change this to the folder where the data set is stored, all the results will be saved in the same folder\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Data** (necessary for part1 and part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAVACCTX</th>\n",
       "      <th>CKBKACTX</th>\n",
       "      <th>finassets</th>\n",
       "      <th>SECESTX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CustID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183843</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183843</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183843</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183843</th>\n",
       "      <td>2056.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>2240.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183849</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183849</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183849</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183849</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183850</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183850</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183850</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183850</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183853</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183853</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183853</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183853</th>\n",
       "      <td>500.0</td>\n",
       "      <td>1205.0</td>\n",
       "      <td>5305.0</td>\n",
       "      <td>3600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183858</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183858</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183858</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183858</th>\n",
       "      <td>38000.0</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>89000.0</td>\n",
       "      <td>38000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183864</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183864</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183864</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183864</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183869</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183869</th>\n",
       "      <td>2300.0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>24300.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183872</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183872</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183872</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183872</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183873</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183873</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183873</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183888</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183888</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183893</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183893</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183893</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183893</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183897</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183897</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183897</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183897</th>\n",
       "      <td>7000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183899</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183899</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183899</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183899</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183909</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183909</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183909</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SAVACCTX  CKBKACTX  finassets  SECESTX\n",
       "CustID                                        \n",
       "183843       NaN       NaN        NaN      NaN\n",
       "183843       NaN       NaN        NaN      NaN\n",
       "183843       NaN       NaN        NaN      NaN\n",
       "183843    2056.0     184.0     2240.0      NaN\n",
       "183849       NaN       NaN        NaN      NaN\n",
       "183849       NaN       NaN        NaN      NaN\n",
       "183849       NaN       NaN        NaN      NaN\n",
       "183849       NaN       NaN        NaN      NaN\n",
       "183850       NaN       NaN        NaN      NaN\n",
       "183850       NaN       NaN        NaN      NaN\n",
       "183850       NaN       NaN        NaN      NaN\n",
       "183850       NaN       NaN        NaN      NaN\n",
       "183853       NaN       NaN        NaN      NaN\n",
       "183853       NaN       NaN        NaN      NaN\n",
       "183853       NaN       NaN        NaN      NaN\n",
       "183853     500.0    1205.0     5305.0   3600.0\n",
       "183858       NaN       NaN        NaN      NaN\n",
       "183858       NaN       NaN        NaN      NaN\n",
       "183858       NaN       NaN        NaN      NaN\n",
       "183858   38000.0   13000.0    89000.0  38000.0\n",
       "183864       NaN       NaN        NaN      NaN\n",
       "183864       NaN       NaN        NaN      NaN\n",
       "183864       NaN       NaN        NaN      NaN\n",
       "183864       0.0       NaN        0.0      NaN\n",
       "183869       NaN       NaN        NaN      NaN\n",
       "183869    2300.0   22000.0    24300.0      NaN\n",
       "183872       NaN       NaN        NaN      NaN\n",
       "183872       NaN       NaN        NaN      NaN\n",
       "183872       NaN       NaN        NaN      NaN\n",
       "183872       NaN       NaN        NaN      NaN\n",
       "183873       NaN       NaN        NaN      NaN\n",
       "183873       NaN       NaN        NaN      NaN\n",
       "183873       NaN       NaN        NaN      NaN\n",
       "183888       NaN       NaN        NaN      NaN\n",
       "183888       NaN       NaN        NaN      NaN\n",
       "183893       NaN       NaN        NaN      NaN\n",
       "183893       NaN       NaN        NaN      NaN\n",
       "183893       NaN       NaN        NaN      NaN\n",
       "183893       NaN       NaN        NaN      NaN\n",
       "183897       NaN       NaN        NaN      NaN\n",
       "183897       NaN       NaN        NaN      NaN\n",
       "183897       NaN       NaN        NaN      NaN\n",
       "183897    7000.0    3000.0    10000.0      NaN\n",
       "183899       NaN       NaN        NaN      NaN\n",
       "183899       NaN       NaN        NaN      NaN\n",
       "183899       NaN       NaN        NaN      NaN\n",
       "183899       NaN       NaN        NaN      NaN\n",
       "183909       NaN       NaN        NaN      NaN\n",
       "183909       NaN       NaN        NaN      NaN\n",
       "183909       NaN       NaN        NaN      NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fs08 = pd.read_csv(os.getcwd()+'\\\\fs08.csv').set_index('CustID')\n",
    "#identifier\n",
    "TIME = ['QINTRVMO', 'QINTRVYR', 'rbtmo_1', 'rbtmo_2', 'diff_1', 'diff_2']\n",
    "ID = ['NEWID']\n",
    "\n",
    "#dependent variables\n",
    "CONS = ['FD','SND','ND','DUR','TOT']\n",
    "FUTCONS = ['fut_' + c for c in CONS]\n",
    "LRUNCONS = ['lrun_' + c for c in CONS]\n",
    "\n",
    "for i in range(len(LRUNCONS)):\n",
    "    fs08[LRUNCONS[i]] = fs08[[CONS[i],FUTCONS[i]]].sum(axis=1)\n",
    "\n",
    "#explanatory variables\n",
    "DEMO = ['age', 'adults', 'PERSLT18', 'MARITAL1', 'CUTENURE'] #exclude , 'FINCBTAX'\n",
    "    #age; number of adults; people below 18; marital status; housing tenure; income in the last 12 months\n",
    "DEMO2 = ['FSALARYM', 'FINCBTXM'] \n",
    "    #FSALARYM: income from salary and wages, CKBKACTX: balance/market value in balance accounts/brookerage accounts;    \n",
    "    #FINCBTXM: Total amount of family income before taxes (Imputed or collected data); (relevant demographics available for the second stimulus only)\n",
    "ASSETS = ['valid_finassets','finassets']\n",
    "    # finassets: sum of 1) SAVACCTX (Total balance/market value (including interest earned) CU had in savings accounts in banks, savings and loans,\n",
    "                         #credit unions, etc., as of the last day of previous month;)\n",
    "                # and    2)CKBKACTX (Total balance or market value (including interest earned) CU had in checking accounts, brokerage accounts, \n",
    "                            #and other similar accounts as of the last day of the previous month\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'qescrowx_sum', 'timeleft'] #exclude , 'orgmrtx_sum',\n",
    "    #morgpayment: morgage payment per month; qblncm1x_sum: sum of principal balances outstanding at the beginning of month M1; orgmrtx_sum: sum of mortgage amounts;\n",
    "    #qescrowx_sum: sum of last regular escrow payments; timeleft: maximum time left on mortgage payment\n",
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #\n",
    "#sample split\n",
    "RBT = ['rbtamt', 'rbtamt_chk', 'rbtamt_e']\n",
    "LAGRBT = ['last_' + var for var in RBT] #lagged variables\n",
    "FUTRBT = ['fut_' + var for var in RBT] #future variables\n",
    "\n",
    "for m in MORTGAGE:\n",
    "    fs08.loc[fs08[m].isna(),m]=0\n",
    "        \n",
    "\n",
    "\n",
    "fs08 = fs08[TIME + ID + CONS + DEMO + DEMO2 + ASSETS + MORTGAGE + RBT + ['rbtamt_1','rbtamt_2'] + LAGRBT + FUTRBT + FUTCONS + LRUNCONS + EDUC] #+ CHGCONS + LAGCONS \n",
    "#fs08 = fs08.loc[fs08['timeleft']>0,:]\n",
    "fs08 = pd.get_dummies(fs08, columns=['CUTENURE','MARITAL1']) #change categorical variables to dummy variables\n",
    "\n",
    "DEMO = [s for s in DEMO if s!='CUTENURE' if s!='MARITAL1'] + ['CUTENURE' + f'_{j}' for j in list(range(1,6)) if j!=3] +['MARITAL1' + f'_{j}' for j in list(range(1,5))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the average rebate amount per individual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08['rbtamt_idmean'] = 0\n",
    "fs08['rbtamt_idmean'] = fs08.groupby('CustID')['rbtamt'].transform('mean')\n",
    "fs08['rbt_count'] = 0\n",
    "fs08['rbt_count'] = fs08.groupby('CustID')['rbtamt'].transform('count')\n",
    "\n",
    "#\n",
    "#sometimes individuals give information of rebate receipt preceding (following) three months of the first (last) interview.\n",
    "#Wherever this is the case, the average rebate should be the weighted mean of rebates received before (after) the relevant time and the actual rebate \n",
    "\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['last_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) #weighted mean,  & (fs08['rbtamt_idmean']>0)\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist() # & (fs08['rbtamt_idmean']>0)\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first') #change for all entries for a given individual\n",
    "\n",
    "\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 1/(1+fs08['rbt_count'])*(fs08['fut_rbtamt']+fs08['rbtamt_idmean']), fs08['rbtamt_idmean']) #weighted mean  & (fs08['rbtamt_idmean']>0)\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0)  & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() #& (fs08['rbtamt_idmean']>0)\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last')  #change for all entries for a given individual\n",
    "#display(fs08.loc[index, ['rbtamt_idmean', 'rbtamt', 'fut_rbtamt', 'last_rbtamt']])\n",
    "\n",
    "#wherever there is no entry for rebates received in the relevant time period but when there were rebates receivde in the past (future) change mean to the value\n",
    "index = fs08.index[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last()))].tolist() #                                                                               \n",
    "fs08['rbtamt_idmean'] = np.where((fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].last())),\n",
    "                                 fs08['fut_rbtamt'], fs08['rbtamt_idmean'])\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('last')\n",
    "\n",
    "index = fs08.index[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first()))].tolist()\n",
    "fs08['rbtamt_idmean'] = np.where((fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()) & (fs08['NEWID'].isin(fs08.groupby('CustID')['NEWID'].first())), \n",
    "                                fs08['last_rbtamt'], fs08['rbtamt_idmean'])\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('first')\n",
    "\n",
    "\n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[(fs08['rbtamt']>0) | (fs08['fut_rbtamt']>0) | (fs08['last_rbtamt']>0) ,'rbt_flag'] = 1\n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('sum')\n",
    "fs08.loc[fs08['rbt_flag']>0, 'rbt_flag'] = 1\n",
    "fs08 = fs08.loc[fs08['rbt_flag']==1]\n",
    "\n",
    "\n",
    "index = fs08.index[fs08['rbtamt_idmean'].isna()].tolist()\n",
    "index = list(set(index))\n",
    "fs08.loc[index, 'rbtamt_idmean'] = fs08.loc[index,'fut_rbtamt']\n",
    "fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['fut_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'fut_rbtamt' ]\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'rbtamt_idmean' ] = fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt_idmean'].isna()), 'last_rbtamt' ]\n",
    "\n",
    "fs08.loc[index,'rbtamt_idmean'] = fs08.loc[index,:].groupby('CustID')['rbtamt_idmean'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations, impute values for financial liquidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(RBT)):\n",
    "    fs08.loc[fs08[RBT[i]]==0, RBT[i]] = np.nan\n",
    "    fs08.loc[fs08[LAGRBT[i]]==0, LAGRBT[i]] = np.nan\n",
    "    fs08.loc[fs08[FUTRBT[i]]==0, FUTRBT[i]] = np.nan\n",
    "    #fs08.loc[(fs08[LAGRBT[i]]==0) | (fs08[LAGRBT[i]].isna()), LAGRBT[i]] =  fs08.loc[(fs08[LAGRBT[i]]==0) | (fs08[LAGRBT[i]].isna())].groupby('CustID')[RBT[i]].shift(-1) \n",
    "    fs08.loc[(fs08[FUTRBT[i]]==0) | (fs08[FUTRBT[i]].isna()), FUTRBT[i]] =  fs08.loc[(fs08[FUTRBT[i]]==0) | (fs08[FUTRBT[i]].isna())].groupby('CustID')[RBT[i]].shift(-1) \n",
    "\n",
    "\n",
    "\n",
    "fs08 = fs08.reset_index()\n",
    "\n",
    "#Iterative imputation for financial liquidity\n",
    "#explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "fs08 = fs08.dropna(subset=CONS+DEMO+DEMO2+MORTGAGE) #Keep only observations that have all info on explanatory variables, dropping missing values on mortgage lowers the sample to half \n",
    "\n",
    "\n",
    "fs08_finit = fs08.copy()\n",
    "fs08_finit = fs08_finit[ ID + CONS + DEMO + DEMO2 + ASSETS + MORTGAGE +  LRUNCONS + EDUC]\n",
    "#fs08_finit = fs08_finit.loc[:,CONS+DEMO+DEMO2+MORTGAGE+['CustID','NEWID','finassets']]\n",
    "labels = list(fs08_finit.columns)\n",
    "imp_mean = IterativeImputer(random_state=0) #use python package iterative imputer\n",
    "imp_mean.fit(fs08_finit[2:])\n",
    "fs08_finit = pd.DataFrame(imp_mean.transform(fs08_finit),columns=labels)\n",
    "fs08_finit = fs08_finit.loc[:,['finassets','NEWID']]\n",
    "fs08_finit = fs08_finit.rename(columns={'finassets':'finassets_it'})\n",
    "\n",
    "fs08 = pd.merge(fs08.sort_values(by = ['NEWID']).reset_index(), fs08_finit.sort_values(by = ['NEWID']).reset_index(), how = 'left', on = 'NEWID', validate = '1:1')\n",
    "fs08 = fs08.drop(columns=['index_x','index_y']) \n",
    "ASSETS = ['valid_finassets','finassets', 'finassets_it' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate treatment and control groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate treatment group:\n",
    "fs08['treat1'] = 0 \n",
    "fs08.loc[fs08['rbtamt'].notna(),'treat1'] = 1 #all entries with actual info on rebate are in the treatment group\n",
    "\n",
    "#three different control groups:\n",
    "#control group 1: those who didn't receive the rebate in the given month\n",
    "fs08['cont1'] = 0\n",
    "fs08.loc[fs08['rbtamt'].isna(), 'cont1'] = 1\n",
    "\n",
    "#control group 2: drop one time period after receipt of rebate, as part of rebate might've been consumed one time period after\n",
    "fs08['cont2'] = 0 \n",
    "fs08['rbt_flag'] = 0 \n",
    "fs08.loc[fs08['rbtamt']>0,'rbt_flag'] = 1 #identifier for rebate\n",
    "fs08['rbt_flag_lag'] = fs08.groupby('CustID')['rbt_flag'].shift(1) #identifier for rebate a period before (lag)\n",
    "fs08.loc[fs08['rbt_flag_lag']==1,'rbt_flag']=1 #change rebate identifier so it capture now if a rebate was received this period or the period before\n",
    "fs08.loc[(fs08['rbtamt'].isna()) & (fs08['rbt_flag']==0), 'cont2'] = 1 #those who didn't receive a rebate now or a period before are in the control group\n",
    "\n",
    "#treatment 2: all individuals who received a rebate last time period. This group should be compared to control group 2 only\n",
    "fs08['treat2'] = 0\n",
    "fs08.loc[(fs08['cont2']==0) & (fs08['treat1']==0),'treat2'] = 1\n",
    "fs08.loc[(fs08['last_rbtamt']>0) & (fs08['rbtamt'].isna()), 'treat2'] = 1\n",
    "\n",
    "#treatment 3: long run consumption response: all individuals who received a rebate in this period and where the rebate interview is not the last\n",
    "fs08['treat3'] = 0\n",
    "fs08.loc[(fs08['rbtamt']>0) & (fs08[FUTCONS[0]].notna()), 'treat3'] = 1\n",
    "\n",
    "#control 3: long-run consumption: those who haven't received a rebate two periods from current period and have information on consumption for next period as well\n",
    "fs08['cont4'] = 0\n",
    "fs08.loc[(fs08['cont2']==1) & (fs08[FUTCONS[0]].notna()),'cont4'] = 1\n",
    "\n",
    "#control 4: those who haven't received the rebate yet\n",
    "fs08['cont3'] = 0 \n",
    "fs08['rbt_flag'] = fs08.groupby('CustID')['rbt_flag'].transform('cumsum') #starts counting from the point on which the first rebate was received\n",
    "fs08.loc[(fs08['rbtamt'].isna()) & (fs08['rbt_flag']==0),'cont3'] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs08_cap = fs08[(np.abs(stats.zscore(fs08.loc[:,CONS+LRUNCONS])) < 3).all(axis=1)] #drop outliers\n",
    "fs08_cap = fs08_cap.loc[fs08_cap['FD']>0] #there are still two observations where food consumption is zero; drop bc of common sense\n",
    "fs08_cap = fs08_cap.loc[fs08['ND']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Descriptive statistics** (part 1 and part 2 cn be run seperately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Take a look at the explanatory variables used for random forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4028.000000\n",
       "mean       49.705313\n",
       "std        15.572740\n",
       "min        21.000000\n",
       "25%        37.500000\n",
       "50%        49.000000\n",
       "75%        61.500000\n",
       "max        84.500000\n",
       "Name: age, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{age} & \\multicolumn{2}{c}{FINCBTXM} & \\multicolumn{2}{c}{finassets} & \\multicolumn{2}{c}{finassets\\_it} \\\\\n",
      "{} &  treat1 &    cont1 &   treat1 &    cont1 &    treat1 &     cont1 &       treat1 &    cont1 \\\\\n",
      "\\midrule\n",
      "count & 4,028.0 & 11,195.0 &  4,028.0 & 11,195.0 &     723.0 &   1,458.0 &      4,028.0 & 11,195.0 \\\\\n",
      "mean  &    49.7 &     50.2 & 61,456.0 & 62,444.7 &  36,182.4 &  44,898.0 &     48,045.8 & 50,124.9 \\\\\n",
      "std   &    15.6 &     15.3 & 46,710.9 & 47,227.1 & 144,797.3 & 214,301.3 &     78,665.0 & 94,903.9 \\\\\n",
      "25\\%   &    37.5 &     38.0 & 28,149.5 & 29,000.0 &     225.0 &     200.0 &      9,616.5 & 11,342.8 \\\\\n",
      "50\\%   &    49.0 &     49.0 & 50,780.5 & 52,056.0 &   2,200.0 &   2,600.0 &     30,743.0 & 31,556.1 \\\\\n",
      "75\\%   &    61.5 &     61.5 & 81,927.0 & 82,553.5 &  14,000.0 &  16,000.0 &     61,857.0 & 62,810.2 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{morgpayment} & \\multicolumn{2}{c}{qblncm1x\\_sum} & \\multicolumn{2}{c}{qescrowx\\_sum} & \\multicolumn{2}{c}{timeleft} \\\\\n",
      "{} &      treat1 &   cont1 &       treat1 &     cont1 &       treat1 &   cont1 &   treat1 &   cont1 \\\\\n",
      "\\midrule\n",
      "count &     1,854.0 & 5,258.0 &      1,861.0 &   5,266.0 &      1,562.0 & 4,326.0 &  1,870.0 & 5,299.0 \\\\\n",
      "mean  &        39.8 &    39.5 &    125,795.3 & 123,429.3 &        359.7 &   364.1 &     19.5 &    19.4 \\\\\n",
      "std   &        54.4 &    57.7 &    107,836.2 & 108,120.2 &        315.3 &   338.7 &      8.7 &     8.8 \\\\\n",
      "25\\%   &        20.0 &    19.6 &     54,423.0 &  53,027.5 &        154.0 &   155.2 &     12.5 &    12.2 \\\\\n",
      "50\\%   &        29.5 &    28.9 &     99,183.0 &  96,212.0 &        275.0 &   273.0 &     21.8 &    21.9 \\\\\n",
      "75\\%   &        45.5 &    44.5 &    165,662.0 & 159,485.8 &        478.0 &   464.0 &     26.9 &    26.9 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'qescrowx_sum', 'timeleft'] #'orgmrtx_sum',\n",
    "expvars = DEMO+DEMO2+ASSETS+MORTGAGE+EDUC+['rbtamt','rbtamt_idmean']\n",
    "\n",
    "#income table\n",
    "for exp in expvars:\n",
    "    if exp == DEMO[0]:\n",
    "        des = fs08_cap.loc[fs08_cap['treat1']==1,exp].describe()\n",
    "        display(des)\n",
    "        des = pd.concat([des, fs08_cap.loc[fs08_cap['cont1']==1,exp].describe()], axis=1, join='inner')\n",
    "    else:\n",
    "        for g in ['treat1','cont1']:\n",
    "            if exp in MORTGAGE:\n",
    "                des = pd.concat([des, fs08_cap.loc[(fs08[g]==1)&(fs08[exp]>0),exp].describe()], axis=1, join='inner')\n",
    "            else:\n",
    "                des = pd.concat([des, fs08_cap.loc[fs08[g]==1,exp].describe()], axis=1, join='inner')\n",
    "\n",
    "index1 = [ i for i in expvars for reps in range(2) ]\n",
    "index2 = ['treat1','cont1']*int((len(index1)/2))\n",
    "tuples = list(zip(index1,index2))\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples) \n",
    "des_cols = list(des)\n",
    "des_cols=[i for i in des_cols if i[0] in ['FINCBTXM','finassets','finassets_it','age']]\n",
    "print(des.iloc[[0,1,2,4,5,6]].to_latex(float_format=\"{:,.1f}\".format, columns=des_cols,multicolumn_format='c')) #table1\n",
    "des_cols = list(des)\n",
    "des_cols = [i for i in des_cols if i[0] in ['morgpayment','qblncm1x_sum','qescrowx_sum','timeleft']]\n",
    "print(des.iloc[[0,1,2,4,5,6]].to_latex(float_format=\"{:,.1f}\".format, columns=des_cols,multicolumn_format='c')) #table2\n",
    "des_cols = list(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average amount of rebate received per household\n",
      "\\begin{tabular}{llrrrrrrr}\n",
      "\\toprule\n",
      "       &             &  count &  mean &   max &  min &  25\\% &   75\\% &  std \\\\\n",
      "\\midrule\n",
      "cont1 & rbtamt\\_mean & 11,195 &   959 & 3,660 &    6 &  600 & 1,200 &  507 \\\\\n",
      "treat1 & rbtamt &  4,028 &   944 & 3,660 &    1 &  600 & 1,200 &  508 \\\\\n",
      "       & rbtamt\\_mean &  4,028 &   945 & 3,660 &    6 &  600 & 1,200 &  502 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Average amount of rebate received per household')\n",
    "des = fs08_cap.loc[fs08_cap['cont1']==1,'rbtamt_idmean'].describe()\n",
    "des = pd.concat([des, fs08_cap.loc[fs08_cap['treat1']==1,'rbtamt_idmean'].describe()], axis=1, join='inner',names=['cont1','treat1'])\n",
    "des = pd.concat([des, fs08_cap.loc[fs08['rbtamt']>0,'rbtamt'].describe()], axis=1, join='inner',names=['cont1','treat1','treat1'])\n",
    "tuples = [('rbtamt_mean', 'cont1'), ('rbtamt_mean', 'treat1'), ('rbtamt', 'treat1')]\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples)\n",
    "print(des.stack().unstack(level=0).stack(level=0).to_latex(float_format=\"{:,.0f}\".format,  columns=['count','mean','max','min','25%','75%','std'])) #table 3\n",
    "#print(des1.loc[['count', 'mean', 'std', 'min','50%','max'],:].to_latex(float_format=\"{:,.1f}\".format))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Descriptives for dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{FD} & \\multicolumn{2}{c}{SND} & \\multicolumn{2}{c}{ND} & \\multicolumn{2}{c}{TOT} \\\\\n",
      "{} &  treat1 &    cont1 &   treat1 &    cont1 &   treat1 &    cont1 &   treat1 &    cont1 \\\\\n",
      "\\midrule\n",
      "count & 4,028.0 & 11,195.0 &  4,028.0 & 11,195.0 &  4,028.0 & 11,195.0 &  4,028.0 & 11,195.0 \\\\\n",
      "mean  & 1,867.7 &  1,795.1 &  4,264.4 &  4,010.1 &  5,318.6 &  5,082.2 &  9,432.4 &  9,119.5 \\\\\n",
      "std   &   981.4 &    943.9 &  1,999.9 &  1,871.9 &  2,532.5 &  2,392.4 &  4,766.9 &  4,645.6 \\\\\n",
      "min   &    65.0 &     26.0 &    250.0 &    133.0 &    372.0 &    444.0 &    372.0 &    747.0 \\\\\n",
      "25\\%   & 1,154.0 &  1,105.0 &  2,768.8 &  2,620.0 &  3,386.8 &  3,290.7 &  5,807.2 &  5,696.3 \\\\\n",
      "50\\%   & 1,695.0 &  1,635.0 &  4,002.5 &  3,736.0 &  4,960.0 &  4,712.0 &  8,568.3 &  8,248.4 \\\\\n",
      "75\\%   & 2,425.0 &  2,320.0 &  5,485.0 &  5,119.0 &  6,844.9 &  6,480.8 & 12,243.1 & 11,653.0 \\\\\n",
      "max   & 5,580.0 &  5,617.0 & 11,881.5 & 11,803.3 & 14,759.7 & 14,799.7 & 31,505.9 & 31,664.5 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "treat = ['treat1','treat2','treat3']\n",
    "cont = ['cont1','cont2','cont3','cont4']\n",
    "#baseline:\n",
    "CONS=['FD','SND','ND','TOT']\n",
    "treat = ['treat1']\n",
    "cont = ['cont1']\n",
    "\n",
    "for i in CONS:\n",
    "    if i=='FD':\n",
    "        des = fs08_cap.loc[fs08_cap['treat1']==1,i].describe()\n",
    "        des = pd.concat([des,fs08_cap.loc[fs08_cap['cont1']==1,i].describe()],join='inner',axis=1)\n",
    "    else:\n",
    "        for g in ['treat1','cont1']:\n",
    "            des = pd.concat([des, fs08_cap.loc[fs08_cap[g]==1,i].describe()],join='inner', axis=1)\n",
    "\n",
    "index1 = ['FD']*2 + ['SND']*2 + ['ND']*2 + ['TOT']*2\n",
    "index2 = ['treat1','cont1']*8\n",
    "tuples = list(zip(index1,index2))\n",
    "des.columns = pd.MultiIndex.from_tuples(tuples)\n",
    "print(des.to_latex(float_format=\"{:,.1f}\".format ,multicolumn_format='c')) #table 4\n",
    "#des.stack().unstack(level=0).stack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare histograms of treatment group with different control groups\n",
    "\n",
    "for i in range(len(CONS)):\n",
    "    for t in treat[0:1]:\n",
    "        for c in cont[0:1]:\n",
    "            plt.figure(figsize=(3.2,2.4))\n",
    "            plt.title(f'{CONS[i]}')\n",
    "            plt.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            #plt.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, density=True, bins=30, color='red')\n",
    "            plt.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            #plt.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= t, density=True, bins=30, color='green')\n",
    "            #plt.set_title(f'{CONS[i]}:')\n",
    "            #plt.tight_layout()\n",
    "            plt.xticks(fontsize = 6)\n",
    "            plt.yticks(fontsize=6)\n",
    "            plt.legend(loc='upper right', frameon=True, fontsize=6)\n",
    "            plt.savefig(os.getcwd() + f'\\\\descriptives\\\\{CONS[i]}_pattern.pdf')\n",
    "            plt.close()\n",
    "CONS=['SND','TOT']\n",
    "fig=plt.figure(figsize=(6.4,2.4))\n",
    "for i in range(len(CONS)):\n",
    "    #plt.figure(figsize=(6.4,2.4))\n",
    "    plot = plt.subplot(1,2,i+1)\n",
    "    for t in treat[0:1]:\n",
    "        for c in cont[0:1]:     \n",
    "            #plot = plt.figure(figsize=(10,3))\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            plot.set_title(f'{CONS[i]}:')\n",
    "            plt.tight_layout()\n",
    "            plt.xticks(fontsize = 8)\n",
    "            plt.yticks(fontsize=8)\n",
    "            plt.legend(loc='upper right', frameon=False, fontsize=8)\n",
    "            #plot_count = plot_count+1\n",
    "            plt.savefig(os.getcwd() + f'\\\\descriptives\\\\SND_TOT_pattern_group1_treat1.pdf') #figure 1\n",
    "plt.close()\n",
    "\n",
    "\n",
    "CONS=['FD','SND','ND','TOT']        \n",
    "plot_count = 1\n",
    "for i in range(len(CONS)):\n",
    "    #plt.figure(figsize=(6.4,2.4))\n",
    "    for t in treat[0:1]:\n",
    "        fig=plt.figure(figsize=(6.4,2.4))\n",
    "        for c in cont[0:3]:     \n",
    "            #plot = plt.figure(figsize=(10,3))\n",
    "            plot = plt.subplot(1,3,cont.index(c)+1)\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[t]==1,CONS[i]], alpha=0.5, label= t, weights=np.ones(len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[t]==1,CONS[i]]), bins=30, color='red')\n",
    "            plot.hist(fs08_cap.loc[fs08_cap[c]==1,CONS[i]], alpha=0.5, label= c, weights=np.ones(len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]])) / len(fs08_cap.loc[fs08_cap[c]==1,CONS[i]]), bins=30, color='green')\n",
    "            plot.set_title(f'{CONS[i]}:')\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc='upper right', frameon=False)\n",
    "            plot_count = plot_count+1\n",
    "            #plt.savefig(os.getcwd() + f'\\\\descriptives\\\\{CONS[i]}_pattern_groupcomp.pdf')\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finassets_it</th>\n",
       "      <th>finassets</th>\n",
       "      <th>qblncm1x_sum</th>\n",
       "      <th>FINCBTXM</th>\n",
       "      <th>age</th>\n",
       "      <th>CUTENURE_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finassets_it</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039973</td>\n",
       "      <td>0.297345</td>\n",
       "      <td>0.220370</td>\n",
       "      <td>0.009109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finassets</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032776</td>\n",
       "      <td>0.140357</td>\n",
       "      <td>0.156547</td>\n",
       "      <td>-0.023834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qblncm1x_sum</th>\n",
       "      <td>0.039973</td>\n",
       "      <td>0.032776</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.366562</td>\n",
       "      <td>-0.214654</td>\n",
       "      <td>0.611398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FINCBTXM</th>\n",
       "      <td>0.297345</td>\n",
       "      <td>0.140357</td>\n",
       "      <td>0.366562</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.151055</td>\n",
       "      <td>0.347557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.220370</td>\n",
       "      <td>0.156547</td>\n",
       "      <td>-0.214654</td>\n",
       "      <td>-0.151055</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.196864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUTENURE_1</th>\n",
       "      <td>0.009109</td>\n",
       "      <td>-0.023834</td>\n",
       "      <td>0.611398</td>\n",
       "      <td>0.347557</td>\n",
       "      <td>-0.196864</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              finassets_it  finassets  qblncm1x_sum  FINCBTXM       age  \\\n",
       "finassets_it      1.000000   1.000000      0.039973  0.297345  0.220370   \n",
       "finassets         1.000000   1.000000      0.032776  0.140357  0.156547   \n",
       "qblncm1x_sum      0.039973   0.032776      1.000000  0.366562 -0.214654   \n",
       "FINCBTXM          0.297345   0.140357      0.366562  1.000000 -0.151055   \n",
       "age               0.220370   0.156547     -0.214654 -0.151055  1.000000   \n",
       "CUTENURE_1        0.009109  -0.023834      0.611398  0.347557 -0.196864   \n",
       "\n",
       "              CUTENURE_1  \n",
       "finassets_it    0.009109  \n",
       "finassets      -0.023834  \n",
       "qblncm1x_sum    0.611398  \n",
       "FINCBTXM        0.347557  \n",
       "age            -0.196864  \n",
       "CUTENURE_1      1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finassets_it</th>\n",
       "      <th>finassets</th>\n",
       "      <th>qblncm1x_sum</th>\n",
       "      <th>FINCBTXM</th>\n",
       "      <th>age</th>\n",
       "      <th>CUTENURE_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finassets_it</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.103821</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.067753</td>\n",
       "      <td>0.070930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finassets</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.177962</td>\n",
       "      <td>0.269407</td>\n",
       "      <td>-0.005001</td>\n",
       "      <td>0.192747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qblncm1x_sum</th>\n",
       "      <td>0.103821</td>\n",
       "      <td>0.177962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.189569</td>\n",
       "      <td>0.171876</td>\n",
       "      <td>0.676245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FINCBTXM</th>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.269407</td>\n",
       "      <td>0.189569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078989</td>\n",
       "      <td>0.194902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.067753</td>\n",
       "      <td>-0.005001</td>\n",
       "      <td>0.171876</td>\n",
       "      <td>0.078989</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.208625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUTENURE_1</th>\n",
       "      <td>0.070930</td>\n",
       "      <td>0.192747</td>\n",
       "      <td>0.676245</td>\n",
       "      <td>0.194902</td>\n",
       "      <td>0.208625</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              finassets_it  finassets  qblncm1x_sum  FINCBTXM       age  \\\n",
       "finassets_it      1.000000   1.000000      0.103821  0.403993  0.067753   \n",
       "finassets         1.000000   1.000000      0.177962  0.269407 -0.005001   \n",
       "qblncm1x_sum      0.103821   0.177962      1.000000  0.189569  0.171876   \n",
       "FINCBTXM          0.403993   0.269407      0.189569  1.000000  0.078989   \n",
       "age               0.067753  -0.005001      0.171876  0.078989  1.000000   \n",
       "CUTENURE_1        0.070930   0.192747      0.676245  0.194902  0.208625   \n",
       "\n",
       "              CUTENURE_1  \n",
       "finassets_it    0.070930  \n",
       "finassets       0.192747  \n",
       "qblncm1x_sum    0.676245  \n",
       "FINCBTXM        0.194902  \n",
       "age             0.208625  \n",
       "CUTENURE_1      1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finassets_it</th>\n",
       "      <th>finassets</th>\n",
       "      <th>qblncm1x_sum</th>\n",
       "      <th>FINCBTXM</th>\n",
       "      <th>age</th>\n",
       "      <th>CUTENURE_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finassets_it</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024288</td>\n",
       "      <td>0.159287</td>\n",
       "      <td>0.138848</td>\n",
       "      <td>0.078261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finassets</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017513</td>\n",
       "      <td>-0.067504</td>\n",
       "      <td>0.198891</td>\n",
       "      <td>0.007436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qblncm1x_sum</th>\n",
       "      <td>0.024288</td>\n",
       "      <td>-0.017513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.153316</td>\n",
       "      <td>-0.081274</td>\n",
       "      <td>0.613950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FINCBTXM</th>\n",
       "      <td>0.159287</td>\n",
       "      <td>-0.067504</td>\n",
       "      <td>0.153316</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.098808</td>\n",
       "      <td>0.215017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.138848</td>\n",
       "      <td>0.198891</td>\n",
       "      <td>-0.081274</td>\n",
       "      <td>-0.098808</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.052226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUTENURE_1</th>\n",
       "      <td>0.078261</td>\n",
       "      <td>0.007436</td>\n",
       "      <td>0.613950</td>\n",
       "      <td>0.215017</td>\n",
       "      <td>-0.052226</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              finassets_it  finassets  qblncm1x_sum  FINCBTXM       age  \\\n",
       "finassets_it      1.000000   1.000000      0.024288  0.159287  0.138848   \n",
       "finassets         1.000000   1.000000     -0.017513 -0.067504  0.198891   \n",
       "qblncm1x_sum      0.024288  -0.017513      1.000000  0.153316 -0.081274   \n",
       "FINCBTXM          0.159287  -0.067504      0.153316  1.000000 -0.098808   \n",
       "age               0.138848   0.198891     -0.081274 -0.098808  1.000000   \n",
       "CUTENURE_1        0.078261   0.007436      0.613950  0.215017 -0.052226   \n",
       "\n",
       "              CUTENURE_1  \n",
       "finassets_it    0.078261  \n",
       "finassets       0.007436  \n",
       "qblncm1x_sum    0.613950  \n",
       "FINCBTXM        0.215017  \n",
       "age            -0.052226  \n",
       "CUTENURE_1      1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.scatter(fs08_cap['FINCBTXM'], fs08_cap['qblncm1x_sum'], s=10)\n",
    "#plt.scatter(fs08_cap['FINCBTXM'], fs08_cap['finassets_it'])\n",
    "#plt.scatter(fs08_cap['age'],fs08_cap['FINCBTXM'],  s=10)\n",
    "display(fs08_cap.loc[:,['finassets_it','finassets','qblncm1x_sum','FINCBTXM','age','CUTENURE_1']].corr())\n",
    "display(fs08_cap.loc[(fs08_cap['age']<=35) & (fs08_cap['FINCBTXM']<50000),['finassets_it','finassets','qblncm1x_sum','FINCBTXM','age','CUTENURE_1']].corr())\n",
    "display(fs08_cap.loc[(fs08_cap['age'].between(35,60)) & (fs08_cap['FINCBTXM']<50000),['finassets_it','finassets','qblncm1x_sum','FINCBTXM','age','CUTENURE_1']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "descriptives for rebate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: Machine learning approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Define sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Run random forest algorithm seperately for treatment and control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #'educ_bachelor','educ_master','educ_doctorate'\n",
    "DEMO = ['age', 'adults', 'PERSLT18','CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5', 'MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'] # exclude 'FINCBTAX',\n",
    "DEMO2 = [ 'FINCBTXM'] # exclude 'FSALARYM',\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum','qescrowx_sum', 'timeleft'] # 'orgmrtx_sum', \n",
    "CONS = ['FD', 'SND', 'ND', 'TOT']\n",
    "CONT = ['cont1', 'cont2', 'cont3']\n",
    "TREAT = 'treat1'\n",
    "treatgroup = TREAT\n",
    "trees = 1000\n",
    "\n",
    "#Random Forest for short term consumption: treatment group 1 with imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC + ['finassets_it']  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_finit.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[fs08_cap[con]==1, [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_nofin.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group with just the observations where financial assets are included\n",
    "TREAT = 'treat1'\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + ['finassets'] + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "#contgroup = CONT[3]\n",
    "treatgroup = TREAT\n",
    "\n",
    "for c in CONS:\n",
    "    rf = dict()\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1) & (fs08_cap['valid_finassets']==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rf[TREAT] = treat\n",
    "    for con in CONT:\n",
    "        cont = fs08_cap.loc[(fs08_cap[con]==1) & (fs08_cap['valid_finassets']==1) , [depvar] + expvars + ['rbtamt_idmean']]\n",
    "        rf[con] = cont\n",
    "    for i in (list(rf)):\n",
    "        y = np.array(rf[i][depvar]) #array for dependent variable\n",
    "        X = np.array(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "        rf[i+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "        X_labels = list(rf[i].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "        rf[i+'_X_labels'] = X_labels #save in dict\n",
    "        rf[i+'_rbtamt'] = np.array(rf[i]['rbtamt_idmean'])\n",
    "        rf[i+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "        rf[i+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_fin.pkl', 'wb')\n",
    "    pickle.dump(rf, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment with treatment group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(rfdicts)\n",
    "EDUC = ['educ_nodegree','educ_highschool','educ_higher'] #'educ_bachelor','educ_master','educ_doctorate'\n",
    "DEMO = ['age', 'adults', 'PERSLT18','CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5', 'MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'] # exclude 'FINCBTAX',\n",
    "DEMO2 = [ 'FINCBTXM'] # exclude 'FSALARYM',\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum','qescrowx_sum', 'timeleft'] # 'orgmrtx_sum', \n",
    "CONS = ['FD', 'SND', 'ND', 'TOT']\n",
    "TREAT = 'treat2'\n",
    "treatgroup = TREAT\n",
    "trees = 1000\n",
    "\n",
    "#include finassets only\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + ['finassets'] + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "\n",
    "for c in CONS:\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1) & (fs08_cap['valid_finassets']==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rfdicts[f'{c}_fin'][TREAT] = treat\n",
    "    y = np.array(rfdicts[f'{c}_fin'][TREAT][depvar]) #array for dependent variable\n",
    "    X = np.array(rfdicts[f'{c}_fin'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "    X_labels = list(rfdicts[f'{c}_fin'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_X_labels'] = X_labels #save in dict\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_rbtamt'] = np.array(rfdicts[f'{c}_fin'][TREAT]['rbtamt_idmean'])\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    rfdicts[f'{c}_fin'][TREAT+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_fin.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[f'{c}_fin'], output)\n",
    "    output.close()\n",
    "    \n",
    "#Random Forest for short term consumption: treatment group without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC  #define explanatory variables + ['finassets_it'] \n",
    "for c in CONS:\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rfdicts[f'{c}_nofin'][TREAT] = treat\n",
    "    y = np.array(rfdicts[f'{c}_nofin'][TREAT][depvar]) #array for dependent variable\n",
    "    X = np.array(rfdicts[f'{c}_nofin'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "    X_labels = list(rfdicts[f'{c}_nofin'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_X_labels'] = X_labels #save in dict\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_rbtamt'] = np.array(rfdicts[f'{c}_nofin'][TREAT]['rbtamt_idmean'])\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    rfdicts[f'{c}_nofin'][TREAT+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_nofin.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[f'{c}_nofin'], output)\n",
    "    output.close()\n",
    "\n",
    "#Random Forest for short term consumption: treatment group without imputations of financial assets\n",
    "expvars = DEMO + DEMO2 + MORTGAGE + EDUC + ['finassets_it']  #define explanatory variables + ['finassets_it'] \n",
    "for c in CONS:\n",
    "    depvar = c\n",
    "    treat = fs08_cap.loc[(fs08_cap[treatgroup]==1), [depvar] + expvars + ['rbtamt_idmean']]\n",
    "    rfdicts[f'{c}_finit'][TREAT] = treat\n",
    "    y = np.array(rfdicts[f'{c}_finit'][TREAT][depvar]) #array for dependent variable\n",
    "    X = np.array(rfdicts[f'{c}_finit'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1)) #array with relevant explanatory variables as columns\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_X'] = X #save as dict entry with keyword treat_X/cont_X\n",
    "    X_labels = list(rfdicts[f'{c}_finit'][TREAT].drop([depvar ,'rbtamt_idmean'], axis=1).columns) #column labels as list\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_X_labels'] = X_labels #save in dict\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_rbtamt'] = np.array(rfdicts[f'{c}_finit'][TREAT]['rbtamt_idmean'])\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_rf'] = RandomForestRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    #rf[i+'_rf'] = ExtraTreesRegressor(n_estimators = trees, random_state = 0, min_samples_leaf = 5, oob_score=True, max_features = 0.33)\n",
    "    rfdicts[f'{c}_finit'][TREAT+'_rf'].fit(X,y)\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{depvar}_finit.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[f'{c}_finit'], output)\n",
    "    output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Predict Outcomes for overall consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_predicitons_rbt(rf_treat, rf_cont, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if (type(X_cont) is not np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_treat_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('X_treat_rbamt needs to have an array like structure')\n",
    "            else:\n",
    "                X_temp = X_treat.copy()\n",
    "                rbtamt_temp = X_treat_rbtamt.copy()\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(X_cont_rbtamt) is not np.ndarray):\n",
    "                raise ValueError('if X_cont is specified, X_cont_rbamt needs to have an array like structure')\n",
    "            if sorted(feature_ids_treat)!=sorted(feature_ids_cont):\n",
    "                raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont')\n",
    "            elif (len(feature_ids_treat)==0) | (len(feature_ids_cont)==0):\n",
    "                raise ValueError(f'if X_treat and X_cont are specified, feature_ids must not be empty')\n",
    "            else:\n",
    "                #stack treatment and control groups on top of each other\n",
    "                X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                #stack rebate for each household on top of each other\n",
    "                rbtamt_temp = pd.concat([pd.DataFrame(X_treat_rbtamt), pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                X_labels = list(X.columns)\n",
    "                X_temp = np.array(X)\n",
    "                rbtamt_temp = np.array(rbtamt_temp)\n",
    "        else: \n",
    "            raise ValueError('X_treat does not have an array like structure')\n",
    "        y = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp)) #for each household predict consumption response\n",
    "        mpc = y/rbtamt_temp[:,0] #calculate fraction of rebate that was consumed\n",
    "        return y,mpc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate consumption response for each household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD_fin:\n",
      "cont2_treat1_y_pred (1479,)\n",
      "cont2_treat1_mpc_pred (1479,)\n",
      "cont1_treat1_y_pred (2181,)\n",
      "cont1_treat1_mpc_pred (2181,)\n",
      "cont3_treat1_y_pred (798,)\n",
      "cont3_treat1_mpc_pred (798,)\n",
      "cont2_treat2_y_pred (1471,)\n",
      "cont2_treat2_mpc_pred (1471,)\n",
      "cont3_treat2_y_pred (790,)\n",
      "cont3_treat2_mpc_pred (790,)\n",
      "FD_finit:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "FD_nofin:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "ND_fin:\n",
      "cont2_treat1_y_pred (1479,)\n",
      "cont2_treat1_mpc_pred (1479,)\n",
      "cont1_treat1_y_pred (2181,)\n",
      "cont1_treat1_mpc_pred (2181,)\n",
      "cont3_treat1_y_pred (798,)\n",
      "cont3_treat1_mpc_pred (798,)\n",
      "cont2_treat2_y_pred (1471,)\n",
      "cont2_treat2_mpc_pred (1471,)\n",
      "cont3_treat2_y_pred (790,)\n",
      "cont3_treat2_mpc_pred (790,)\n",
      "ND_finit:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "ND_nofin:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "SND_fin:\n",
      "cont2_treat1_y_pred (1479,)\n",
      "cont2_treat1_mpc_pred (1479,)\n",
      "cont1_treat1_y_pred (2181,)\n",
      "cont1_treat1_mpc_pred (2181,)\n",
      "cont3_treat1_y_pred (798,)\n",
      "cont3_treat1_mpc_pred (798,)\n",
      "cont2_treat2_y_pred (1471,)\n",
      "cont2_treat2_mpc_pred (1471,)\n",
      "cont3_treat2_y_pred (790,)\n",
      "cont3_treat2_mpc_pred (790,)\n",
      "SND_finit:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "SND_nofin:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "TOT_fin:\n",
      "cont2_treat1_y_pred (1479,)\n",
      "cont2_treat1_mpc_pred (1479,)\n",
      "cont1_treat1_y_pred (2181,)\n",
      "cont1_treat1_mpc_pred (2181,)\n",
      "cont3_treat1_y_pred (798,)\n",
      "cont3_treat1_mpc_pred (798,)\n",
      "cont2_treat2_y_pred (1471,)\n",
      "cont2_treat2_mpc_pred (1471,)\n",
      "cont3_treat2_y_pred (790,)\n",
      "cont3_treat2_mpc_pred (790,)\n",
      "TOT_finit:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n",
      "TOT_nofin:\n",
      "cont2_treat1_y_pred (12435,)\n",
      "cont2_treat1_mpc_pred (12435,)\n",
      "cont1_treat1_y_pred (15223,)\n",
      "cont1_treat1_mpc_pred (15223,)\n",
      "cont3_treat1_y_pred (10801,)\n",
      "cont3_treat1_mpc_pred (10801,)\n",
      "cont2_treat2_y_pred (11665,)\n",
      "cont2_treat2_mpc_pred (11665,)\n",
      "cont3_treat2_y_pred (10031,)\n",
      "cont3_treat2_mpc_pred (10031,)\n"
     ]
    }
   ],
   "source": [
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\condistr\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for t in treat:\n",
    "        for c in cont:\n",
    "            if (t=='treat2') & (c=='cont1'):\n",
    "                pass\n",
    "            else:\n",
    "                y,mpc = uplift_predicitons_rbt(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                              rfdicts[k][c+'_X'],rfdicts[k][c+'_rbtamt'],rfdicts[k][t+'_X_labels'],rfdicts[k][c+'_X_labels'])\n",
    "                rfdicts[k][f'{c}_{t}_y_pred'] = y\n",
    "                print(f'{c}_{t}_y_pred', y.shape)\n",
    "                rfdicts[k][f'{c}_{t}_mpc_pred'] = mpc\n",
    "                print(f'{c}_{t}_mpc_pred', mpc.shape)\n",
    "\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate basic histograms for the consumption response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD_fin\n",
      "FD_finit\n",
      "FD_nofin\n",
      "ND_fin\n",
      "ND_finit\n",
      "ND_nofin\n",
      "SND_fin\n",
      "SND_finit\n",
      "SND_nofin\n",
      "TOT_fin\n",
      "TOT_finit\n",
      "TOT_nofin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\condistr\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for t in treat:\n",
    "        for c in cont:\n",
    "            if (t=='treat2') & (c=='cont1'):\n",
    "                pass            \n",
    "            else:\n",
    "                y = rfdicts[k][f'{c}_{t}_y_pred']\n",
    "                mpc = rfdicts[k][f'{c}_{t}_mpc_pred']\n",
    "                plt.hist(y, bins=40,  edgecolor='black')\n",
    "                lower = round((min(y)/100),1)*100\n",
    "                upper = round((max(y)/100),1)*100+1\n",
    "                ticks = int((upper-lower)/4)\n",
    "                plt.xticks(np.arange(lower, upper, ticks))\n",
    "                plt.title(f'Pred cons resp distr., {vartype},{c},{t}')\n",
    "                plt.xlabel(f'consumption in {cons}, number of observations {y.shape[0]}')\n",
    "                plt.axvline(np.median(y), color='red', linestyle='dashed', linewidth=1)\n",
    "                min_ylim, max_ylim = plt.ylim()\n",
    "                plt.text(np.median(y)*2, max_ylim*0.9, 'Median: {:.2f}'.format(np.median(y)),fontsize=8)\n",
    "                plt.ylabel(f'number of individuals in bin')\n",
    "                plt.savefig(newpath + f'\\\\{vartype}_{c}_{t}_y_pred.pdf')\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot consumption responses in a subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONS = ['FD','SND','ND','TOT']\n",
    "fintype = ['nofin','finit','fin']\n",
    "rfdicts_keys = list(rfdicts)\n",
    "for f in fintype:\n",
    "    rfdicts_subplot = [k for k in rfdicts_keys if k.split('_')[0] in CONS if k.split('_')[1]==f]\n",
    "\n",
    "    fig=plt.figure(figsize=(12,6))\n",
    "\n",
    "    for i,k in enumerate(rfdicts_subplot):\n",
    "        c = 'cont1'\n",
    "        t= 'treat1'\n",
    "        plot = plt.subplot(2,2,i+1)    \n",
    "        y = rfdicts[k][f'{c}_{t}_y_pred']\n",
    "        plot.hist(y, bins=40,  edgecolor='black')\n",
    "        lower = round((min(y)/100),1)*100\n",
    "        upper = round((max(y)/100),1)*100+1\n",
    "        ticks = int((upper-lower)/4)\n",
    "        plt.xticks(np.arange(lower, upper, ticks), fontsize = 8)\n",
    "        plt.axvline(np.median(y), color='red', linestyle='dashed', linewidth=1)\n",
    "        min_ylim, max_ylim = plt.ylim()\n",
    "        plt.text(np.median(y)*2, max_ylim*0.9, 'Median: {:.2f}'.format(np.median(y)),fontsize=8)\n",
    "        plt.title(k.split('_')[0])\n",
    "        plt.tight_layout()\n",
    "        plt.yticks(fontsize=8)\n",
    "        #if i == 2:\n",
    "            #plt.ylabel('households in bin')\n",
    "        #    plt.xlabel('predicted consumption response')\n",
    "        #plt.savefig(os.getcwd() + f'\\\\condistr\\\\{CONS[0]}_{CONS[1]}_{c}_{t}_{f}.pdf') #figure 1\n",
    "        plt.savefig(os.getcwd() + f'\\\\condistr\\\\allcons_{c}_{t}_{f}.pdf') #figure 1\n",
    "    #fig.suptitle(f'Predicted consumption response')\n",
    "    plt.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate tables for consumption responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD\n",
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{FD} & \\multicolumn{2}{c}{SND} & \\multicolumn{2}{c}{ND} & \\multicolumn{2}{c}{TOT} \\\\\n",
      "{} &    finit &    nofin &    finit &    nofin &    finit &    nofin &    finit &    nofin \\\\\n",
      "\\midrule\n",
      "count & 15,223.0 & 15,223.0 & 15,223.0 & 15,223.0 & 15,223.0 & 15,223.0 & 15,223.0 & 15,223.0 \\\\\n",
      "mean  &     72.4 &     77.3 &    255.0 &    272.3 &    226.9 &    267.9 &    272.1 &    335.8 \\\\\n",
      "std   &    241.3 &    236.8 &    436.2 &    436.1 &    554.9 &    556.5 &    972.4 &  1,000.9 \\\\\n",
      "min   & -1,026.0 &   -948.7 & -1,896.1 & -2,308.9 & -2,594.0 & -2,289.4 & -4,635.1 & -4,571.5 \\\\\n",
      "25\\%   &    -69.3 &    -60.5 &     -3.4 &     14.4 &    -94.6 &    -61.8 &   -255.1 &   -221.0 \\\\\n",
      "50\\%   &     74.2 &     79.8 &    249.6 &    266.0 &    223.2 &    268.3 &    292.6 &    359.5 \\\\\n",
      "75\\%   &    219.3 &    216.2 &    517.9 &    534.8 &    556.8 &    597.3 &    821.4 &    920.1 \\\\\n",
      "max   &  1,320.2 &  1,250.9 &  2,063.2 &  2,130.1 &  2,560.0 &  2,506.2 &  5,148.6 &  5,650.8 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{c}{FD} & \\multicolumn{2}{c}{ND} & \\multicolumn{2}{c}{SND} & \\multicolumn{2}{c}{TOT} \\\\\n",
      "{} &   finit &   nofin &   finit &   nofin &   finit &   nofin &   finit &    nofin \\\\\n",
      "\\midrule\n",
      "mean &   0.119 &   0.129 &   0.404 &   0.485 &   0.426 &   0.463 &   0.520 &    0.645 \\\\\n",
      "std  &   1.288 &   1.465 &   3.892 &   5.098 &   3.711 &   4.396 &   5.343 &    6.430 \\\\\n",
      "min  & -26.505 & -24.948 & -46.663 & -56.494 & -41.236 & -45.904 & -97.500 & -102.254 \\\\\n",
      "25\\%  &  -0.076 &  -0.067 &  -0.106 &  -0.069 &  -0.004 &   0.016 &  -0.285 &   -0.237 \\\\\n",
      "50\\%  &   0.085 &   0.093 &   0.257 &   0.313 &   0.286 &   0.310 &   0.338 &    0.422 \\\\\n",
      "75\\%  &   0.275 &   0.273 &   0.687 &   0.743 &   0.642 &   0.660 &   1.038 &    1.152 \\\\\n",
      "max  &  77.694 &  86.404 & 238.843 & 303.299 & 235.839 & 277.846 & 462.083 &  517.390 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('FD', 'finit'),\n",
       " ('FD', 'nofin'),\n",
       " ('ND', 'finit'),\n",
       " ('ND', 'nofin'),\n",
       " ('SND', 'finit'),\n",
       " ('SND', 'nofin'),\n",
       " ('TOT', 'finit'),\n",
       " ('TOT', 'nofin')]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fintype = ['nofin','finit','fin']\n",
    "rfdicts_keys = list(rfdicts)\n",
    "list(rfdicts[rfdicts_keys[0]])\n",
    "cons = list(set([k.split('_')[0] for k in rfdicts_keys]))\n",
    "spec = list(set([k.split('_')[0] for k in rfdicts_keys]))\n",
    "c = 'cont1'\n",
    "t = 'treat1'\n",
    "rfdicts_subkeys = [k for k in rfdicts_keys if k.split('_')[1]!='fin' ]\n",
    "\n",
    "for i,k in enumerate(rfdicts_subkeys):\n",
    "    cr = rfdicts[k][f'{c}_{t}_y_pred']\n",
    "    mpc = rfdicts[k][f'{c}_{t}_mpc_pred']\n",
    "    if i==0:\n",
    "        print(k.split('_')[0])\n",
    "        cr_pred = pd.DataFrame(cr, columns=['cr_'+k.split('_')[0]+'_'+k.split('_')[1]]).describe()\n",
    "        mpc_pred = pd.DataFrame(mpc, columns=['mpc_'+k.split('_')[0]+'_'+k.split('_')[1]]).describe()\n",
    "    else:\n",
    "        cr_pred = cr_pred.join(pd.DataFrame(cr, columns=['cr_'+k.split('_')[0]+'_'+k.split('_')[1]]).describe())\n",
    "        #display(cr_pred)\n",
    "        mpc_pred = mpc_pred.join(pd.DataFrame(mpc, columns=['mpc_'+k.split('_')[0]+'_'+k.split('_')[1]]).describe())\n",
    "        #display(mpc_pred)\n",
    "\n",
    "index1 = [i.split('_')[1] for i in list(mpc_pred)]\n",
    "index2 = [i.split('_')[2] for i in list(mpc_pred)]\n",
    "tuples = list(zip(index1,index2))\n",
    "mpc_pred.columns = pd.MultiIndex.from_tuples(tuples) \n",
    "index1 = [i.split('_')[1] for i in list(cr_pred)]\n",
    "index2 = [i.split('_')[2] for i in list(cr_pred)]\n",
    "tuples = list(zip(index1,index2))\n",
    "cr_pred.columns = pd.MultiIndex.from_tuples(tuples) \n",
    "\n",
    "\n",
    "cr_pred\n",
    "mpc_pred\n",
    "#tuples = sorted(tuples, key = lambda x: x[0], reverse = False)\n",
    "tuples = tuples[:2] + tuples[4:6] + tuples[2:4] + tuples[6:]\n",
    "\n",
    "print(cr_pred.to_latex(float_format=\"{:,.1f}\".format,columns = tuples, multicolumn_format='c'))\n",
    "print(mpc_pred.iloc[1:].to_latex(float_format=\"{:,.3f}\".format,multicolumn_format='c'))#table1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** Variable importance plot for treatment and control group separately and as a weighted sum for the whole sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfdicts_keys = list(rfdicts)\n",
    "#print(rfdicts_keys)\n",
    "#print(list(rfdicts[rfdicts_keys[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "FD_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "FD_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "FD_nofin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "ND_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "ND_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "ND_nofin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "SND_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "SND_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "SND_nofin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "TOT_fin\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "TOT_finit\n",
      "['treat1', 'cont2', 'cont1', 'cont3']\n",
      "TOT_nofin\n"
     ]
    }
   ],
   "source": [
    "#def vimp_plot_uplift()\n",
    "rfdicts_keys = list(rfdicts)\n",
    "uplift_imp_subplot = dict()\n",
    "for k in rfdicts_keys:\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\varimp\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    print(treat+cont)\n",
    "    uplift_imp = dict()\n",
    "    for i in treat+cont:\n",
    "        importances = (rfdicts[k][i+'_rf'].feature_importances_)\n",
    "        X_importances = [(label, importance) for label, importance in zip(rfdicts[k][i+'_X_labels'],importances)]\n",
    "        #X_importances = [(round(importance,2), label) for importance, label in zip(importances, rf[i+'_X_labels'])]\n",
    "        X_importances = sorted(X_importances, key = lambda x:x[1], reverse = False)\n",
    "        uplift_imp[i+'_varimp_values'] = [x[1] for x in X_importances]\n",
    "        uplift_imp[i+'_varimp_labels'] = [x[0] for x in X_importances]\n",
    "        uplift_imp[i+'_varimp_tuples'] = X_importances\n",
    "        \n",
    "    \n",
    "    for i in treat + cont:\n",
    "        X_importances = sorted(X_importances, key = lambda x:x[0].upper(), reverse = False) #sort in alphabetical order\n",
    "        uplift_imp[i+'_values'] = [x[1] for x in X_importances] #importances \n",
    "        uplift_imp[i+'_labels'] = [x[0] for x in X_importances] \n",
    "        shape = rfdicts[k][i+'_X'].shape\n",
    "        uplift_imp[i+'_sample'] = shape[0] \n",
    "    \n",
    "    for t in treat:\n",
    "        plotgroups = [t]\n",
    "        for c in cont:\n",
    "            plotgroups = plotgroups + [c] + [c+'_'+t]\n",
    "            uplift_imp[f'{c}_{t}_sample'] = uplift_imp[f'{t}_sample'] + uplift_imp[f'{c}_sample'] \n",
    "            uplift_imp[f'{c}_{t}_varimp_values'] = [uplift_imp[f'{t}_sample']/(uplift_imp[f'{c}_{t}_sample'])*uplift_imp[f'{t}_values'][i] + \n",
    "            uplift_imp[f'{c}_sample']/(uplift_imp[f'{c}_{t}_sample'])*uplift_imp[f'{c}_values'][i] for i in range(len(uplift_imp[f'{t}_values']))]\n",
    "\n",
    "            up_importances = [(label, importance) for label, importance in zip(uplift_imp[f'{t}_labels'],uplift_imp[f'{c}_{t}_varimp_values'])]\n",
    "            up_importances = sorted(up_importances, key = lambda x:x[1], reverse = False)\n",
    "            uplift_imp[f'{c}_{t}_varimp_tuples'] = up_importances            \n",
    "            uplift_imp[f'{c}_{t}_varimp_values'] = [up[1] for up in up_importances]\n",
    "            uplift_imp[f'{c}_{t}_varimp_labels'] = [up[0] for up in up_importances]\n",
    "            uplift_imp_subplot[f'{cons}_{c}_{t}_varimp_values_{vartype}'] = uplift_imp[f'{c}_{t}_varimp_values']\n",
    "            uplift_imp_subplot[f'{cons}_{c}_{t}_varimp_labels_{vartype}'] = uplift_imp[f'{c}_{t}_varimp_labels']\n",
    "        print(k)   \n",
    "    for g in plotgroups:\n",
    "        freq_series = pd.Series(uplift_imp[g+'_varimp_values'])\n",
    "        y_labels = uplift_imp[g+'_varimp_labels']\n",
    "\n",
    "        # Plot the figure.\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        ax = freq_series.plot(kind='barh')\n",
    "        ssize=uplift_imp[g+'_sample']\n",
    "        #ax.set_title(f'{k},{g.upper()}')\n",
    "        #ax.set_xlabel(f'sample size = {str(ssize)}')\n",
    "        ax.set_ylabel(f'Variable')\n",
    "        ax.set_yticklabels(y_labels)\n",
    "        #ax.set_xlim(-40, 300) # expand xlim to make labels easier to read\n",
    "\n",
    "        rects = ax.patches\n",
    "\n",
    "        # For each bar: Place a label\n",
    "        for rect in rects:\n",
    "            # Get X and Y placement of label from rect.\n",
    "            x_value = rect.get_width()\n",
    "            y_value = rect.get_y() + rect.get_height() / 2\n",
    "\n",
    "            # Number of points between bar and label. Change to your liking.\n",
    "            space = 3\n",
    "            # Vertical alignment for positive values\n",
    "            ha = 'left'\n",
    "\n",
    "            # Use X value as label and format number with one decimal place\n",
    "            label = \"{:.3f}\".format(x_value)\n",
    "\n",
    "            # Create annotation\n",
    "            plt.annotate(\n",
    "                label,                      # Use `label` as label\n",
    "                (x_value, y_value),         # Place label at end of the bar\n",
    "                xytext=(space, 0),          # Horizontally shift label by `space`\n",
    "                textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "                va='center',                # Vertically center label\n",
    "                ha=ha)                      # Horizontally align label differently for\n",
    "                                            # positive and negative values.\n",
    "        plt.savefig(f'{newpath}\\\\{cons}_{vartype}_{g}.pdf')\n",
    "        plt.close()\n",
    "        #plt.savefig(\"image.png\")\n",
    "        #plt.savefig(newpath + '\\\\'+ pathend +f'_{i}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(uplift_imp_subplot))\n",
    "consgroups = ['FD','SND'],['ND','TOT']\n",
    "j=1\n",
    "for consg in consgroups:\n",
    "    for i,cons in enumerate(consg): #,'ND','TOT'\n",
    "        if i == 0:\n",
    "            plotsubgroups = [cons+'_cont1_treat1']\n",
    "        else:\n",
    "            plotsubgroups = plotsubgroups + [cons+'_cont1_treat1']\n",
    "    plotsubgroups = sorted(plotsubgroups) \n",
    "    plotsubgroups\n",
    "    #plotsubgroups = plotsubgroups[:2] + plotsubgroups[4:6] + plotsubgroups[2:4] + plotsubgroups[6:]    \n",
    "    varimpvalues = [p + '_varimp_values_nofin' for p in plotsubgroups] + [p + '_varimp_values_finit' for p in plotsubgroups]\n",
    "    varimpvalues = sorted(varimpvalues)\n",
    "    varimpvalues = varimpvalues[:2] + varimpvalues[4:6] + varimpvalues[2:4] + varimpvalues[6:] \n",
    "\n",
    "    varimplabels = [p + '_varimp_labels_nofin' for p in plotsubgroups] + [p + '_varimp_labels_finit' for p in plotsubgroups]\n",
    "    varimplabels = sorted(varimplabels)\n",
    "    varimplabels = varimplabels[:2] + varimplabels[4:6] + varimplabels[2:4] + varimplabels[6:] \n",
    "    #print(varimpvalues)\n",
    "    #print(varimplabels)\n",
    "    #\n",
    "    # \n",
    "    plotsubgroups\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    i=0\n",
    "    for v in range(len(varimpvalues)):\n",
    "        plot = plt.subplot(len(plotsubgroups),2,i+1) \n",
    "        freq_series = pd.Series(uplift_imp_subplot[varimpvalues[v]])\n",
    "        y_labels = uplift_imp_subplot[varimplabels[v]]\n",
    "        # Plot the figure.\n",
    "\n",
    "        ax = freq_series.plot(kind='barh')\n",
    "        ax.set_title(varimpvalues[i].split('_')[0], fontsize = 14)\n",
    "        ax.set_yticklabels(y_labels, fontsize = 14)\n",
    "        #ax.set_xlim(-40, 300) # expand xlim to make labels easier to read\n",
    "\n",
    "        rects = ax.patches\n",
    "        i = i+1\n",
    "        #print(i)\n",
    "        # For each bar: Place a label\n",
    "        for rect in rects:\n",
    "            # Get X and Y placement of label from rect.\n",
    "            x_value = rect.get_width()\n",
    "            y_value = rect.get_y() + rect.get_height() / 2\n",
    "\n",
    "            # Number of points between bar and label. Change to your liking.\n",
    "            space = 3\n",
    "            # Vertical alignment for positive values\n",
    "            ha = 'left'\n",
    "\n",
    "            # Use X value as label and format number with one decimal place\n",
    "            label = \"{:.3f}\".format(x_value)\n",
    "\n",
    "            # Create annotation\n",
    "            plt.annotate(\n",
    "                label,                      # Use `label` as label\n",
    "                (x_value, y_value),         # Place label at end of the bar\n",
    "                xytext=(space, 0),          # Horizontally shift label by `space`\n",
    "                textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "                va='center',                # Vertically center label\n",
    "                ha=ha, fontsize =14)                      # Horizontally align label differently for\n",
    "                                            # positive and negative values.\n",
    "        #plt.savefig(f'{newpath}\\\\{vartype}_{g}.pdf')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.getcwd()+f'\\\\varimp\\\\varimp_subplot_{j}.pdf')\n",
    "    plt.close()\n",
    "    j = j+1\n",
    "    #    #plt.savefig(\"image.png\")\n",
    "    #    #plt.savefig(newpath + '\\\\'+ pathend +f'_{i}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Partial dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6.2** Function for uplift 2model partial dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def uplift_num_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[], feature_ids_treat=[], feature_ids_cont=[], types=['mean'], percentile='none', grid_lower=5, grid_upper=95 ): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor (or multiple predictors) in a random forest uplift 2 model approach.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    y_pred: corresponding predicted values of dependent variable (type: ndarray). \n",
    "            If input types is a list the columns in the array correspond to the chosen types in the same order\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is int):\n",
    "                if f_id > (X_temp.shape[1]-1):\n",
    "                    raise ValueError(f'positional number of {f_id} exceeds array shape')\n",
    "                else:\n",
    "                    X_temp = X_treat.copy()\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str):\n",
    "                if f_id not in feature_ids:\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids is not passed to the function')\n",
    "                else:\n",
    "                    f_id = feature_ids.index(f_id)\n",
    "                    f_id_label = f_id\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('f_id needs to be either an integer or a string')\n",
    "        elif (type(X_cont) is np.ndarray) & (type(X_treat) is np.ndarray):\n",
    "            if (type(f_id) is int):\n",
    "                raise ValueError(f'if X_cont is specified, then f_id needs to be a string variable')\n",
    "            elif (type(X_treat) is np.ndarray) & (type(f_id) is str) & ((f_id not in feature_ids_treat) |  (f_id not in feature_ids_cont)):\n",
    "                    raise ValueError(f'explanatory variable {f_id} is not in data frame or feature_ids_treat or feature_ids_cont is not passed to the function')\n",
    "            else:\n",
    "                if (sorted(feature_ids_treat)!=sorted(feature_ids_cont)) & (type(X_cont_rbtamt) is not np.ndarray) :\n",
    "                    raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                else:\n",
    "                    X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                    mean_rbt = np.array(mean_rbt)\n",
    "                    X_labels = list(X.columns)\n",
    "                    X_temp = np.array(X)\n",
    "                    f_id_label = f_id\n",
    "                    f_id = X_labels.index(f_id)             \n",
    "        else: \n",
    "            raise ValueError('Either X_cont or X_treat does not have an array like structure')\n",
    "\n",
    "                       #['age', 'adults', 'PERSLT18'\n",
    "        X_unique = np.array(list(set(X_temp[:, f_id])))\n",
    "        if len(X_unique)*3 > 100:\n",
    "            grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower), np.percentile(X_temp[:, f_id], grid_upper), 100)\n",
    "        else:\n",
    "            grid = np.linspace(np.percentile(X_temp[:, f_id],grid_lower),np.percentile(X_temp[:, f_id],grid_upper), len(X_unique)*2)\n",
    "       \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types.copy()\n",
    "        #column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "\n",
    "        if set(types) <= set(function_labels):\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i]\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif type(percentile) is not list: # 0<=percentile<=100:\n",
    "                    raise ValueError('percentile must be list')\n",
    "                else:\n",
    "                    column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "                    if len(percentile)>1:\n",
    "                        for p in percentile[1:]:\n",
    "                            types = types + ['percentile'] #pass\n",
    "                            nptypes = nptypes + [np.percentile]\n",
    "                            column_labels = column_labels + ['percentile_' +str(p)]\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "        y_pred = np.zeros((len(grid),len(types)))\n",
    "        mpc_pred = np.zeros((len(grid), len(types)))\n",
    "        p_pos = 0\n",
    "        for i, val in enumerate(grid): # i returns the counter, val returns the value at position of counter on grid \n",
    "            X_temp[:, f_id] = val\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            p_pos = 0\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[i,j] = nptypes[j](y_temp,percentile[p_pos])\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp,percentile[p_pos])\n",
    "                    p_pos = p_pos + 1\n",
    "                else:\n",
    "                    y_pred[i,j] = nptypes[j](y_temp)\n",
    "                    mpc_pred[i,j] = nptypes[j](mean_rbt_temp)\n",
    "        for j in range(len(column_labels)):\n",
    "            if column_labels[j] == 'percentile':\n",
    "                column_labels[j] = 'percentile_' + str(percentile)\n",
    "            else:\n",
    "                pass \n",
    "        column_labels_cr = ['cr_'+ lab for lab in column_labels]\n",
    "        column_labels_mpc = ['mpc_' + lab for lab in column_labels]\n",
    "        column_labels = ['grid']+column_labels_cr+column_labels_mpc\n",
    "        column_labels = [str(f_id_label)+ '_' + lab for lab in column_labels]\n",
    "        df = pd.DataFrame(np.c_[grid, y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return df #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for partial dependency of categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def uplift_cat_2m_partial_dependency_mpc(rf_treat, rf_cont, f_id, feature_ids_treat, X_treat, X_treat_rbtamt, X_cont=[], X_cont_rbtamt=[],  feature_ids_cont=[], types=['mean'], percentile='none'): #def partial_dependency(rf, X, y, feature_ids = [], f_id = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the partial dependency of response variable on a predictor (or multiple predictors) in a random forest uplift 2 model approach.\n",
    "    Inputs:\n",
    "    rf_treat: random forest regressor (from sklearn.ensemble) based on the treatment group (necessary)\n",
    "    rf_cont: random forest model (from sklearn.ensemble) based on the control group (necessary)\n",
    "    X_treat: array-like object consisting of all explanatory variables used in the random forest approach (necessary). \n",
    "             If X_cont is specified X_treat is assumed to consist only of observations in the treatment group. \n",
    "             Otherwise, X_treat is assumed to be the combined observations of control and treatment group.\n",
    "    X_cont: array-like object consisting of all explanatory variables used in the random forest approach for the control group (optional).\n",
    "    f_id: string or integer that captures the name or the position of the variable for which the partial dependence is calculated (necessary).\n",
    "          If f_id is a string, X_cont, feature_ids_treat, and feature_ids_cont need to be specified. \n",
    "          If f_id is an integer it captures the positional place of the explanatory variable in the dataframe for which it calculates the partial dependency. \n",
    "          If f_id is an integer X_cont, feature_ids_treat, and feature_ids_cont should not be specified bc it cannot be guaranteed that the positions of explanatory variables are the same\n",
    "          in treatment and control group.\n",
    "    feature_ids_treat: list of variable names in control group. Index needs to correspond to the position of the variables in X_treat. Needs to be specified if f_id is a string.\n",
    "    feature_ids_cont: list of variable names in treatment group. Index needs to correspond to the position of the variables in X_cont. Needs to be specified if f_id is a string.\n",
    "    types: list of different functions for which the variance dependence plot is calculated. \n",
    "           Default is 'mean', other options include 'median', 'std' (standard deviation) and 'percentile'.\n",
    "           If 'percentile' is included in types, percentile input needs to specified.\n",
    "    percentile: single value that corresponds to the percentile if percentile is included in types \n",
    "    grid_lower/grid_upper: The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n",
    "    1. Generate a data frame that consists of the combined sample of treatment and control group\n",
    "    2. Sample a grid of values of a predictor.\n",
    "    3. For each value, replace every row of that predictor with this value. \n",
    "       Calculate the average of the prediction (for the whole sample) between the random forest models of treatment and control group for each grid point. \n",
    "    \n",
    "    Output: \n",
    "    grid: grid  of variable for which the partial dependence is calculate (type: ndarray)\n",
    "    y_pred: corresponding predicted values of dependent variable (type: ndarray). \n",
    "            If input types is a list the columns in the array correspond to the chosen types in the same order\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    if (type(rf_treat) is not RandomForestRegressor) | (type(rf_cont) is not RandomForestRegressor):\n",
    "        raise ValueError('rf_treat or rf_cont are not random forest regressors')\n",
    "    else:\n",
    "        if type(X_cont) is not np.ndarray:\n",
    "            if (type(X_treat) is np.ndarray) & (type(f_id) is list):\n",
    "                if (len(f_id)<2):\n",
    "                    raise ValueError(f'{f_id} must be a list of hot-encoding features a hence neither empty nor have a length of 1')\n",
    "                else:\n",
    "                    cat = dict()\n",
    "                    positions = []\n",
    "                    for f in f_id:\n",
    "                        if type(f) is not str:\n",
    "                            raise ValueError('features in f_id list must be variable names of string type')\n",
    "                        else:\n",
    "                            if f not in feature_ids_treat:\n",
    "                                raise ValueError(f'categorical variable {f_id} is not a varibale of data frame')                                     \n",
    "                            else:\n",
    "                                cat[f+'_id'] = feature_ids_treat.index(f)\n",
    "                                cat[f+'_id_label'] = f\n",
    "                                positions = positions + [cat[f+'_id']]\n",
    "                    f_tuple = list(zip(f_id,positions))\n",
    "                    f_tuple = sorted(f_tuple, key = lambda x:x[1], reverse = False)\n",
    "                    f_id = [i[0] for i in f_tuple]\n",
    "                    positions = [i[1] for i in f_tuple]\n",
    "                    X_temp = X_treat.copy()\n",
    "            else:\n",
    "                raise ValueError('Either X_treat or f_id is not correctly specified')\n",
    "        elif (type(X_treat) is np.ndarray) & (type(X_treat) is np.ndarray) & (type(f_id) is list):\n",
    "            if (len(f_id)<2):\n",
    "                raise ValueError(f'{f_id} must be a list of hot-encoding features a hence neither empty nor have a length of 1')                            \n",
    "            else:\n",
    "                cat = dict()\n",
    "                positions = []\n",
    "                for f in f_id:\n",
    "                    if type(f) is not str:\n",
    "                        raise ValueError('features in f_id list must be variable names of string type')\n",
    "                    else:\n",
    "                        if (f not in feature_ids_treat) | (f not in feature_ids_cont):\n",
    "                            raise ValueError(f'categorical variable {f_id} is not a varibale of cont or treat data frame')                                     \n",
    "                        elif (sorted(feature_ids_treat)!=sorted(feature_ids_cont)) & (type(X_cont_rbtamt) is not np.ndarray):\n",
    "                            raise ValueError(f'feature_ids_treat needs to be the same as feature_ids_cont or X_cont_rbtamt is not correctly specified')\n",
    "                        else:\n",
    "                            cat[f+'_id'] = feature_ids_treat.index(f)\n",
    "                            cat[f+'_id_label'] = f\n",
    "                            #cat[f+'_tuple'] = listzip\n",
    "                            positions = positions + [cat[f+'_id']]\n",
    "                f_tuple = list(zip(f_id,positions))\n",
    "                f_tuple = sorted(f_tuple, key = lambda x:x[1], reverse = False)\n",
    "                f_id = [i[0] for i in f_tuple]\n",
    "                positions = [i[1] for i in f_tuple]\n",
    "                X = pd.concat([pd.DataFrame(X_treat, columns = feature_ids_cont), pd.DataFrame(X_cont, columns = feature_ids_treat)], join = 'inner', ignore_index=True)\n",
    "                mean_rbt = pd.concat([pd.DataFrame(X_treat_rbtamt),pd.DataFrame(X_cont_rbtamt)], join = 'inner', ignore_index=True)\n",
    "                mean_rbt = np.array(mean_rbt)\n",
    "                X_labels = list(X.columns)\n",
    "                X_temp = np.array(X)\n",
    "        else:\n",
    "            raise ValueError('X_treat and X_cont (if specified) need to be array types, f_id has to be a list of hot-encoded categorical variables')\n",
    "        \n",
    "        for f in f_id:\n",
    "            print(np.max(X_temp[:,cat[f+'_id']]))\n",
    "            print(np.min(X_temp[:,cat[f+'_id']]))\n",
    "            if (np.max(X_temp[:,cat[f+'_id']])!=1) | (np.min(X_temp[:,cat[f+'_id']])!=0):\n",
    "                raise ValueError('hot encoded variable is not of binary classification')\n",
    "            else:\n",
    "                pass\n",
    "        #grid = np.linspace(np.percentile(X_temp[:, f_id], grid_lower),\n",
    "        #np.percentile(X_temp[:, f_id], grid_upper),\n",
    "        \n",
    "        nptypes = ['1']*len(types)\n",
    "        functions = [np.mean,np.std,np.percentile,np.median]\n",
    "        function_labels = ['mean','std','percentile','median']\n",
    "        column_labels = types.copy()\n",
    "        #column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "\n",
    "        if set(types) <= set(function_labels):\n",
    "            for i in range(len(functions)):\n",
    "                for j in range(len(types)):\n",
    "                    if function_labels[i] in types[j]:\n",
    "                        nptypes[j] = functions[i]\n",
    "\n",
    "            if (np.percentile in nptypes):\n",
    "                if percentile=='none':\n",
    "                    raise ValueError('percentile needs to be defined')\n",
    "                elif type(percentile) is not list: # 0<=percentile<=100:\n",
    "                    raise ValueError('percentile must be list')\n",
    "                else:\n",
    "                    column_labels[column_labels.index('percentile')] = 'percentile_' + str(percentile[0])\n",
    "                    if len(percentile)>1:\n",
    "                        for p in percentile[1:]:\n",
    "                            types = types + ['percentile'] #pass\n",
    "                            nptypes = nptypes + [np.percentile]\n",
    "                            column_labels = column_labels + ['percentile_' +str(p)]\n",
    "        else:\n",
    "            raise ValueError('types not specified correctly ')\n",
    "        #y_pred = np.zeros((len(grid),len(types)))\n",
    "        #mpc_pred = np.zeros((len(grid), len(types)))        \n",
    "        y_pred = np.zeros((1, len(types)*len(f_id)))\n",
    "        mpc_pred = np.zeros((1, len(types)*len(f_id)))\n",
    "        grid_row = np.identity(len(f_id))\n",
    "        k=0\n",
    "        \n",
    "        column_labels_cr=[]\n",
    "        column_labels_mpc=[]\n",
    "        for f in range(len(f_id)): # i returns the counter, val returns the value at position of counter on grid\n",
    "            p_pos = 0\n",
    "            A = np.array([list(grid_row[f]) for _ in range(len(X_temp))])\n",
    "            X_temp[:, positions] = A\n",
    "            y_temp = (rf_treat.predict(X_temp) - rf_cont.predict(X_temp))\n",
    "            mean_rbt_temp = (y_temp/mean_rbt)\n",
    "            for j in range(len(types)):\n",
    "                if types[j] == 'percentile':\n",
    "                    y_pred[0,k] = nptypes[j](y_temp,percentile[p_pos])\n",
    "                    mpc_pred[0,k] = nptypes[j](mean_rbt_temp,percentile[p_pos])\n",
    "                    column_labels_cr = column_labels_cr + ['cr_'+ f_id[f] +'_'+ types[j] + str(percentile[p_pos])]\n",
    "                    column_labels_mpc= column_labels_mpc + ['mpc_'+ f_id[f] +'_'+ types[j] + str(percentile[p_pos])]\n",
    "                    p_pos = p_pos+1\n",
    "                else:\n",
    "                    y_pred[0,k] = nptypes[j](y_temp)\n",
    "                    mpc_pred[0,k] = nptypes[j](mean_rbt_temp)\n",
    "                    column_labels_cr = column_labels_cr + ['cr_'+ f_id[f] +'_'+ types[j]]\n",
    "                    column_labels_mpc= column_labels_mpc + ['mpc_'+ f_id[f] +'_'+ types[j]]\n",
    "                k = k+1\n",
    "        column_labels = column_labels_cr + column_labels_mpc\n",
    "        df = pd.DataFrame(np.c_[y_pred, mpc_pred], columns = column_labels)\n",
    "        #y_pred = pd.DataFrame(y_pred,columns=types)\n",
    "        return df #grid, y_pred y_pred_mean, y_pred_med, y_pred_p90  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_id = ['educ_nodegree', 'educ_highschool','educ_higher']\n",
    "#display(uplift_cat_2m_partial_dependency_mpc(rfdicts['DUR_fin_baseline']['treat1_rf'], rfdicts['DUR_fin_baseline']['cont1_rf'], f_id, rfdicts['DUR_fin_baseline']['cont1_X_labels'], rfdicts['DUR_fin_baseline']['treat1_X'], rfdicts['DUR_fin_baseline']['treat1_rbtamt'], X_cont=rfdicts['DUR_fin_baseline']['cont1_X'], X_cont_rbtamt=rfdicts['DUR_fin_baseline']['cont1_rbtamt'],  feature_ids_cont=rfdicts['DUR_fin_baseline']['treat1_X_labels'], types=['mean','percentile','std'],percentile=[25,75]))\n",
    "#f_id = ['educ_highschool','educ_higher','educ_nodegree']\n",
    "#display(uplift_cat_2m_partial_dependency_mpc(rfdicts['TOT_fin']['treat1_rf'], rfdicts['TOT_fin']['cont1_rf'], f_id, rfdicts['TOT_fin']['cont1_X_labels'], rfdicts['TOT_fin']['treat1_X'], rfdicts['TOT_fin']['treat1_rbtamt'], X_cont=rfdicts['TOT_fin']['cont1_X'], X_cont_rbtamt=rfdicts['TOT_fin']['cont1_rbtamt'],  feature_ids_cont=rfdicts['TOT_fin']['treat1_X_labels'], types=['mean','std'])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run partial dependence function for given sample and explanatory variables. this may take a while. Hence, save as later as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['treat1', 'cont1', 'cont2', 'cont3', 'treat1_X', 'treat1_X_labels', 'treat1_rbtamt', 'treat1_rf', 'cont1_X', 'cont1_X_labels', 'cont1_rbtamt', 'cont1_rf', 'cont2_X', 'cont2_X_labels', 'cont2_rbtamt', 'cont2_rf', 'cont3_X', 'cont3_X_labels', 'cont3_rbtamt', 'cont3_rf', 'cont2_treat1_y_pred', 'cont2_treat1_mpc_pred', 'cont1_treat1_y_pred', 'cont1_treat1_mpc_pred', 'cont3_treat1_y_pred', 'cont3_treat1_mpc_pred']\n",
      "['FD_fin', 'FD_finit', 'FD_nofin', 'ND_fin', 'ND_finit', 'ND_nofin', 'SND_fin', 'SND_finit', 'SND_nofin', 'TOT_fin', 'TOT_finit', 'TOT_nofin']\n"
     ]
    }
   ],
   "source": [
    "## read python dict back from the file\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rfdict_list = os.listdir(os.getcwd()+'\\\\rf_dicts')\n",
    "rfdict_list = [i for i in rfdict_list if i[-4:]=='.pkl']\n",
    "\n",
    "rfdicts = dict()\n",
    "\n",
    "for rf in rfdict_list:\n",
    "    pkl_file = open(os.getcwd()+'\\\\rf_dicts\\\\'+rf, 'rb')\n",
    "    rfdicts[rf[:-4]] = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "\n",
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "print(list(rfdicts[rfdicts_keys[0]]))\n",
    "print(list(rfdicts))\n",
    "#rfdicts['DUR_fin']['cont1_X_labels']\n",
    "INCOME = ['FINCBTXM'] #'FINCBTAX','FSALARYM',\n",
    "CONTROL = ['adults', 'PERSLT18']\n",
    "MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'qescrowx_sum', 'timeleft'] #'orgmrtx_sum'\n",
    "CAT = [['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5'],['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'],['educ_nodegree','educ_highschool','educ_higher']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SND_fin', 'SND_finit', 'SND_nofin', 'TOT_fin', 'TOT_finit', 'TOT_nofin']\n",
      "SND_fin:\n",
      "SND_fin treat1 cont1 FINCBTXM\n",
      "SND_fin treat1 cont1 finassets\n",
      "SND_fin treat1 cont1 morgpayment\n",
      "SND_fin treat1 cont1 qblncm1x_sum\n",
      "SND_fin treat1 cont1 qescrowx_sum\n",
      "SND_fin treat1 cont1 timeleft\n",
      "SND_fin treat1 cont1 age\n",
      "SND_fin treat1 cont1 adults\n",
      "SND_fin treat1 cont1 PERSLT18\n",
      "SND_fin treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_fin treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_finit:\n",
      "SND_finit treat1 cont1 FINCBTXM\n",
      "SND_finit treat1 cont1 finassets_it\n",
      "SND_finit treat1 cont1 morgpayment\n",
      "SND_finit treat1 cont1 qblncm1x_sum\n",
      "SND_finit treat1 cont1 qescrowx_sum\n",
      "SND_finit treat1 cont1 timeleft\n",
      "SND_finit treat1 cont1 age\n",
      "SND_finit treat1 cont1 adults\n",
      "SND_finit treat1 cont1 PERSLT18\n",
      "SND_finit treat1 cont1 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_finit treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_finit treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_nofin:\n",
      "SND_nofin treat1 cont1 FINCBTXM\n",
      "SND_nofin treat1 cont1 morgpayment\n",
      "SND_nofin treat1 cont1 qblncm1x_sum\n",
      "SND_nofin treat1 cont1 qescrowx_sum\n",
      "SND_nofin treat1 cont1 timeleft\n",
      "SND_nofin treat1 cont1 age\n",
      "SND_nofin treat1 cont1 adults\n",
      "SND_nofin treat1 cont1 PERSLT18\n",
      "SND_nofin treat1 cont1 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_nofin treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "SND_nofin treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_fin:\n",
      "TOT_fin treat1 cont1 FINCBTXM\n",
      "TOT_fin treat1 cont1 finassets\n",
      "TOT_fin treat1 cont1 morgpayment\n",
      "TOT_fin treat1 cont1 qblncm1x_sum\n",
      "TOT_fin treat1 cont1 qescrowx_sum\n",
      "TOT_fin treat1 cont1 timeleft\n",
      "TOT_fin treat1 cont1 age\n",
      "TOT_fin treat1 cont1 adults\n",
      "TOT_fin treat1 cont1 PERSLT18\n",
      "TOT_fin treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_fin treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_finit:\n",
      "TOT_finit treat1 cont1 FINCBTXM\n",
      "TOT_finit treat1 cont1 finassets_it\n",
      "TOT_finit treat1 cont1 morgpayment\n",
      "TOT_finit treat1 cont1 qblncm1x_sum\n",
      "TOT_finit treat1 cont1 qescrowx_sum\n",
      "TOT_finit treat1 cont1 timeleft\n",
      "TOT_finit treat1 cont1 age\n",
      "TOT_finit treat1 cont1 adults\n",
      "TOT_finit treat1 cont1 PERSLT18\n",
      "TOT_finit treat1 cont1 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_finit treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_finit treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_nofin:\n",
      "TOT_nofin treat1 cont1 FINCBTXM\n",
      "TOT_nofin treat1 cont1 morgpayment\n",
      "TOT_nofin treat1 cont1 qblncm1x_sum\n",
      "TOT_nofin treat1 cont1 qescrowx_sum\n",
      "TOT_nofin treat1 cont1 timeleft\n",
      "TOT_nofin treat1 cont1 age\n",
      "TOT_nofin treat1 cont1 adults\n",
      "TOT_nofin treat1 cont1 PERSLT18\n",
      "TOT_nofin treat1 cont1 ['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_nofin treat1 cont1 ['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "TOT_nofin treat1 cont1 ['educ_nodegree', 'educ_highschool', 'educ_higher']\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#def vimp_plot_uplift()\n",
    "rfdicts_keys = list(rfdicts)\n",
    "rfdicts_keys = [key for key in rfdicts_keys if (key.split('_')[0]=='SND') | (key.split('_')[0]=='TOT')]\n",
    "#rfdicts_keys = rfdicts_keys[:1]\n",
    "print(rfdicts_keys)\n",
    "for k in rfdicts_keys:\n",
    "    print(f'{k}:')\n",
    "    rf_keys = list(rfdicts[k])\n",
    "    cont = list(set([key[0:5] for key in rf_keys if key[0:4]=='cont']))\n",
    "    treat = list(set([key[0:6] for key in rf_keys if key[0:5]=='treat']))\n",
    "    cons = k.split('_')[0]\n",
    "    vartype = k.split('_')[1]\n",
    "    newpath = os.getcwd() + '\\\\pdp\\\\' + cons\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "    for t in treat:\n",
    "        for c in ['cont1']:\n",
    "            expvars = rfdicts[k][c+'_X_labels']\n",
    "            INCOME = ['FINCBTXM'] #'FINCBTAX','FSALARYM',\n",
    "            if 'finassets' in expvars:\n",
    "                INCOME = INCOME + ['finassets']\n",
    "            if 'finassets_it' in expvars:\n",
    "                INCOME = INCOME + ['finassets_it']\n",
    "            for v in INCOME:\n",
    "                print(k,t,c,v)\n",
    "                if INCOME.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                               feature_ids_treat=rfdicts[k][t+'_X_labels'], feature_ids_cont=rfdicts[k][t+'_X_labels'], \n",
    "                                                               types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                                        feature_ids_treat=rfdicts[k][t+'_X_labels'],feature_ids_cont=rfdicts[k][t+'_X_labels'], \n",
    "                                                                        types=['mean','percentile','std','median'], percentile=[25,75])) \n",
    "            rfdicts[k][f'{c}_{t}_pdp_INCOME'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_INCOME.csv')\n",
    "            MORTGAGE = ['morgpayment', 'qblncm1x_sum', 'qescrowx_sum', 'timeleft']\n",
    "            for v in MORTGAGE:\n",
    "                print(k,t,c,v)\n",
    "                if MORTGAGE.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'], \n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                                        feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75]))    \n",
    "            rfdicts[k][f'{c}_{t}_pdp_MORTGAGE'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_MORTGAGE.csv')\n",
    "            \n",
    "            for v in ['age']:\n",
    "                print(k,t,c,v)\n",
    "                df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "            rfdicts[k][f'{c}_{t}_pdp_age'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_age.csv')\n",
    "            \n",
    "            for v in CONTROL:\n",
    "                print(k,t,c,v)\n",
    "                if CONTROL.index(v)==0:\n",
    "                    df = uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                               X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                               feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75])\n",
    "                else:\n",
    "                    df = df.join(uplift_num_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],v,rfdicts[k][t+'_X'],rfdicts[k][t+'_rbtamt'],\n",
    "                                                                        X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],feature_ids_treat=rfdicts[k][t+'_X_labels'],\n",
    "                                                                        feature_ids_cont=rfdicts[k][t+'_X_labels'], types=['mean','percentile','std','median'], percentile=[25,75]))    \n",
    "            rfdicts[k][f'{c}_{t}_pdp_CONTROL'] = df\n",
    "            df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_CONTROL.csv')\n",
    "            \n",
    "            CAT = [['CUTENURE_1', 'CUTENURE_2', 'CUTENURE_4', 'CUTENURE_5'],['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'],['educ_nodegree','educ_highschool','educ_higher']]\n",
    "            i = 1\n",
    "            if 'finassets' in expvars:\n",
    "                CAT = [['MARITAL1_1', 'MARITAL1_2', 'MARITAL1_3', 'MARITAL1_4'],['educ_nodegree','educ_highschool','educ_higher']]\n",
    "            for cat in CAT:\n",
    "                print(k,t,c,cat)\n",
    "                df = uplift_cat_2m_partial_dependency_mpc(rfdicts[k][t+'_rf'],rfdicts[k][c+'_rf'],cat,rfdicts[k][t+'_X_labels'],rfdicts[k][t+'_X'],\n",
    "                                                           rfdicts[k][t+'_rbtamt'],X_cont=rfdicts[k][c+'_X'],X_cont_rbtamt=rfdicts[k][c+'_rbtamt'],\n",
    "                                                           feature_ids_cont=rfdicts[k][c+'_X_labels'],types=['mean','percentile','std','median'],\n",
    "                                                           percentile=[25,75])\n",
    "                rfdicts[k][f'{c}_{t}_pdp_CAT_{i}'] = df\n",
    "                df.to_csv(f'{newpath}\\\\{vartype}_{c}_{t}_CAT_{i}.csv')\n",
    "                i = i+1\n",
    "    output = open(os.getcwd() + f'\\\\rf_dicts\\\\{cons}_{vartype}.pkl', 'wb')\n",
    "    pickle.dump(rfdicts[k], output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SND_fin', 'SND_finit', 'SND_nofin', 'TOT_fin', 'TOT_finit', 'TOT_nofin']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfdicts_keys = list(rfdicts)\n",
    "\n",
    "rfdicts_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot partial dependeny as comparison between the different specifications for a given control group and type of consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import PercentFormatter #plot as percentage\n",
    "import seaborn #plot density and histogram at the same time\n",
    "# Set directory where files are downloaded to. Chdir has to be changed in order to run on another computer\n",
    "os.chdir('C:\\\\Users\\justu\\\\Desktop\\\\Masterarbeit\\\\Data\\\\CE') #change this to the folder where the data set is stored, all the results will be saved in the same folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "#five variables to plot: income, age, finanassets, mortgage, education; education is categorical all are stored in different csv files\n",
    "#generate list of tuples for filename and variable\n",
    "pdp_tuples = [('INCOME','FINCBTXM'),('MORTGAGE','qblncm1x_sum'),('MORTGAGE','timeleft'),('age','age')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('INCOME', 'FINCBTXM')\n",
      "('MORTGAGE', 'qblncm1x_sum')\n",
      "('MORTGAGE', 'timeleft')\n",
      "('age', 'age')\n",
      "('INCOME', 'FINCBTXM')\n",
      "('MORTGAGE', 'qblncm1x_sum')\n",
      "('MORTGAGE', 'timeleft')\n",
      "('age', 'age')\n"
     ]
    }
   ],
   "source": [
    "control = 'cont1'\n",
    "treatment = 'treat1'\n",
    "consumption = ['FD','SND','ND','TOT']\n",
    "crtype = ['cr','mpc']\n",
    "for cr in crtype:\n",
    "    for t in pdp_tuples:\n",
    "        print(t)\n",
    "        fig = plt.figure(figsize=(18, 10))\n",
    "        for c,cons in enumerate(consumption):\n",
    "            path = os.getcwd()+f'\\\\pdp\\\\{cons}'\n",
    "            pdp_list = os.listdir(path)\n",
    "            #plot income\n",
    "            pdp_list = [p for p in pdp_list if (p.split('_')[0]=='finit') | (p.split('_')[0]=='nofin') if p.split('_')[3]==f'{t[0]}.csv' if p.split('_')[1]==control if p.split('_')[2]==treatment]\n",
    "            pdp_plot_dict = dict()\n",
    "            for i,p in enumerate(pdp_list):\n",
    "                pdp_plot = pd.read_csv(f'{path}\\\\{p}')\n",
    "                #display(pdp_plot.head())\n",
    "                colnames = list(pdp_plot)\n",
    "                expvar = t[1]\n",
    "                colnames = [c for c in colnames if c.split('_')[0]==expvar.split('_')[0]]\n",
    "                pdp_plot = pdp_plot.loc[:,colnames]\n",
    "                colrenames = [p.split('_')[0]+'_'+n for n in colnames]\n",
    "                rename = dict(zip(colnames,colrenames))\n",
    "                pdp_plot = pdp_plot.rename(columns=rename)\n",
    "                pdp_plot_dict[p] = pdp_plot\n",
    "\n",
    "            for i,p in enumerate(pdp_list):\n",
    "                if i == 0:\n",
    "                    pdp_plot = pdp_plot_dict[p]\n",
    "                else:\n",
    "                    pdp_plot = pdp_plot.join(pdp_plot_dict[p])\n",
    "            #display(pdp_plot.head())\n",
    "            plot = plt.subplot(int(len(consumption)/2),2,c+1)\n",
    "            colors=['red','green']\n",
    "            for j,i in enumerate(['nofin','finit']):\n",
    "                plt.plot(pdp_plot[f'{i}_{expvar}_grid'],pdp_plot[f'{i}_{expvar}_{cr}_mean'],label=f'{i} mean', color=colors[j])\n",
    "                plt.margins(x=0)\n",
    "                plt.plot(pdp_plot[f'{i}_{expvar}_grid'],pdp_plot[f'{i}_{expvar}_{cr}_median'],label=f'{i} median', color=colors[j], linestyle='dashed')\n",
    "                plt.margins(x=0)\n",
    "            if c==0:\n",
    "                plt.legend()\n",
    "            plt.title(cons, fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.getcwd()+f'\\\\pdp\\\\pdp_{cr}_{t[1]}_{control}_{treatment}.pdf')\n",
    "        plt.close()\n",
    "#expvars = list(set([c.split('_')[0] for c in colnames]))[1:]\n",
    "#types = [c.lstrip(c.split('_')[0]) for c in colnames]\n",
    "#plt.plot(pdp_plot[colnames[1]],pdp_plot[colnames[2]])\n",
    "#colnames\n",
    "#pdp_plot[colnames[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n"
     ]
    }
   ],
   "source": [
    "pdp_tuples = [('CAT_1','CUTENURE_'),('CAT_3','educ_')]\n",
    "educ = ['nodegree','highschool','higher']\n",
    "\n",
    "#path = os.getcwd()+f'\\\\pdp\\\\{cons}'\n",
    "\n",
    "#pdp_list = os.listdir(path)\n",
    "#print(pdp_list)\n",
    "            #plot income\n",
    "treatment = 'treat1'\n",
    "control = 'cont1'\n",
    "consumption = ['FD','SND','ND','TOT']\n",
    "for t in pdp_tuples:\n",
    "    ax = dict()\n",
    "    fig, (ax[0],ax[1]) = plt.subplots(1,2, figsize=(20,6))\n",
    "    \n",
    "    for j,types in enumerate(['finit','nofin']):\n",
    "        pdp_car_dict = dict()\n",
    "        for c in consumption:\n",
    "            carvar_means = []\n",
    "            path = os.getcwd()+f'\\\\pdp\\\\{c}'\n",
    "            pdp_list = os.listdir(path)\n",
    "            pdp_list = [p for p in pdp_list if  (p.split('_')[0]==types) if p[-9:-4]==f'{t[0]}' if p.split('_')[1]==control if p.split('_')[2]==treatment] ##(p.split('_')[0]=='finit') |\n",
    "            for p in pdp_list:\n",
    "                pdp_plot = pd.read_csv(f'{path}\\\\{p}')\n",
    "                catvars = [v for v in list(pdp_plot) if v[-6:]=='median' if v[0:3]=='mpc']\n",
    "                if 'mpc_CUTENURE_5_median' in catvars:\n",
    "                    print('check')\n",
    "                    pdp_plot = pdp_plot.drop(columns='mpc_CUTENURE_5_median')\n",
    "                    catvars = [v for v in catvars if v!='mpc_CUTENURE_5_median']\n",
    "                labels = [v[4:-7] for v in catvars]\n",
    "                catvars_val = [pdp_plot.loc[0,v] for v in catvars]\n",
    "                catvars_tuples = list(zip(catvars,catvars_val))\n",
    "                #print(catvars_tuples)\n",
    "                pdp_car_dict[f'{c}_tuples'] = catvars_tuples\n",
    "                pdp_car_dict[f'{c}_values'] = catvars_val\n",
    "                #pdp_car_dict[f'{c}_rects'] = ax.bar(x - width/2, catvars_val, width, label=c)\n",
    "\n",
    "                    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "\n",
    "\n",
    "\n",
    "        #plt.subplot(2,1,j+1)\n",
    "        \n",
    "        \n",
    "        #f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row')\n",
    "        width = 0.23\n",
    "        x = np.arange(len(labels))\n",
    "        #print(x-width/2)\n",
    "        #print((x-width/2)/2)\n",
    "        #print((x-width/2)/2+0.0875)\n",
    "        pos = (x-width/2)-width\n",
    "        hatch = ['','/','//','\\\\']\n",
    "        #print(pos)\n",
    "        for i,c in enumerate(consumption):\n",
    "            #print(x-width/4)\n",
    "            pdp_car_dict[f'{c}_rects'] = ax[j].bar(pos, pdp_car_dict[f'{c}_values'], width, label=c, hatch=hatch[i])\n",
    "            pos = pos +width\n",
    "            #print(pos)\n",
    "            def autolabel(rects):\n",
    "                \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "                for rect in rects:\n",
    "                    height = rect.get_height()\n",
    "                    ax[j].annotate('{:,.2f}'.format(height),\n",
    "                                xy=(rect.get_x() + rect.get_width() / 2, height-0.005),\n",
    "                                xytext=(0, 3),  # 3 points vertical offset\n",
    "                                textcoords=\"offset points\",\n",
    "                                fontsize=12,\n",
    "                                ha='center', va='bottom')\n",
    "\n",
    "\n",
    "            autolabel(pdp_car_dict[f'{c}_rects'])\n",
    "        #ax.set_ylabel('Scores')\n",
    "        #ax.set_title('Scores by group and gender')\n",
    "        #ax.set_title(\"Title for second plot\")\n",
    "        ax[j].set_title(types)\n",
    "        ax[j].set_xticks(x)\n",
    "        ax[j].set_xticklabels(labels)\n",
    "        if j == 0:\n",
    "            ax[j].legend()\n",
    "        else:\n",
    "            pass\n",
    "    fig.tight_layout()\n",
    "    plt.savefig\n",
    "    plt.savefig(os.getcwd()+f'\\\\pdp\\\\pdp_mpc_{t[1]}_{control}_{treatment}.pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = 'cont1'\n",
    "treatment = 'treat1'\n",
    "consumption = ['FD','SND','ND','TOT']\n",
    "crtype = ['cr','mpc']\n",
    "pdp_tuples = [('INCOME','finassets_it')]\n",
    "for cr in crtype:\n",
    "    for t in pdp_tuples:\n",
    "        fig = plt.figure(figsize=(18, 10))\n",
    "        for c,cons in enumerate(consumption):\n",
    "            path = os.getcwd()+f'\\\\pdp\\\\{cons}'\n",
    "            pdp_list = os.listdir(path)\n",
    "            #plot income\n",
    "            pdp_list = [p for p in pdp_list if (p.split('_')[0]=='fin') | (p.split('_')[0]=='finit') if p.split('_')[3]==f'{t[0]}.csv' if p.split('_')[1]==control if p.split('_')[2]==treatment]\n",
    "            pdp_plot_dict = dict()\n",
    "            for i,p in enumerate(pdp_list):\n",
    "                pdp_plot = pd.read_csv(f'{path}\\\\{p}')\n",
    "                #display(pdp_plot.head())\n",
    "                colnames = list(pdp_plot)\n",
    "                expvar = t[1]\n",
    "                colnames = [c for c in colnames if c.split('_')[0]==expvar.split('_')[0]]\n",
    "                pdp_plot = pdp_plot.loc[:,colnames]\n",
    "                colrenames = [p.split('_')[0]+'_'+n for n in colnames]\n",
    "                rename = dict(zip(colnames,colrenames))\n",
    "                pdp_plot = pdp_plot.rename(columns=rename)\n",
    "                pdp_plot_dict[p] = pdp_plot\n",
    "\n",
    "            for i,p in enumerate(pdp_list):\n",
    "                if i == 0:\n",
    "                    pdp_plot = pdp_plot_dict[p]\n",
    "                else:\n",
    "                    pdp_plot = pdp_plot.join(pdp_plot_dict[p])\n",
    "            pdp_plot.loc[pdp_plot[f'fin_finassets_grid']>pdp_plot[f'finit_finassets_it_grid'].max(),[f'fin_finassets_{cr}_mean',f'fin_finassets_{cr}_median']] =np.nan\n",
    "            plot = plt.subplot(int(len(consumption)/2),2,c+1)\n",
    "            colors=['red','green']\n",
    "            for j,i in enumerate(['finit','fin']):\n",
    "                if i == 'fin':\n",
    "                    expvar = 'finassets'\n",
    "                else:\n",
    "                    expvar = 'finassets_it'\n",
    "                plt.plot(pdp_plot[f'{i}_{expvar}_grid'],pdp_plot[f'{i}_{expvar}_{cr}_mean'],label=f'{i} median', color=colors[j])\n",
    "                plt.margins(x=0)\n",
    "                plt.plot(pdp_plot[f'{i}_{expvar}_grid'],pdp_plot[f'{i}_{expvar}_{cr}_median'],label=f'{i} median', color=colors[j], linestyle='dashed')\n",
    "                plt.margins(x=0)\n",
    "            if c==0:\n",
    "                plt.legend()\n",
    "            plt.title(cons, fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.getcwd()+f'\\\\pdp\\\\pdp_{cr}_{t[1]}_{control}_{treatment}.pdf')\n",
    "        plt.close()\n",
    "#expvars = list(set([c.split('_')[0] for c in colnames]))[1:]\n",
    "#types = [c.lstrip(c.split('_')[0]) for c in colnames]\n",
    "#plt.plot(pdp_plot[colnames[1]],pdp_plot[colnames[2]])\n",
    "#colnames\n",
    "#pdp_plot[colnames[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7** Visualize tree (not done yet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    170.0\n",
       "mean       1.0\n",
       "std        0.0\n",
       "min        1.0\n",
       "25%        1.0\n",
       "50%        1.0\n",
       "75%        1.0\n",
       "max        1.0\n",
       "Name: CUTENURE_5, dtype: float64"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs08_cap.loc[fs08_cap['CUTENURE_5']==1,'CUTENURE_5'].describe()\n",
    "#fs08_cap.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = ['FD_','SND']\n",
    "check = [key for key in pds_keys if key[0:7] == 'pdp_' + cons[0]]\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull out one tree from forest\n",
    "tree = rf.estimators_[5]\n",
    "\n",
    "#export image to a dot file\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = x_list, rounded = True, precision = 1 )\n",
    "(graph, )= pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree_check.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
